{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.ArticlesHandler import ArticlesHandler\n",
    "from utils import solve, embedding_matrix_2_kNN, get_rate, accuracy, precision, recall, f1_score\n",
    "from utils import Config\n",
    "import time\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "from postprocessing.SelectLabelsPostprocessor import SelectLabelsPostprocessor\n",
    "from pygcn.utils import encode_onehot, load_from_features, accuracy\n",
    "from pygcn.models import GCN\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import config file and check some values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method of decomposition: GloVe\n"
     ]
    }
   ],
   "source": [
    "config = Config(file='config')\n",
    "\n",
    "assert (config.num_fake_articles + config.num_real_articles > \n",
    "        config.num_nearest_neighbours), \"Can't have more neighbours than nodes!\"\n",
    "\n",
    "print(\"Method of decomposition:\", config.method_decomposition_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the articles and decompose the tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset Random Poltical News Dataset\n",
      "Performing decomposition...\n",
      "Loading Glove Model\n",
      "Done. 400000  words loaded!\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading dataset\", config.dataset_name)\n",
    "articles = ArticlesHandler(config)\n",
    "\n",
    "print(\"Performing decomposition...\")\n",
    "C = articles.get_tensor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.set(\"num_unknown_labels\", 195)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = articles.articles.labels\n",
    "all_labels = articles.articles.labels_untouched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj, features, all_labels = load_from_features(C, all_labels, config)\n",
    "_, _, labels = load_from_features(C, labels, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 2, 2,\n",
      "        2, 2, 2, 1, 1, 1])\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "# idx_train = range(150)\n",
    "# idx_val = range(150, 175)\n",
    "# idx_test = range(175, 200)\n",
    "print(labels)\n",
    "idx_train = np.where(labels)[0]\n",
    "idx_val = np.where(1 - abs(labels))[0][:90]\n",
    "idx_test = np.where(1 - abs(labels))[0][90:]\n",
    "\n",
    "print(len(idx_train))\n",
    "\n",
    "idx_train = torch.LongTensor(idx_train)\n",
    "idx_val = torch.LongTensor(idx_val)\n",
    "idx_test = torch.LongTensor(idx_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 loss_train: 0.9865 acc_train: 0.5000 loss_val: 0.9175 acc_val: 0.5556 time: 0.0048s\n",
      "Epoch: 0002 loss_train: 0.9603 acc_train: 0.5000 loss_val: 0.8732 acc_val: 0.5333 time: 0.0050s\n",
      "Epoch: 0003 loss_train: 0.8525 acc_train: 0.6000 loss_val: 0.8406 acc_val: 0.5111 time: 0.0043s\n",
      "Epoch: 0004 loss_train: 0.8005 acc_train: 0.7000 loss_val: 0.8147 acc_val: 0.4556 time: 0.0070s\n",
      "Epoch: 0005 loss_train: 0.6382 acc_train: 0.9000 loss_val: 0.7976 acc_val: 0.4444 time: 0.0065s\n",
      "Epoch: 0006 loss_train: 0.7644 acc_train: 0.4000 loss_val: 0.7790 acc_val: 0.4444 time: 0.0067s\n",
      "Epoch: 0007 loss_train: 0.8114 acc_train: 0.4000 loss_val: 0.7519 acc_val: 0.4444 time: 0.0039s\n",
      "Epoch: 0008 loss_train: 0.6737 acc_train: 0.6000 loss_val: 0.7263 acc_val: 0.5000 time: 0.0073s\n",
      "Epoch: 0009 loss_train: 0.8677 acc_train: 0.4000 loss_val: 0.7116 acc_val: 0.5333 time: 0.0055s\n",
      "Epoch: 0010 loss_train: 0.7122 acc_train: 0.7000 loss_val: 0.7050 acc_val: 0.5333 time: 0.0037s\n",
      "Epoch: 0011 loss_train: 0.7205 acc_train: 0.4000 loss_val: 0.7007 acc_val: 0.5333 time: 0.0039s\n",
      "Epoch: 0012 loss_train: 0.7084 acc_train: 0.5000 loss_val: 0.6975 acc_val: 0.5333 time: 0.0036s\n",
      "Epoch: 0013 loss_train: 0.6524 acc_train: 0.6000 loss_val: 0.6950 acc_val: 0.5333 time: 0.0035s\n",
      "Epoch: 0014 loss_train: 0.6490 acc_train: 0.6000 loss_val: 0.6940 acc_val: 0.5333 time: 0.0034s\n",
      "Epoch: 0015 loss_train: 0.8250 acc_train: 0.2000 loss_val: 0.6938 acc_val: 0.5556 time: 0.0035s\n",
      "Epoch: 0016 loss_train: 0.6553 acc_train: 0.6000 loss_val: 0.6996 acc_val: 0.5778 time: 0.0035s\n",
      "Epoch: 0017 loss_train: 0.7969 acc_train: 0.5000 loss_val: 0.7019 acc_val: 0.6222 time: 0.0035s\n",
      "Epoch: 0018 loss_train: 0.6030 acc_train: 0.7000 loss_val: 0.7032 acc_val: 0.6222 time: 0.0035s\n",
      "Epoch: 0019 loss_train: 0.6571 acc_train: 0.6000 loss_val: 0.7034 acc_val: 0.6444 time: 0.0034s\n",
      "Epoch: 0020 loss_train: 0.7128 acc_train: 0.5000 loss_val: 0.7031 acc_val: 0.6444 time: 0.0034s\n",
      "Epoch: 0021 loss_train: 0.7655 acc_train: 0.5000 loss_val: 0.6948 acc_val: 0.6222 time: 0.0035s\n",
      "Epoch: 0022 loss_train: 0.6984 acc_train: 0.6000 loss_val: 0.6900 acc_val: 0.5778 time: 0.0034s\n",
      "Epoch: 0023 loss_train: 0.5736 acc_train: 0.6000 loss_val: 0.6861 acc_val: 0.5556 time: 0.0034s\n",
      "Epoch: 0024 loss_train: 0.6569 acc_train: 0.5000 loss_val: 0.6844 acc_val: 0.5444 time: 0.0035s\n",
      "Epoch: 0025 loss_train: 0.6519 acc_train: 0.6000 loss_val: 0.6842 acc_val: 0.5556 time: 0.0035s\n",
      "Epoch: 0026 loss_train: 0.6464 acc_train: 0.6000 loss_val: 0.6849 acc_val: 0.5556 time: 0.0034s\n",
      "Epoch: 0027 loss_train: 0.5266 acc_train: 0.7000 loss_val: 0.6868 acc_val: 0.5889 time: 0.0034s\n",
      "Epoch: 0028 loss_train: 0.6598 acc_train: 0.5000 loss_val: 0.6900 acc_val: 0.6222 time: 0.0034s\n",
      "Epoch: 0029 loss_train: 0.6171 acc_train: 0.8000 loss_val: 0.6990 acc_val: 0.6444 time: 0.0033s\n",
      "Epoch: 0030 loss_train: 0.6438 acc_train: 0.6000 loss_val: 0.7013 acc_val: 0.6556 time: 0.0029s\n",
      "Epoch: 0031 loss_train: 0.6827 acc_train: 0.5000 loss_val: 0.6980 acc_val: 0.6444 time: 0.0029s\n",
      "Epoch: 0032 loss_train: 0.7092 acc_train: 0.5000 loss_val: 0.6938 acc_val: 0.6556 time: 0.0031s\n",
      "Epoch: 0033 loss_train: 0.7035 acc_train: 0.6000 loss_val: 0.6876 acc_val: 0.6222 time: 0.0030s\n",
      "Epoch: 0034 loss_train: 0.6261 acc_train: 0.6000 loss_val: 0.6865 acc_val: 0.6222 time: 0.0031s\n",
      "Epoch: 0035 loss_train: 0.6828 acc_train: 0.5000 loss_val: 0.6838 acc_val: 0.6111 time: 0.0029s\n",
      "Epoch: 0036 loss_train: 0.6530 acc_train: 0.7000 loss_val: 0.6822 acc_val: 0.6000 time: 0.0029s\n",
      "Epoch: 0037 loss_train: 0.5629 acc_train: 0.7000 loss_val: 0.6795 acc_val: 0.5778 time: 0.0029s\n",
      "Epoch: 0038 loss_train: 0.6410 acc_train: 0.8000 loss_val: 0.6790 acc_val: 0.5778 time: 0.0030s\n",
      "Epoch: 0039 loss_train: 0.7188 acc_train: 0.5000 loss_val: 0.6794 acc_val: 0.5889 time: 0.0029s\n",
      "Epoch: 0040 loss_train: 0.7491 acc_train: 0.3000 loss_val: 0.6790 acc_val: 0.5889 time: 0.0030s\n",
      "Epoch: 0041 loss_train: 0.5987 acc_train: 0.7000 loss_val: 0.6783 acc_val: 0.5778 time: 0.0029s\n",
      "Epoch: 0042 loss_train: 0.6295 acc_train: 0.7000 loss_val: 0.6795 acc_val: 0.6000 time: 0.0030s\n",
      "Epoch: 0043 loss_train: 0.5760 acc_train: 0.7000 loss_val: 0.6845 acc_val: 0.6667 time: 0.0030s\n",
      "Epoch: 0044 loss_train: 0.5642 acc_train: 0.6000 loss_val: 0.6886 acc_val: 0.6667 time: 0.0029s\n",
      "Epoch: 0045 loss_train: 0.7089 acc_train: 0.4000 loss_val: 0.6929 acc_val: 0.7333 time: 0.0029s\n",
      "Epoch: 0046 loss_train: 0.5997 acc_train: 0.6000 loss_val: 0.6995 acc_val: 0.7000 time: 0.0029s\n",
      "Epoch: 0047 loss_train: 0.5450 acc_train: 0.7000 loss_val: 0.7033 acc_val: 0.6556 time: 0.0029s\n",
      "Epoch: 0048 loss_train: 0.5921 acc_train: 0.6000 loss_val: 0.6982 acc_val: 0.6667 time: 0.0039s\n",
      "Epoch: 0049 loss_train: 0.6145 acc_train: 0.4000 loss_val: 0.6900 acc_val: 0.7222 time: 0.0030s\n",
      "Epoch: 0050 loss_train: 0.6264 acc_train: 0.6000 loss_val: 0.6795 acc_val: 0.6556 time: 0.0029s\n",
      "Epoch: 0051 loss_train: 0.6164 acc_train: 0.5000 loss_val: 0.6710 acc_val: 0.6778 time: 0.0029s\n",
      "Epoch: 0052 loss_train: 0.5437 acc_train: 0.8000 loss_val: 0.6675 acc_val: 0.6000 time: 0.0029s\n",
      "Epoch: 0053 loss_train: 0.5953 acc_train: 0.8000 loss_val: 0.6675 acc_val: 0.5889 time: 0.0029s\n",
      "Epoch: 0054 loss_train: 0.6294 acc_train: 0.5000 loss_val: 0.6671 acc_val: 0.5889 time: 0.0029s\n",
      "Epoch: 0055 loss_train: 0.6121 acc_train: 0.6000 loss_val: 0.6668 acc_val: 0.6000 time: 0.0055s\n",
      "Epoch: 0056 loss_train: 0.5279 acc_train: 0.8000 loss_val: 0.6680 acc_val: 0.6444 time: 0.0042s\n",
      "Epoch: 0057 loss_train: 0.5685 acc_train: 0.6000 loss_val: 0.6758 acc_val: 0.6556 time: 0.0031s\n",
      "Epoch: 0058 loss_train: 0.5738 acc_train: 0.5000 loss_val: 0.6847 acc_val: 0.7222 time: 0.0032s\n",
      "Epoch: 0059 loss_train: 0.5439 acc_train: 0.7000 loss_val: 0.6854 acc_val: 0.7222 time: 0.0032s\n",
      "Epoch: 0060 loss_train: 0.4947 acc_train: 0.7000 loss_val: 0.6826 acc_val: 0.7111 time: 0.0046s\n",
      "Epoch: 0061 loss_train: 0.5479 acc_train: 0.9000 loss_val: 0.6777 acc_val: 0.7222 time: 0.0049s\n",
      "Epoch: 0062 loss_train: 0.4918 acc_train: 0.8000 loss_val: 0.6733 acc_val: 0.6556 time: 0.0039s\n",
      "Epoch: 0063 loss_train: 0.4589 acc_train: 0.8000 loss_val: 0.6720 acc_val: 0.6889 time: 0.0043s\n",
      "Epoch: 0064 loss_train: 0.5774 acc_train: 0.7000 loss_val: 0.6712 acc_val: 0.6889 time: 0.0093s\n",
      "Epoch: 0065 loss_train: 0.6040 acc_train: 0.7000 loss_val: 0.6758 acc_val: 0.7222 time: 0.0037s\n",
      "Epoch: 0066 loss_train: 0.5779 acc_train: 0.8000 loss_val: 0.6833 acc_val: 0.7111 time: 0.0035s\n",
      "Epoch: 0067 loss_train: 0.5144 acc_train: 0.7000 loss_val: 0.6870 acc_val: 0.7333 time: 0.0034s\n",
      "Epoch: 0068 loss_train: 0.6382 acc_train: 0.5000 loss_val: 0.6849 acc_val: 0.7222 time: 0.0035s\n",
      "Epoch: 0069 loss_train: 0.5742 acc_train: 0.6000 loss_val: 0.6766 acc_val: 0.7000 time: 0.0036s\n",
      "Epoch: 0070 loss_train: 0.5162 acc_train: 0.7000 loss_val: 0.6742 acc_val: 0.7000 time: 0.0036s\n",
      "Epoch: 0071 loss_train: 0.4927 acc_train: 0.8000 loss_val: 0.6781 acc_val: 0.7111 time: 0.0034s\n",
      "Epoch: 0072 loss_train: 0.4741 acc_train: 0.7000 loss_val: 0.6767 acc_val: 0.7111 time: 0.0036s\n",
      "Epoch: 0073 loss_train: 0.5368 acc_train: 0.8000 loss_val: 0.6705 acc_val: 0.7000 time: 0.0034s\n",
      "Epoch: 0074 loss_train: 0.4564 acc_train: 0.8000 loss_val: 0.6646 acc_val: 0.7000 time: 0.0036s\n",
      "Epoch: 0075 loss_train: 0.5781 acc_train: 0.6000 loss_val: 0.6635 acc_val: 0.7000 time: 0.0034s\n",
      "Epoch: 0076 loss_train: 0.5278 acc_train: 0.7000 loss_val: 0.6650 acc_val: 0.7111 time: 0.0034s\n",
      "Epoch: 0077 loss_train: 0.4583 acc_train: 0.8000 loss_val: 0.6699 acc_val: 0.7111 time: 0.0034s\n",
      "Epoch: 0078 loss_train: 0.4553 acc_train: 0.8000 loss_val: 0.6754 acc_val: 0.7222 time: 0.0035s\n",
      "Epoch: 0079 loss_train: 0.4939 acc_train: 0.8000 loss_val: 0.6727 acc_val: 0.7222 time: 0.0034s\n",
      "Epoch: 0080 loss_train: 0.5212 acc_train: 0.6000 loss_val: 0.6687 acc_val: 0.7111 time: 0.0034s\n",
      "Epoch: 0081 loss_train: 0.5492 acc_train: 0.7000 loss_val: 0.6590 acc_val: 0.7000 time: 0.0034s\n",
      "Epoch: 0082 loss_train: 0.5064 acc_train: 0.9000 loss_val: 0.6604 acc_val: 0.7111 time: 0.0034s\n",
      "Epoch: 0083 loss_train: 0.5835 acc_train: 0.8000 loss_val: 0.6723 acc_val: 0.7111 time: 0.0034s\n",
      "Epoch: 0084 loss_train: 0.5250 acc_train: 0.6000 loss_val: 0.6904 acc_val: 0.6667 time: 0.0036s\n",
      "Epoch: 0085 loss_train: 0.4951 acc_train: 0.8000 loss_val: 0.6865 acc_val: 0.6778 time: 0.0035s\n",
      "Epoch: 0086 loss_train: 0.4677 acc_train: 0.8000 loss_val: 0.6845 acc_val: 0.7000 time: 0.0033s\n",
      "Epoch: 0087 loss_train: 0.5041 acc_train: 0.8000 loss_val: 0.6713 acc_val: 0.7111 time: 0.0034s\n",
      "Epoch: 0088 loss_train: 0.5622 acc_train: 0.7000 loss_val: 0.6595 acc_val: 0.7333 time: 0.0034s\n",
      "Epoch: 0089 loss_train: 0.4936 acc_train: 0.6000 loss_val: 0.6628 acc_val: 0.6889 time: 0.0036s\n",
      "Epoch: 0090 loss_train: 0.5367 acc_train: 0.6000 loss_val: 0.6630 acc_val: 0.6778 time: 0.0034s\n",
      "Epoch: 0091 loss_train: 0.5370 acc_train: 0.7000 loss_val: 0.6616 acc_val: 0.7111 time: 0.0034s\n",
      "Epoch: 0092 loss_train: 0.4967 acc_train: 0.9000 loss_val: 0.6650 acc_val: 0.6778 time: 0.0034s\n",
      "Epoch: 0093 loss_train: 0.5065 acc_train: 0.6000 loss_val: 0.6845 acc_val: 0.6889 time: 0.0034s\n",
      "Epoch: 0094 loss_train: 0.4395 acc_train: 0.8000 loss_val: 0.7157 acc_val: 0.6333 time: 0.0034s\n",
      "Epoch: 0095 loss_train: 0.4403 acc_train: 0.8000 loss_val: 0.7437 acc_val: 0.6000 time: 0.0034s\n",
      "Epoch: 0096 loss_train: 0.5499 acc_train: 0.8000 loss_val: 0.7311 acc_val: 0.6222 time: 0.0035s\n",
      "Epoch: 0097 loss_train: 0.4497 acc_train: 0.8000 loss_val: 0.7093 acc_val: 0.6667 time: 0.0034s\n",
      "Epoch: 0098 loss_train: 0.4268 acc_train: 0.8000 loss_val: 0.6838 acc_val: 0.7000 time: 0.0034s\n",
      "Epoch: 0099 loss_train: 0.4797 acc_train: 0.6000 loss_val: 0.6702 acc_val: 0.7111 time: 0.0034s\n",
      "Epoch: 0100 loss_train: 0.4729 acc_train: 0.7000 loss_val: 0.6679 acc_val: 0.6889 time: 0.0033s\n",
      "Epoch: 0101 loss_train: 0.4632 acc_train: 0.7000 loss_val: 0.6669 acc_val: 0.6778 time: 0.0036s\n",
      "Epoch: 0102 loss_train: 0.4143 acc_train: 0.7000 loss_val: 0.6720 acc_val: 0.7000 time: 0.0035s\n",
      "Epoch: 0103 loss_train: 0.4411 acc_train: 0.7000 loss_val: 0.6832 acc_val: 0.6889 time: 0.0035s\n",
      "Epoch: 0104 loss_train: 0.4488 acc_train: 0.7000 loss_val: 0.7003 acc_val: 0.6778 time: 0.0039s\n",
      "Epoch: 0105 loss_train: 0.5133 acc_train: 0.7000 loss_val: 0.7203 acc_val: 0.6333 time: 0.0040s\n",
      "Epoch: 0106 loss_train: 0.3912 acc_train: 0.9000 loss_val: 0.7248 acc_val: 0.6222 time: 0.0031s\n",
      "Epoch: 0107 loss_train: 0.4030 acc_train: 0.8000 loss_val: 0.7106 acc_val: 0.6667 time: 0.0035s\n",
      "Epoch: 0108 loss_train: 0.3640 acc_train: 0.8000 loss_val: 0.6897 acc_val: 0.6778 time: 0.0037s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0109 loss_train: 0.4445 acc_train: 0.8000 loss_val: 0.6723 acc_val: 0.7000 time: 0.0065s\n",
      "Epoch: 0110 loss_train: 0.4212 acc_train: 0.7000 loss_val: 0.6685 acc_val: 0.7111 time: 0.0036s\n",
      "Epoch: 0111 loss_train: 0.4622 acc_train: 0.7000 loss_val: 0.6688 acc_val: 0.7222 time: 0.0035s\n",
      "Epoch: 0112 loss_train: 0.3875 acc_train: 0.8000 loss_val: 0.6734 acc_val: 0.6889 time: 0.0055s\n",
      "Epoch: 0113 loss_train: 0.4406 acc_train: 0.8000 loss_val: 0.6862 acc_val: 0.6889 time: 0.0037s\n",
      "Epoch: 0114 loss_train: 0.4060 acc_train: 0.6000 loss_val: 0.7129 acc_val: 0.6444 time: 0.0041s\n",
      "Epoch: 0115 loss_train: 0.3611 acc_train: 0.7000 loss_val: 0.7379 acc_val: 0.6000 time: 0.0060s\n",
      "Epoch: 0116 loss_train: 0.4826 acc_train: 0.8000 loss_val: 0.7373 acc_val: 0.6111 time: 0.0053s\n",
      "Epoch: 0117 loss_train: 0.4164 acc_train: 0.9000 loss_val: 0.7196 acc_val: 0.6444 time: 0.0046s\n",
      "Epoch: 0118 loss_train: 0.4488 acc_train: 0.6000 loss_val: 0.6898 acc_val: 0.6778 time: 0.0038s\n",
      "Epoch: 0119 loss_train: 0.4247 acc_train: 0.8000 loss_val: 0.6768 acc_val: 0.6778 time: 0.0031s\n",
      "Epoch: 0120 loss_train: 0.3978 acc_train: 0.8000 loss_val: 0.6716 acc_val: 0.6889 time: 0.0030s\n",
      "Epoch: 0121 loss_train: 0.4219 acc_train: 0.7000 loss_val: 0.6722 acc_val: 0.6778 time: 0.0030s\n",
      "Epoch: 0122 loss_train: 0.4554 acc_train: 0.9000 loss_val: 0.6783 acc_val: 0.6889 time: 0.0030s\n",
      "Epoch: 0123 loss_train: 0.3474 acc_train: 0.8000 loss_val: 0.6981 acc_val: 0.6667 time: 0.0032s\n",
      "Epoch: 0124 loss_train: 0.4101 acc_train: 0.9000 loss_val: 0.7135 acc_val: 0.6333 time: 0.0057s\n",
      "Epoch: 0125 loss_train: 0.3676 acc_train: 0.9000 loss_val: 0.7244 acc_val: 0.6222 time: 0.0047s\n",
      "Epoch: 0126 loss_train: 0.4389 acc_train: 0.8000 loss_val: 0.7146 acc_val: 0.6333 time: 0.0037s\n",
      "Epoch: 0127 loss_train: 0.4901 acc_train: 0.7000 loss_val: 0.7015 acc_val: 0.6333 time: 0.0037s\n",
      "Epoch: 0128 loss_train: 0.3333 acc_train: 0.9000 loss_val: 0.6874 acc_val: 0.6778 time: 0.0034s\n",
      "Epoch: 0129 loss_train: 0.4453 acc_train: 0.7000 loss_val: 0.6791 acc_val: 0.6889 time: 0.0036s\n",
      "Epoch: 0130 loss_train: 0.4187 acc_train: 0.7000 loss_val: 0.6728 acc_val: 0.6556 time: 0.0033s\n",
      "Epoch: 0131 loss_train: 0.4085 acc_train: 0.9000 loss_val: 0.6706 acc_val: 0.6667 time: 0.0033s\n",
      "Epoch: 0132 loss_train: 0.3907 acc_train: 0.9000 loss_val: 0.6742 acc_val: 0.6778 time: 0.0038s\n",
      "Epoch: 0133 loss_train: 0.4765 acc_train: 0.7000 loss_val: 0.6848 acc_val: 0.6778 time: 0.0031s\n",
      "Epoch: 0134 loss_train: 0.3593 acc_train: 0.9000 loss_val: 0.7071 acc_val: 0.6333 time: 0.0031s\n",
      "Epoch: 0135 loss_train: 0.4516 acc_train: 0.8000 loss_val: 0.7151 acc_val: 0.6333 time: 0.0036s\n",
      "Epoch: 0136 loss_train: 0.3513 acc_train: 0.9000 loss_val: 0.7078 acc_val: 0.6222 time: 0.0034s\n",
      "Epoch: 0137 loss_train: 0.3535 acc_train: 0.9000 loss_val: 0.6931 acc_val: 0.6444 time: 0.0030s\n",
      "Epoch: 0138 loss_train: 0.4002 acc_train: 0.8000 loss_val: 0.6815 acc_val: 0.6778 time: 0.0032s\n",
      "Epoch: 0139 loss_train: 0.3818 acc_train: 0.8000 loss_val: 0.6800 acc_val: 0.6556 time: 0.0032s\n",
      "Epoch: 0140 loss_train: 0.4011 acc_train: 0.9000 loss_val: 0.6831 acc_val: 0.6556 time: 0.0033s\n",
      "Epoch: 0141 loss_train: 0.4152 acc_train: 0.8000 loss_val: 0.6853 acc_val: 0.6556 time: 0.0034s\n",
      "Epoch: 0142 loss_train: 0.3145 acc_train: 0.9000 loss_val: 0.6902 acc_val: 0.6667 time: 0.0030s\n",
      "Epoch: 0143 loss_train: 0.4829 acc_train: 0.7000 loss_val: 0.7082 acc_val: 0.6444 time: 0.0029s\n",
      "Epoch: 0144 loss_train: 0.3848 acc_train: 0.8000 loss_val: 0.7597 acc_val: 0.6111 time: 0.0036s\n",
      "Epoch: 0145 loss_train: 0.5644 acc_train: 0.8000 loss_val: 0.7547 acc_val: 0.6111 time: 0.0031s\n",
      "Epoch: 0146 loss_train: 0.3764 acc_train: 0.8000 loss_val: 0.7382 acc_val: 0.6222 time: 0.0034s\n",
      "Epoch: 0147 loss_train: 0.4576 acc_train: 0.9000 loss_val: 0.6987 acc_val: 0.6778 time: 0.0031s\n",
      "Epoch: 0148 loss_train: 0.3961 acc_train: 0.7000 loss_val: 0.6885 acc_val: 0.7222 time: 0.0030s\n",
      "Epoch: 0149 loss_train: 0.2879 acc_train: 1.0000 loss_val: 0.6954 acc_val: 0.7000 time: 0.0035s\n",
      "Epoch: 0150 loss_train: 0.4530 acc_train: 0.8000 loss_val: 0.6920 acc_val: 0.7000 time: 0.0031s\n",
      "Epoch: 0151 loss_train: 0.3647 acc_train: 0.8000 loss_val: 0.6891 acc_val: 0.6889 time: 0.0036s\n",
      "Epoch: 0152 loss_train: 0.4286 acc_train: 0.8000 loss_val: 0.7022 acc_val: 0.6556 time: 0.0029s\n",
      "Epoch: 0153 loss_train: 0.3494 acc_train: 0.8000 loss_val: 0.7263 acc_val: 0.6111 time: 0.0033s\n",
      "Epoch: 0154 loss_train: 0.3622 acc_train: 0.8000 loss_val: 0.7410 acc_val: 0.6111 time: 0.0035s\n",
      "Epoch: 0155 loss_train: 0.3679 acc_train: 0.8000 loss_val: 0.7368 acc_val: 0.6111 time: 0.0031s\n",
      "Epoch: 0156 loss_train: 0.3130 acc_train: 0.9000 loss_val: 0.7360 acc_val: 0.6111 time: 0.0033s\n",
      "Epoch: 0157 loss_train: 0.3596 acc_train: 0.9000 loss_val: 0.7321 acc_val: 0.6222 time: 0.0030s\n",
      "Epoch: 0158 loss_train: 0.3751 acc_train: 0.8000 loss_val: 0.7117 acc_val: 0.6556 time: 0.0029s\n",
      "Epoch: 0159 loss_train: 0.3198 acc_train: 0.9000 loss_val: 0.7020 acc_val: 0.6444 time: 0.0035s\n",
      "Epoch: 0160 loss_train: 0.4157 acc_train: 0.7000 loss_val: 0.7013 acc_val: 0.6667 time: 0.0033s\n",
      "Epoch: 0161 loss_train: 0.3897 acc_train: 0.7000 loss_val: 0.7015 acc_val: 0.6667 time: 0.0033s\n",
      "Epoch: 0162 loss_train: 0.3892 acc_train: 0.9000 loss_val: 0.7101 acc_val: 0.6333 time: 0.0029s\n",
      "Epoch: 0163 loss_train: 0.3671 acc_train: 0.9000 loss_val: 0.7114 acc_val: 0.6333 time: 0.0029s\n",
      "Epoch: 0164 loss_train: 0.3823 acc_train: 0.7000 loss_val: 0.7141 acc_val: 0.6556 time: 0.0050s\n",
      "Epoch: 0165 loss_train: 0.2830 acc_train: 1.0000 loss_val: 0.7284 acc_val: 0.6444 time: 0.0039s\n",
      "Epoch: 0166 loss_train: 0.3920 acc_train: 0.9000 loss_val: 0.7270 acc_val: 0.6444 time: 0.0056s\n",
      "Epoch: 0167 loss_train: 0.3724 acc_train: 0.8000 loss_val: 0.7183 acc_val: 0.6556 time: 0.0086s\n",
      "Epoch: 0168 loss_train: 0.3056 acc_train: 0.9000 loss_val: 0.7106 acc_val: 0.6333 time: 0.0038s\n",
      "Epoch: 0169 loss_train: 0.3630 acc_train: 0.8000 loss_val: 0.7047 acc_val: 0.6667 time: 0.0033s\n",
      "Epoch: 0170 loss_train: 0.3902 acc_train: 0.8000 loss_val: 0.7042 acc_val: 0.6667 time: 0.0035s\n",
      "Epoch: 0171 loss_train: 0.3715 acc_train: 0.8000 loss_val: 0.7063 acc_val: 0.6556 time: 0.0034s\n",
      "Epoch: 0172 loss_train: 0.3333 acc_train: 0.9000 loss_val: 0.7100 acc_val: 0.6222 time: 0.0041s\n",
      "Epoch: 0173 loss_train: 0.3515 acc_train: 0.8000 loss_val: 0.7131 acc_val: 0.6333 time: 0.0058s\n",
      "Epoch: 0174 loss_train: 0.3224 acc_train: 0.9000 loss_val: 0.7241 acc_val: 0.6556 time: 0.0029s\n",
      "Epoch: 0175 loss_train: 0.3343 acc_train: 0.9000 loss_val: 0.7318 acc_val: 0.6444 time: 0.0032s\n",
      "Epoch: 0176 loss_train: 0.4095 acc_train: 0.8000 loss_val: 0.7218 acc_val: 0.6333 time: 0.0030s\n",
      "Epoch: 0177 loss_train: 0.3153 acc_train: 1.0000 loss_val: 0.7186 acc_val: 0.6333 time: 0.0030s\n",
      "Epoch: 0178 loss_train: 0.3707 acc_train: 0.8000 loss_val: 0.7158 acc_val: 0.6778 time: 0.0030s\n",
      "Epoch: 0179 loss_train: 0.3850 acc_train: 0.8000 loss_val: 0.7162 acc_val: 0.6889 time: 0.0030s\n",
      "Epoch: 0180 loss_train: 0.3949 acc_train: 0.9000 loss_val: 0.7177 acc_val: 0.6889 time: 0.0030s\n",
      "Epoch: 0181 loss_train: 0.3462 acc_train: 0.7000 loss_val: 0.7236 acc_val: 0.6444 time: 0.0033s\n",
      "Epoch: 0182 loss_train: 0.2815 acc_train: 1.0000 loss_val: 0.7472 acc_val: 0.6444 time: 0.0033s\n",
      "Epoch: 0183 loss_train: 0.3131 acc_train: 0.9000 loss_val: 0.7889 acc_val: 0.6111 time: 0.0036s\n",
      "Epoch: 0184 loss_train: 0.4007 acc_train: 0.8000 loss_val: 0.8033 acc_val: 0.6111 time: 0.0036s\n",
      "Epoch: 0185 loss_train: 0.3551 acc_train: 0.8000 loss_val: 0.7907 acc_val: 0.6111 time: 0.0036s\n",
      "Epoch: 0186 loss_train: 0.3543 acc_train: 0.8000 loss_val: 0.7613 acc_val: 0.6333 time: 0.0036s\n",
      "Epoch: 0187 loss_train: 0.3180 acc_train: 0.9000 loss_val: 0.7375 acc_val: 0.6444 time: 0.0036s\n",
      "Epoch: 0188 loss_train: 0.3056 acc_train: 0.9000 loss_val: 0.7320 acc_val: 0.6889 time: 0.0036s\n",
      "Epoch: 0189 loss_train: 0.3916 acc_train: 0.8000 loss_val: 0.7369 acc_val: 0.7111 time: 0.0036s\n",
      "Epoch: 0190 loss_train: 0.3406 acc_train: 1.0000 loss_val: 0.7356 acc_val: 0.6889 time: 0.0035s\n",
      "Epoch: 0191 loss_train: 0.4110 acc_train: 0.7000 loss_val: 0.7353 acc_val: 0.6556 time: 0.0035s\n",
      "Epoch: 0192 loss_train: 0.2804 acc_train: 0.9000 loss_val: 0.7503 acc_val: 0.6333 time: 0.0035s\n",
      "Epoch: 0193 loss_train: 0.3877 acc_train: 0.8000 loss_val: 0.7594 acc_val: 0.6444 time: 0.0035s\n",
      "Epoch: 0194 loss_train: 0.2601 acc_train: 0.9000 loss_val: 0.7640 acc_val: 0.6333 time: 0.0035s\n",
      "Epoch: 0195 loss_train: 0.2908 acc_train: 0.9000 loss_val: 0.7605 acc_val: 0.6556 time: 0.0036s\n",
      "Epoch: 0196 loss_train: 0.3177 acc_train: 0.9000 loss_val: 0.7550 acc_val: 0.6333 time: 0.0032s\n",
      "Epoch: 0197 loss_train: 0.2843 acc_train: 0.9000 loss_val: 0.7562 acc_val: 0.6333 time: 0.0036s\n",
      "Epoch: 0198 loss_train: 0.3442 acc_train: 0.9000 loss_val: 0.7553 acc_val: 0.6333 time: 0.0036s\n",
      "Epoch: 0199 loss_train: 0.2578 acc_train: 1.0000 loss_val: 0.7536 acc_val: 0.6333 time: 0.0036s\n",
      "Epoch: 0200 loss_train: 0.2909 acc_train: 0.9000 loss_val: 0.7479 acc_val: 0.6444 time: 0.0036s\n",
      "Epoch: 0201 loss_train: 0.2694 acc_train: 0.9000 loss_val: 0.7449 acc_val: 0.6889 time: 0.0036s\n",
      "Epoch: 0202 loss_train: 0.3247 acc_train: 0.9000 loss_val: 0.7452 acc_val: 0.6889 time: 0.0035s\n",
      "Epoch: 0203 loss_train: 0.2372 acc_train: 1.0000 loss_val: 0.7528 acc_val: 0.6444 time: 0.0059s\n",
      "Epoch: 0204 loss_train: 0.2616 acc_train: 1.0000 loss_val: 0.7728 acc_val: 0.6556 time: 0.0060s\n",
      "Epoch: 0205 loss_train: 0.3142 acc_train: 0.9000 loss_val: 0.7887 acc_val: 0.6333 time: 0.0051s\n",
      "Epoch: 0206 loss_train: 0.3491 acc_train: 0.8000 loss_val: 0.8051 acc_val: 0.6222 time: 0.0036s\n",
      "Epoch: 0207 loss_train: 0.2626 acc_train: 0.9000 loss_val: 0.7989 acc_val: 0.6333 time: 0.0056s\n",
      "Epoch: 0208 loss_train: 0.2731 acc_train: 0.9000 loss_val: 0.7927 acc_val: 0.6556 time: 0.0038s\n",
      "Epoch: 0209 loss_train: 0.2457 acc_train: 0.9000 loss_val: 0.7882 acc_val: 0.6556 time: 0.0035s\n",
      "Epoch: 0210 loss_train: 0.2622 acc_train: 0.9000 loss_val: 0.8017 acc_val: 0.6444 time: 0.0036s\n",
      "Epoch: 0211 loss_train: 0.4571 acc_train: 0.7000 loss_val: 0.8011 acc_val: 0.6444 time: 0.0072s\n",
      "Epoch: 0212 loss_train: 0.3191 acc_train: 0.9000 loss_val: 0.7902 acc_val: 0.6444 time: 0.0039s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0213 loss_train: 0.2959 acc_train: 0.9000 loss_val: 0.7775 acc_val: 0.6333 time: 0.0090s\n",
      "Epoch: 0214 loss_train: 0.2901 acc_train: 0.9000 loss_val: 0.7736 acc_val: 0.6889 time: 0.0037s\n",
      "Epoch: 0215 loss_train: 0.3096 acc_train: 0.8000 loss_val: 0.7765 acc_val: 0.6444 time: 0.0029s\n",
      "Epoch: 0216 loss_train: 0.3209 acc_train: 0.9000 loss_val: 0.7914 acc_val: 0.6556 time: 0.0032s\n",
      "Epoch: 0217 loss_train: 0.2773 acc_train: 0.9000 loss_val: 0.8208 acc_val: 0.6222 time: 0.0038s\n",
      "Epoch: 0218 loss_train: 0.3267 acc_train: 0.9000 loss_val: 0.8322 acc_val: 0.6222 time: 0.0059s\n",
      "Epoch: 0219 loss_train: 0.2564 acc_train: 0.9000 loss_val: 0.8320 acc_val: 0.6222 time: 0.0065s\n",
      "Epoch: 0220 loss_train: 0.3076 acc_train: 0.8000 loss_val: 0.7997 acc_val: 0.6556 time: 0.0072s\n",
      "Epoch: 0221 loss_train: 0.3128 acc_train: 0.8000 loss_val: 0.7787 acc_val: 0.6778 time: 0.0050s\n",
      "Epoch: 0222 loss_train: 0.3459 acc_train: 0.8000 loss_val: 0.7822 acc_val: 0.6889 time: 0.0060s\n",
      "Epoch: 0223 loss_train: 0.2734 acc_train: 1.0000 loss_val: 0.7792 acc_val: 0.6778 time: 0.0039s\n",
      "Epoch: 0224 loss_train: 0.2617 acc_train: 0.9000 loss_val: 0.7798 acc_val: 0.6667 time: 0.0030s\n",
      "Epoch: 0225 loss_train: 0.3695 acc_train: 0.8000 loss_val: 0.8078 acc_val: 0.6333 time: 0.0022s\n",
      "Epoch: 0226 loss_train: 0.3198 acc_train: 0.9000 loss_val: 0.8720 acc_val: 0.6222 time: 0.0103s\n",
      "Epoch: 0227 loss_train: 0.3852 acc_train: 0.8000 loss_val: 0.8732 acc_val: 0.6222 time: 0.0038s\n",
      "Epoch: 0228 loss_train: 0.2009 acc_train: 0.9000 loss_val: 0.8471 acc_val: 0.6111 time: 0.0031s\n",
      "Epoch: 0229 loss_train: 0.2324 acc_train: 1.0000 loss_val: 0.8084 acc_val: 0.6333 time: 0.0143s\n",
      "Epoch: 0230 loss_train: 0.3138 acc_train: 0.8000 loss_val: 0.7741 acc_val: 0.6333 time: 0.0036s\n",
      "Epoch: 0231 loss_train: 0.2116 acc_train: 0.9000 loss_val: 0.7682 acc_val: 0.6889 time: 0.0080s\n",
      "Epoch: 0232 loss_train: 0.2514 acc_train: 0.9000 loss_val: 0.7714 acc_val: 0.6889 time: 0.0047s\n",
      "Epoch: 0233 loss_train: 0.2741 acc_train: 0.8000 loss_val: 0.7664 acc_val: 0.6778 time: 0.0039s\n",
      "Epoch: 0234 loss_train: 0.3254 acc_train: 0.9000 loss_val: 0.7928 acc_val: 0.6444 time: 0.0081s\n",
      "Epoch: 0235 loss_train: 0.2908 acc_train: 0.9000 loss_val: 0.8279 acc_val: 0.6333 time: 0.0052s\n",
      "Epoch: 0236 loss_train: 0.2775 acc_train: 0.9000 loss_val: 0.8568 acc_val: 0.6222 time: 0.0078s\n",
      "Epoch: 0237 loss_train: 0.2132 acc_train: 1.0000 loss_val: 0.8633 acc_val: 0.6222 time: 0.0057s\n",
      "Epoch: 0238 loss_train: 0.2823 acc_train: 0.9000 loss_val: 0.8530 acc_val: 0.6222 time: 0.0032s\n",
      "Epoch: 0239 loss_train: 0.2238 acc_train: 1.0000 loss_val: 0.8277 acc_val: 0.6556 time: 0.0076s\n",
      "Epoch: 0240 loss_train: 0.2773 acc_train: 0.9000 loss_val: 0.8028 acc_val: 0.6667 time: 0.0038s\n",
      "Epoch: 0241 loss_train: 0.2936 acc_train: 0.9000 loss_val: 0.8022 acc_val: 0.6778 time: 0.0039s\n",
      "Epoch: 0242 loss_train: 0.3720 acc_train: 0.8000 loss_val: 0.8055 acc_val: 0.6333 time: 0.0094s\n",
      "Epoch: 0243 loss_train: 0.1847 acc_train: 1.0000 loss_val: 0.8262 acc_val: 0.6556 time: 0.0067s\n",
      "Epoch: 0244 loss_train: 0.1370 acc_train: 1.0000 loss_val: 0.8597 acc_val: 0.6111 time: 0.0084s\n",
      "Epoch: 0245 loss_train: 0.2380 acc_train: 0.9000 loss_val: 0.8803 acc_val: 0.6222 time: 0.0041s\n",
      "Epoch: 0246 loss_train: 0.3657 acc_train: 0.8000 loss_val: 0.8465 acc_val: 0.6222 time: 0.0054s\n",
      "Epoch: 0247 loss_train: 0.3359 acc_train: 0.8000 loss_val: 0.8514 acc_val: 0.6222 time: 0.0084s\n",
      "Epoch: 0248 loss_train: 0.2572 acc_train: 0.9000 loss_val: 0.8746 acc_val: 0.6222 time: 0.0032s\n",
      "Epoch: 0249 loss_train: 0.2087 acc_train: 0.9000 loss_val: 0.8781 acc_val: 0.6222 time: 0.0154s\n",
      "Epoch: 0250 loss_train: 0.1649 acc_train: 0.9000 loss_val: 0.8511 acc_val: 0.6333 time: 0.0050s\n",
      "Epoch: 0251 loss_train: 0.3169 acc_train: 0.8000 loss_val: 0.8253 acc_val: 0.6333 time: 0.0074s\n",
      "Epoch: 0252 loss_train: 0.2375 acc_train: 0.9000 loss_val: 0.8150 acc_val: 0.6778 time: 0.0030s\n",
      "Epoch: 0253 loss_train: 0.2830 acc_train: 0.9000 loss_val: 0.8215 acc_val: 0.6889 time: 0.0035s\n",
      "Epoch: 0254 loss_train: 0.2670 acc_train: 0.9000 loss_val: 0.8159 acc_val: 0.6778 time: 0.0080s\n",
      "Epoch: 0255 loss_train: 0.1820 acc_train: 0.9000 loss_val: 0.8292 acc_val: 0.6333 time: 0.0040s\n",
      "Epoch: 0256 loss_train: 0.3189 acc_train: 0.9000 loss_val: 0.8611 acc_val: 0.6333 time: 0.0044s\n",
      "Epoch: 0257 loss_train: 0.2756 acc_train: 0.8000 loss_val: 0.8887 acc_val: 0.6333 time: 0.0067s\n",
      "Epoch: 0258 loss_train: 0.2706 acc_train: 0.9000 loss_val: 0.8829 acc_val: 0.6222 time: 0.0051s\n",
      "Epoch: 0259 loss_train: 0.1950 acc_train: 0.9000 loss_val: 0.8703 acc_val: 0.6222 time: 0.0035s\n",
      "Epoch: 0260 loss_train: 0.2064 acc_train: 1.0000 loss_val: 0.8512 acc_val: 0.6556 time: 0.0100s\n",
      "Epoch: 0261 loss_train: 0.1797 acc_train: 0.9000 loss_val: 0.8360 acc_val: 0.6333 time: 0.0052s\n",
      "Epoch: 0262 loss_train: 0.2667 acc_train: 0.9000 loss_val: 0.8275 acc_val: 0.6556 time: 0.0082s\n",
      "Epoch: 0263 loss_train: 0.2005 acc_train: 0.9000 loss_val: 0.8250 acc_val: 0.6667 time: 0.0057s\n",
      "Epoch: 0264 loss_train: 0.3205 acc_train: 0.9000 loss_val: 0.8222 acc_val: 0.6667 time: 0.0078s\n",
      "Epoch: 0265 loss_train: 0.2047 acc_train: 0.9000 loss_val: 0.8223 acc_val: 0.6667 time: 0.0053s\n",
      "Epoch: 0266 loss_train: 0.3675 acc_train: 0.8000 loss_val: 0.8295 acc_val: 0.6444 time: 0.0038s\n",
      "Epoch: 0267 loss_train: 0.2179 acc_train: 0.9000 loss_val: 0.8400 acc_val: 0.6333 time: 0.0059s\n",
      "Epoch: 0268 loss_train: 0.2219 acc_train: 0.9000 loss_val: 0.8309 acc_val: 0.6333 time: 0.0039s\n",
      "Epoch: 0269 loss_train: 0.3123 acc_train: 1.0000 loss_val: 0.8262 acc_val: 0.6444 time: 0.0032s\n",
      "Epoch: 0270 loss_train: 0.2413 acc_train: 0.9000 loss_val: 0.8244 acc_val: 0.6333 time: 0.0030s\n",
      "Epoch: 0271 loss_train: 0.2515 acc_train: 0.9000 loss_val: 0.8104 acc_val: 0.6444 time: 0.0067s\n",
      "Epoch: 0272 loss_train: 0.3164 acc_train: 0.9000 loss_val: 0.8034 acc_val: 0.6444 time: 0.0036s\n",
      "Epoch: 0273 loss_train: 0.2181 acc_train: 1.0000 loss_val: 0.8053 acc_val: 0.6444 time: 0.0037s\n",
      "Epoch: 0274 loss_train: 0.3228 acc_train: 0.9000 loss_val: 0.8021 acc_val: 0.6444 time: 0.0034s\n",
      "Epoch: 0275 loss_train: 0.1941 acc_train: 0.9000 loss_val: 0.8011 acc_val: 0.6667 time: 0.0035s\n",
      "Epoch: 0276 loss_train: 0.3230 acc_train: 1.0000 loss_val: 0.8115 acc_val: 0.6556 time: 0.0036s\n",
      "Epoch: 0277 loss_train: 0.2578 acc_train: 1.0000 loss_val: 0.8259 acc_val: 0.6556 time: 0.0035s\n",
      "Epoch: 0278 loss_train: 0.2125 acc_train: 1.0000 loss_val: 0.8386 acc_val: 0.6333 time: 0.0034s\n",
      "Epoch: 0279 loss_train: 0.2155 acc_train: 0.9000 loss_val: 0.8448 acc_val: 0.6333 time: 0.0034s\n",
      "Epoch: 0280 loss_train: 0.2309 acc_train: 1.0000 loss_val: 0.8425 acc_val: 0.6333 time: 0.0036s\n",
      "Epoch: 0281 loss_train: 0.3476 acc_train: 0.8000 loss_val: 0.8249 acc_val: 0.6667 time: 0.0033s\n",
      "Epoch: 0282 loss_train: 0.2378 acc_train: 0.9000 loss_val: 0.8261 acc_val: 0.6667 time: 0.0030s\n",
      "Epoch: 0283 loss_train: 0.1894 acc_train: 1.0000 loss_val: 0.8328 acc_val: 0.6667 time: 0.0031s\n",
      "Epoch: 0284 loss_train: 0.2372 acc_train: 0.9000 loss_val: 0.8454 acc_val: 0.6556 time: 0.0031s\n",
      "Epoch: 0285 loss_train: 0.2590 acc_train: 1.0000 loss_val: 0.8638 acc_val: 0.6556 time: 0.0033s\n",
      "Epoch: 0286 loss_train: 0.2103 acc_train: 0.9000 loss_val: 0.8755 acc_val: 0.6333 time: 0.0031s\n",
      "Epoch: 0287 loss_train: 0.1847 acc_train: 0.9000 loss_val: 0.8780 acc_val: 0.6222 time: 0.0030s\n",
      "Epoch: 0288 loss_train: 0.2774 acc_train: 0.9000 loss_val: 0.8672 acc_val: 0.6556 time: 0.0030s\n",
      "Epoch: 0289 loss_train: 0.1864 acc_train: 0.9000 loss_val: 0.8660 acc_val: 0.6556 time: 0.0030s\n",
      "Epoch: 0290 loss_train: 0.2308 acc_train: 1.0000 loss_val: 0.8840 acc_val: 0.6222 time: 0.0029s\n",
      "Epoch: 0291 loss_train: 0.1940 acc_train: 0.9000 loss_val: 0.9005 acc_val: 0.6222 time: 0.0032s\n",
      "Epoch: 0292 loss_train: 0.2624 acc_train: 0.9000 loss_val: 0.8733 acc_val: 0.6556 time: 0.0031s\n",
      "Epoch: 0293 loss_train: 0.2577 acc_train: 0.9000 loss_val: 0.8638 acc_val: 0.6444 time: 0.0043s\n",
      "Epoch: 0294 loss_train: 0.2167 acc_train: 1.0000 loss_val: 0.8613 acc_val: 0.6556 time: 0.0039s\n",
      "Epoch: 0295 loss_train: 0.2405 acc_train: 0.8000 loss_val: 0.8555 acc_val: 0.6556 time: 0.0041s\n",
      "Epoch: 0296 loss_train: 0.1176 acc_train: 1.0000 loss_val: 0.8608 acc_val: 0.6333 time: 0.0036s\n",
      "Epoch: 0297 loss_train: 0.2118 acc_train: 0.9000 loss_val: 0.8641 acc_val: 0.6333 time: 0.0050s\n",
      "Epoch: 0298 loss_train: 0.2943 acc_train: 0.9000 loss_val: 0.8502 acc_val: 0.6333 time: 0.0035s\n",
      "Epoch: 0299 loss_train: 0.2944 acc_train: 1.0000 loss_val: 0.8633 acc_val: 0.6333 time: 0.0037s\n",
      "Epoch: 0300 loss_train: 0.1382 acc_train: 1.0000 loss_val: 0.8798 acc_val: 0.6222 time: 0.0030s\n",
      "Epoch: 0301 loss_train: 0.2234 acc_train: 0.9000 loss_val: 0.8832 acc_val: 0.6333 time: 0.0030s\n",
      "Epoch: 0302 loss_train: 0.2093 acc_train: 0.9000 loss_val: 0.8652 acc_val: 0.6444 time: 0.0072s\n",
      "Epoch: 0303 loss_train: 0.2280 acc_train: 0.9000 loss_val: 0.8589 acc_val: 0.6667 time: 0.0032s\n",
      "Epoch: 0304 loss_train: 0.1912 acc_train: 1.0000 loss_val: 0.8590 acc_val: 0.6667 time: 0.0030s\n",
      "Epoch: 0305 loss_train: 0.2087 acc_train: 1.0000 loss_val: 0.8632 acc_val: 0.6556 time: 0.0031s\n",
      "Epoch: 0306 loss_train: 0.2585 acc_train: 0.9000 loss_val: 0.8771 acc_val: 0.6333 time: 0.0035s\n",
      "Epoch: 0307 loss_train: 0.1453 acc_train: 1.0000 loss_val: 0.9041 acc_val: 0.6222 time: 0.0032s\n",
      "Epoch: 0308 loss_train: 0.2828 acc_train: 0.9000 loss_val: 0.9054 acc_val: 0.6222 time: 0.0044s\n",
      "Epoch: 0309 loss_train: 0.3209 acc_train: 0.9000 loss_val: 0.8869 acc_val: 0.6333 time: 0.0040s\n",
      "Epoch: 0310 loss_train: 0.2095 acc_train: 1.0000 loss_val: 0.8754 acc_val: 0.6333 time: 0.0036s\n",
      "Epoch: 0311 loss_train: 0.2216 acc_train: 0.9000 loss_val: 0.8664 acc_val: 0.6667 time: 0.0033s\n",
      "Epoch: 0312 loss_train: 0.2444 acc_train: 1.0000 loss_val: 0.8695 acc_val: 0.6556 time: 0.0033s\n",
      "Epoch: 0313 loss_train: 0.1753 acc_train: 1.0000 loss_val: 0.8807 acc_val: 0.6333 time: 0.0031s\n",
      "Epoch: 0314 loss_train: 0.1775 acc_train: 1.0000 loss_val: 0.8929 acc_val: 0.6444 time: 0.0032s\n",
      "Epoch: 0315 loss_train: 0.2285 acc_train: 0.9000 loss_val: 0.8787 acc_val: 0.6333 time: 0.0036s\n",
      "Epoch: 0316 loss_train: 0.2061 acc_train: 1.0000 loss_val: 0.8723 acc_val: 0.6444 time: 0.0032s\n",
      "Epoch: 0317 loss_train: 0.2125 acc_train: 0.9000 loss_val: 0.8763 acc_val: 0.6444 time: 0.0038s\n",
      "Epoch: 0318 loss_train: 0.2035 acc_train: 1.0000 loss_val: 0.8860 acc_val: 0.6444 time: 0.0034s\n",
      "Epoch: 0319 loss_train: 0.2014 acc_train: 1.0000 loss_val: 0.9083 acc_val: 0.6333 time: 0.0032s\n",
      "Epoch: 0320 loss_train: 0.2454 acc_train: 0.9000 loss_val: 0.9519 acc_val: 0.6222 time: 0.0035s\n",
      "Epoch: 0321 loss_train: 0.1718 acc_train: 0.9000 loss_val: 0.9916 acc_val: 0.6222 time: 0.0033s\n",
      "Epoch: 0322 loss_train: 0.2275 acc_train: 0.8000 loss_val: 0.9818 acc_val: 0.6222 time: 0.0035s\n",
      "Epoch: 0323 loss_train: 0.2066 acc_train: 0.9000 loss_val: 0.9347 acc_val: 0.6444 time: 0.0035s\n",
      "Epoch: 0324 loss_train: 0.2293 acc_train: 0.9000 loss_val: 0.8911 acc_val: 0.6667 time: 0.0043s\n",
      "Epoch: 0325 loss_train: 0.1536 acc_train: 1.0000 loss_val: 0.8973 acc_val: 0.6778 time: 0.0038s\n",
      "Epoch: 0326 loss_train: 0.2860 acc_train: 0.8000 loss_val: 0.9011 acc_val: 0.6889 time: 0.0036s\n",
      "Epoch: 0327 loss_train: 0.2070 acc_train: 1.0000 loss_val: 0.8966 acc_val: 0.6556 time: 0.0037s\n",
      "Epoch: 0328 loss_train: 0.2870 acc_train: 0.9000 loss_val: 0.9315 acc_val: 0.6333 time: 0.0040s\n",
      "Epoch: 0329 loss_train: 0.1527 acc_train: 1.0000 loss_val: 1.0234 acc_val: 0.6222 time: 0.0037s\n",
      "Epoch: 0330 loss_train: 0.3493 acc_train: 0.8000 loss_val: 1.0336 acc_val: 0.6333 time: 0.0037s\n",
      "Epoch: 0331 loss_train: 0.3804 acc_train: 0.9000 loss_val: 0.9556 acc_val: 0.6222 time: 0.0036s\n",
      "Epoch: 0332 loss_train: 0.2100 acc_train: 0.9000 loss_val: 0.8998 acc_val: 0.6667 time: 0.0036s\n",
      "Epoch: 0333 loss_train: 0.1519 acc_train: 1.0000 loss_val: 0.9134 acc_val: 0.6889 time: 0.0041s\n",
      "Epoch: 0334 loss_train: 0.2146 acc_train: 0.9000 loss_val: 0.9268 acc_val: 0.6889 time: 0.0038s\n",
      "Epoch: 0335 loss_train: 0.3561 acc_train: 0.7000 loss_val: 0.8955 acc_val: 0.6556 time: 0.0037s\n",
      "Epoch: 0336 loss_train: 0.1556 acc_train: 1.0000 loss_val: 0.9211 acc_val: 0.6222 time: 0.0036s\n",
      "Epoch: 0337 loss_train: 0.1229 acc_train: 1.0000 loss_val: 0.9941 acc_val: 0.6222 time: 0.0038s\n",
      "Epoch: 0338 loss_train: 0.3088 acc_train: 0.8000 loss_val: 0.9743 acc_val: 0.6222 time: 0.0038s\n",
      "Epoch: 0339 loss_train: 0.2746 acc_train: 0.8000 loss_val: 0.9121 acc_val: 0.6222 time: 0.0034s\n",
      "Epoch: 0340 loss_train: 0.2003 acc_train: 0.9000 loss_val: 0.8727 acc_val: 0.6556 time: 0.0037s\n",
      "Epoch: 0341 loss_train: 0.2543 acc_train: 1.0000 loss_val: 0.8667 acc_val: 0.6889 time: 0.0043s\n",
      "Epoch: 0342 loss_train: 0.1971 acc_train: 1.0000 loss_val: 0.8611 acc_val: 0.6889 time: 0.0040s\n",
      "Epoch: 0343 loss_train: 0.1922 acc_train: 1.0000 loss_val: 0.8447 acc_val: 0.6889 time: 0.0038s\n",
      "Epoch: 0344 loss_train: 0.1823 acc_train: 1.0000 loss_val: 0.8403 acc_val: 0.6556 time: 0.0036s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0345 loss_train: 0.2328 acc_train: 1.0000 loss_val: 0.8701 acc_val: 0.6222 time: 0.0050s\n",
      "Epoch: 0346 loss_train: 0.1666 acc_train: 0.9000 loss_val: 0.9231 acc_val: 0.6111 time: 0.0042s\n",
      "Epoch: 0347 loss_train: 0.2358 acc_train: 0.9000 loss_val: 0.9567 acc_val: 0.6222 time: 0.0043s\n",
      "Epoch: 0348 loss_train: 0.2942 acc_train: 0.9000 loss_val: 0.9414 acc_val: 0.6111 time: 0.0031s\n",
      "Epoch: 0349 loss_train: 0.2129 acc_train: 0.9000 loss_val: 0.9149 acc_val: 0.6222 time: 0.0033s\n",
      "Epoch: 0350 loss_train: 0.2175 acc_train: 0.9000 loss_val: 0.8986 acc_val: 0.6667 time: 0.0064s\n",
      "Epoch: 0351 loss_train: 0.2157 acc_train: 1.0000 loss_val: 0.9006 acc_val: 0.6556 time: 0.0049s\n",
      "Epoch: 0352 loss_train: 0.1112 acc_train: 1.0000 loss_val: 0.9125 acc_val: 0.6556 time: 0.0038s\n",
      "Epoch: 0353 loss_train: 0.2218 acc_train: 1.0000 loss_val: 0.9168 acc_val: 0.6556 time: 0.0036s\n",
      "Epoch: 0354 loss_train: 0.2322 acc_train: 0.9000 loss_val: 0.9221 acc_val: 0.6667 time: 0.0074s\n",
      "Epoch: 0355 loss_train: 0.1521 acc_train: 1.0000 loss_val: 0.9394 acc_val: 0.6444 time: 0.0034s\n",
      "Epoch: 0356 loss_train: 0.1802 acc_train: 1.0000 loss_val: 0.9553 acc_val: 0.6444 time: 0.0088s\n",
      "Epoch: 0357 loss_train: 0.2405 acc_train: 0.9000 loss_val: 0.9672 acc_val: 0.6222 time: 0.0038s\n",
      "Epoch: 0358 loss_train: 0.1496 acc_train: 1.0000 loss_val: 0.9898 acc_val: 0.6333 time: 0.0032s\n",
      "Epoch: 0359 loss_train: 0.2076 acc_train: 0.9000 loss_val: 0.9719 acc_val: 0.6222 time: 0.0037s\n",
      "Epoch: 0360 loss_train: 0.1938 acc_train: 0.9000 loss_val: 0.9479 acc_val: 0.6667 time: 0.0066s\n",
      "Epoch: 0361 loss_train: 0.1725 acc_train: 1.0000 loss_val: 0.9441 acc_val: 0.6667 time: 0.0048s\n",
      "Epoch: 0362 loss_train: 0.2157 acc_train: 0.9000 loss_val: 0.9411 acc_val: 0.6556 time: 0.0034s\n",
      "Epoch: 0363 loss_train: 0.1488 acc_train: 0.9000 loss_val: 0.9382 acc_val: 0.6556 time: 0.0034s\n",
      "Epoch: 0364 loss_train: 0.2027 acc_train: 1.0000 loss_val: 0.9479 acc_val: 0.6667 time: 0.0030s\n",
      "Epoch: 0365 loss_train: 0.1616 acc_train: 1.0000 loss_val: 0.9955 acc_val: 0.6333 time: 0.0030s\n",
      "Epoch: 0366 loss_train: 0.1788 acc_train: 0.9000 loss_val: 1.0568 acc_val: 0.6222 time: 0.0030s\n",
      "Epoch: 0367 loss_train: 0.1865 acc_train: 0.9000 loss_val: 1.0724 acc_val: 0.6222 time: 0.0030s\n",
      "Epoch: 0368 loss_train: 0.2435 acc_train: 0.9000 loss_val: 1.0273 acc_val: 0.6222 time: 0.0033s\n",
      "Epoch: 0369 loss_train: 0.1929 acc_train: 0.9000 loss_val: 0.9777 acc_val: 0.6444 time: 0.0030s\n",
      "Epoch: 0370 loss_train: 0.1605 acc_train: 0.9000 loss_val: 0.9453 acc_val: 0.6556 time: 0.0030s\n",
      "Epoch: 0371 loss_train: 0.1511 acc_train: 1.0000 loss_val: 0.9298 acc_val: 0.6556 time: 0.0030s\n",
      "Epoch: 0372 loss_train: 0.1482 acc_train: 1.0000 loss_val: 0.9273 acc_val: 0.6556 time: 0.0030s\n",
      "Epoch: 0373 loss_train: 0.1429 acc_train: 1.0000 loss_val: 0.9307 acc_val: 0.6667 time: 0.0030s\n",
      "Epoch: 0374 loss_train: 0.1350 acc_train: 1.0000 loss_val: 0.9402 acc_val: 0.6556 time: 0.0030s\n",
      "Epoch: 0375 loss_train: 0.1555 acc_train: 1.0000 loss_val: 0.9538 acc_val: 0.6556 time: 0.0030s\n",
      "Epoch: 0376 loss_train: 0.1617 acc_train: 0.9000 loss_val: 0.9500 acc_val: 0.6556 time: 0.0031s\n",
      "Epoch: 0377 loss_train: 0.1368 acc_train: 1.0000 loss_val: 0.9519 acc_val: 0.6556 time: 0.0030s\n",
      "Epoch: 0378 loss_train: 0.1737 acc_train: 0.9000 loss_val: 0.9462 acc_val: 0.6667 time: 0.0029s\n",
      "Epoch: 0379 loss_train: 0.1573 acc_train: 1.0000 loss_val: 0.9455 acc_val: 0.6667 time: 0.0029s\n",
      "Epoch: 0380 loss_train: 0.1494 acc_train: 1.0000 loss_val: 0.9535 acc_val: 0.6667 time: 0.0029s\n",
      "Epoch: 0381 loss_train: 0.1286 acc_train: 1.0000 loss_val: 0.9664 acc_val: 0.6556 time: 0.0029s\n",
      "Epoch: 0382 loss_train: 0.1130 acc_train: 1.0000 loss_val: 0.9910 acc_val: 0.6333 time: 0.0029s\n",
      "Epoch: 0383 loss_train: 0.1304 acc_train: 1.0000 loss_val: 1.0216 acc_val: 0.6333 time: 0.0029s\n",
      "Epoch: 0384 loss_train: 0.1400 acc_train: 1.0000 loss_val: 1.0386 acc_val: 0.6111 time: 0.0029s\n",
      "Epoch: 0385 loss_train: 0.1791 acc_train: 0.9000 loss_val: 1.0372 acc_val: 0.6222 time: 0.0029s\n",
      "Epoch: 0386 loss_train: 0.1590 acc_train: 0.9000 loss_val: 1.0207 acc_val: 0.6333 time: 0.0029s\n",
      "Epoch: 0387 loss_train: 0.1118 acc_train: 1.0000 loss_val: 1.0130 acc_val: 0.6444 time: 0.0029s\n",
      "Epoch: 0388 loss_train: 0.2098 acc_train: 1.0000 loss_val: 1.0040 acc_val: 0.6667 time: 0.0029s\n",
      "Epoch: 0389 loss_train: 0.1634 acc_train: 1.0000 loss_val: 0.9931 acc_val: 0.6667 time: 0.0029s\n",
      "Epoch: 0390 loss_train: 0.0955 acc_train: 1.0000 loss_val: 0.9873 acc_val: 0.6556 time: 0.0029s\n",
      "Epoch: 0391 loss_train: 0.1478 acc_train: 1.0000 loss_val: 0.9871 acc_val: 0.6556 time: 0.0029s\n",
      "Epoch: 0392 loss_train: 0.2630 acc_train: 0.9000 loss_val: 0.9976 acc_val: 0.6667 time: 0.0029s\n",
      "Epoch: 0393 loss_train: 0.1297 acc_train: 1.0000 loss_val: 1.0450 acc_val: 0.6111 time: 0.0029s\n",
      "Epoch: 0394 loss_train: 0.2195 acc_train: 0.9000 loss_val: 1.0998 acc_val: 0.6222 time: 0.0029s\n",
      "Epoch: 0395 loss_train: 0.1410 acc_train: 1.0000 loss_val: 1.1221 acc_val: 0.6111 time: 0.0030s\n",
      "Epoch: 0396 loss_train: 0.2937 acc_train: 0.8000 loss_val: 1.0506 acc_val: 0.6222 time: 0.0029s\n",
      "Epoch: 0397 loss_train: 0.1748 acc_train: 0.9000 loss_val: 0.9955 acc_val: 0.6444 time: 0.0030s\n",
      "Epoch: 0398 loss_train: 0.1414 acc_train: 0.9000 loss_val: 0.9657 acc_val: 0.6556 time: 0.0030s\n",
      "Epoch: 0399 loss_train: 0.1371 acc_train: 1.0000 loss_val: 0.9709 acc_val: 0.6667 time: 0.0029s\n",
      "Epoch: 0400 loss_train: 0.1599 acc_train: 1.0000 loss_val: 0.9767 acc_val: 0.6667 time: 0.0040s\n",
      "Epoch: 0401 loss_train: 0.2151 acc_train: 1.0000 loss_val: 0.9690 acc_val: 0.6556 time: 0.0046s\n",
      "Epoch: 0402 loss_train: 0.1619 acc_train: 1.0000 loss_val: 1.0139 acc_val: 0.6222 time: 0.0037s\n",
      "Epoch: 0403 loss_train: 0.1781 acc_train: 1.0000 loss_val: 1.0695 acc_val: 0.6222 time: 0.0037s\n",
      "Epoch: 0404 loss_train: 0.1455 acc_train: 0.9000 loss_val: 1.0974 acc_val: 0.6222 time: 0.0034s\n",
      "Epoch: 0405 loss_train: 0.1859 acc_train: 0.9000 loss_val: 1.0622 acc_val: 0.6111 time: 0.0049s\n",
      "Epoch: 0406 loss_train: 0.1820 acc_train: 0.9000 loss_val: 1.0119 acc_val: 0.6556 time: 0.0050s\n",
      "Epoch: 0407 loss_train: 0.1974 acc_train: 0.9000 loss_val: 0.9997 acc_val: 0.6556 time: 0.0034s\n",
      "Epoch: 0408 loss_train: 0.1702 acc_train: 1.0000 loss_val: 1.0105 acc_val: 0.6667 time: 0.0047s\n",
      "Epoch: 0409 loss_train: 0.1390 acc_train: 1.0000 loss_val: 1.0151 acc_val: 0.6556 time: 0.0055s\n",
      "Epoch: 0410 loss_train: 0.1232 acc_train: 1.0000 loss_val: 1.0225 acc_val: 0.6667 time: 0.0036s\n",
      "Epoch: 0411 loss_train: 0.1747 acc_train: 0.9000 loss_val: 1.0618 acc_val: 0.6222 time: 0.0036s\n",
      "Epoch: 0412 loss_train: 0.1301 acc_train: 1.0000 loss_val: 1.1164 acc_val: 0.6222 time: 0.0034s\n",
      "Epoch: 0413 loss_train: 0.2234 acc_train: 0.9000 loss_val: 1.1379 acc_val: 0.6222 time: 0.0035s\n",
      "Epoch: 0414 loss_train: 0.2163 acc_train: 1.0000 loss_val: 1.1033 acc_val: 0.6111 time: 0.0056s\n",
      "Epoch: 0415 loss_train: 0.2615 acc_train: 0.9000 loss_val: 1.0584 acc_val: 0.6222 time: 0.0036s\n",
      "Epoch: 0416 loss_train: 0.1113 acc_train: 1.0000 loss_val: 1.0266 acc_val: 0.6667 time: 0.0039s\n",
      "Epoch: 0417 loss_train: 0.1896 acc_train: 0.9000 loss_val: 1.0139 acc_val: 0.6556 time: 0.0035s\n",
      "Epoch: 0418 loss_train: 0.1770 acc_train: 1.0000 loss_val: 1.0108 acc_val: 0.6556 time: 0.0037s\n",
      "Epoch: 0419 loss_train: 0.1028 acc_train: 1.0000 loss_val: 1.0089 acc_val: 0.6667 time: 0.0039s\n",
      "Epoch: 0420 loss_train: 0.1062 acc_train: 1.0000 loss_val: 1.0194 acc_val: 0.6556 time: 0.0043s\n",
      "Epoch: 0421 loss_train: 0.1428 acc_train: 1.0000 loss_val: 1.0371 acc_val: 0.6444 time: 0.0036s\n",
      "Epoch: 0422 loss_train: 0.1297 acc_train: 1.0000 loss_val: 1.0415 acc_val: 0.6444 time: 0.0036s\n",
      "Epoch: 0423 loss_train: 0.1599 acc_train: 1.0000 loss_val: 1.0367 acc_val: 0.6444 time: 0.0036s\n",
      "Epoch: 0424 loss_train: 0.2542 acc_train: 1.0000 loss_val: 1.0370 acc_val: 0.6556 time: 0.0064s\n",
      "Epoch: 0425 loss_train: 0.1448 acc_train: 0.9000 loss_val: 1.0297 acc_val: 0.6667 time: 0.0035s\n",
      "Epoch: 0426 loss_train: 0.1485 acc_train: 1.0000 loss_val: 1.0313 acc_val: 0.6667 time: 0.0036s\n",
      "Epoch: 0427 loss_train: 0.1142 acc_train: 1.0000 loss_val: 1.0326 acc_val: 0.6667 time: 0.0084s\n",
      "Epoch: 0428 loss_train: 0.0792 acc_train: 1.0000 loss_val: 1.0357 acc_val: 0.6667 time: 0.0045s\n",
      "Epoch: 0429 loss_train: 0.0804 acc_train: 1.0000 loss_val: 1.0487 acc_val: 0.6444 time: 0.0073s\n",
      "Epoch: 0430 loss_train: 0.1163 acc_train: 1.0000 loss_val: 1.0761 acc_val: 0.6333 time: 0.0084s\n",
      "Epoch: 0431 loss_train: 0.1590 acc_train: 0.9000 loss_val: 1.0836 acc_val: 0.6333 time: 0.0041s\n",
      "Epoch: 0432 loss_train: 0.0773 acc_train: 1.0000 loss_val: 1.0874 acc_val: 0.6333 time: 0.0098s\n",
      "Epoch: 0433 loss_train: 0.1862 acc_train: 0.9000 loss_val: 1.0573 acc_val: 0.6556 time: 0.0037s\n",
      "Epoch: 0434 loss_train: 0.1282 acc_train: 1.0000 loss_val: 1.0513 acc_val: 0.6667 time: 0.0091s\n",
      "Epoch: 0435 loss_train: 0.1583 acc_train: 1.0000 loss_val: 1.0456 acc_val: 0.6667 time: 0.0034s\n",
      "Epoch: 0436 loss_train: 0.1399 acc_train: 0.9000 loss_val: 1.0533 acc_val: 0.6667 time: 0.0034s\n",
      "Epoch: 0437 loss_train: 0.1167 acc_train: 1.0000 loss_val: 1.0525 acc_val: 0.6667 time: 0.0086s\n",
      "Epoch: 0438 loss_train: 0.0890 acc_train: 1.0000 loss_val: 1.0600 acc_val: 0.6556 time: 0.0044s\n",
      "Epoch: 0439 loss_train: 0.0999 acc_train: 1.0000 loss_val: 1.0809 acc_val: 0.6333 time: 0.0031s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0440 loss_train: 0.2463 acc_train: 0.9000 loss_val: 1.0771 acc_val: 0.6333 time: 0.0095s\n",
      "Epoch: 0441 loss_train: 0.1492 acc_train: 1.0000 loss_val: 1.0733 acc_val: 0.6444 time: 0.0053s\n",
      "Epoch: 0442 loss_train: 0.1477 acc_train: 1.0000 loss_val: 1.0691 acc_val: 0.6444 time: 0.0059s\n",
      "Epoch: 0443 loss_train: 0.1955 acc_train: 0.9000 loss_val: 1.0654 acc_val: 0.6444 time: 0.0069s\n",
      "Epoch: 0444 loss_train: 0.2013 acc_train: 0.9000 loss_val: 1.0630 acc_val: 0.6444 time: 0.0047s\n",
      "Epoch: 0445 loss_train: 0.1572 acc_train: 1.0000 loss_val: 1.0724 acc_val: 0.6444 time: 0.0063s\n",
      "Epoch: 0446 loss_train: 0.1260 acc_train: 1.0000 loss_val: 1.0760 acc_val: 0.6333 time: 0.0044s\n",
      "Epoch: 0447 loss_train: 0.1327 acc_train: 0.9000 loss_val: 1.0611 acc_val: 0.6556 time: 0.0036s\n",
      "Epoch: 0448 loss_train: 0.1564 acc_train: 1.0000 loss_val: 1.0545 acc_val: 0.6556 time: 0.0051s\n",
      "Epoch: 0449 loss_train: 0.0845 acc_train: 1.0000 loss_val: 1.0532 acc_val: 0.6556 time: 0.0056s\n",
      "Epoch: 0450 loss_train: 0.1607 acc_train: 1.0000 loss_val: 1.0474 acc_val: 0.6667 time: 0.0034s\n",
      "Epoch: 0451 loss_train: 0.1349 acc_train: 1.0000 loss_val: 1.0510 acc_val: 0.6556 time: 0.0035s\n",
      "Epoch: 0452 loss_train: 0.1524 acc_train: 1.0000 loss_val: 1.0602 acc_val: 0.6667 time: 0.0092s\n",
      "Epoch: 0453 loss_train: 0.1492 acc_train: 1.0000 loss_val: 1.0844 acc_val: 0.6556 time: 0.0045s\n",
      "Epoch: 0454 loss_train: 0.0720 acc_train: 1.0000 loss_val: 1.1021 acc_val: 0.6444 time: 0.0050s\n",
      "Epoch: 0455 loss_train: 0.1274 acc_train: 1.0000 loss_val: 1.1065 acc_val: 0.6444 time: 0.0055s\n",
      "Epoch: 0456 loss_train: 0.1725 acc_train: 0.9000 loss_val: 1.0892 acc_val: 0.6556 time: 0.0037s\n",
      "Epoch: 0457 loss_train: 0.1139 acc_train: 1.0000 loss_val: 1.0795 acc_val: 0.6556 time: 0.0033s\n",
      "Epoch: 0458 loss_train: 0.2008 acc_train: 0.8000 loss_val: 1.0857 acc_val: 0.6667 time: 0.0081s\n",
      "Epoch: 0459 loss_train: 0.1204 acc_train: 1.0000 loss_val: 1.1089 acc_val: 0.6556 time: 0.0051s\n",
      "Epoch: 0460 loss_train: 0.1028 acc_train: 1.0000 loss_val: 1.1432 acc_val: 0.6444 time: 0.0045s\n",
      "Epoch: 0461 loss_train: 0.0941 acc_train: 1.0000 loss_val: 1.1855 acc_val: 0.6222 time: 0.0065s\n",
      "Epoch: 0462 loss_train: 0.2364 acc_train: 0.9000 loss_val: 1.1590 acc_val: 0.6444 time: 0.0032s\n",
      "Epoch: 0463 loss_train: 0.1998 acc_train: 0.9000 loss_val: 1.1486 acc_val: 0.6444 time: 0.0031s\n",
      "Epoch: 0464 loss_train: 0.1006 acc_train: 1.0000 loss_val: 1.1435 acc_val: 0.6444 time: 0.0060s\n",
      "Epoch: 0465 loss_train: 0.1078 acc_train: 0.9000 loss_val: 1.1182 acc_val: 0.6556 time: 0.0040s\n",
      "Epoch: 0466 loss_train: 0.1293 acc_train: 1.0000 loss_val: 1.0912 acc_val: 0.6556 time: 0.0037s\n",
      "Epoch: 0467 loss_train: 0.1132 acc_train: 1.0000 loss_val: 1.0725 acc_val: 0.6667 time: 0.0055s\n",
      "Epoch: 0468 loss_train: 0.0928 acc_train: 1.0000 loss_val: 1.0671 acc_val: 0.6556 time: 0.0050s\n",
      "Epoch: 0469 loss_train: 0.1649 acc_train: 1.0000 loss_val: 1.0648 acc_val: 0.6556 time: 0.0032s\n",
      "Epoch: 0470 loss_train: 0.0982 acc_train: 1.0000 loss_val: 1.0665 acc_val: 0.6556 time: 0.0031s\n",
      "Epoch: 0471 loss_train: 0.1398 acc_train: 1.0000 loss_val: 1.0819 acc_val: 0.6556 time: 0.0058s\n",
      "Epoch: 0472 loss_train: 0.1158 acc_train: 1.0000 loss_val: 1.1054 acc_val: 0.6556 time: 0.0053s\n",
      "Epoch: 0473 loss_train: 0.1691 acc_train: 0.9000 loss_val: 1.1148 acc_val: 0.6444 time: 0.0035s\n",
      "Epoch: 0474 loss_train: 0.0942 acc_train: 1.0000 loss_val: 1.1259 acc_val: 0.6444 time: 0.0035s\n",
      "Epoch: 0475 loss_train: 0.0877 acc_train: 1.0000 loss_val: 1.1293 acc_val: 0.6444 time: 0.0066s\n",
      "Epoch: 0476 loss_train: 0.1293 acc_train: 1.0000 loss_val: 1.1163 acc_val: 0.6444 time: 0.0035s\n",
      "Epoch: 0477 loss_train: 0.1379 acc_train: 1.0000 loss_val: 1.1121 acc_val: 0.6556 time: 0.0036s\n",
      "Epoch: 0478 loss_train: 0.1812 acc_train: 0.9000 loss_val: 1.1132 acc_val: 0.6556 time: 0.0048s\n",
      "Epoch: 0479 loss_train: 0.1849 acc_train: 1.0000 loss_val: 1.1084 acc_val: 0.6556 time: 0.0049s\n",
      "Epoch: 0480 loss_train: 0.1078 acc_train: 1.0000 loss_val: 1.1110 acc_val: 0.6556 time: 0.0030s\n",
      "Epoch: 0481 loss_train: 0.1208 acc_train: 1.0000 loss_val: 1.1235 acc_val: 0.6556 time: 0.0054s\n",
      "Epoch: 0482 loss_train: 0.0990 acc_train: 1.0000 loss_val: 1.1270 acc_val: 0.6556 time: 0.0064s\n",
      "Epoch: 0483 loss_train: 0.1234 acc_train: 1.0000 loss_val: 1.1237 acc_val: 0.6556 time: 0.0036s\n",
      "Epoch: 0484 loss_train: 0.0726 acc_train: 1.0000 loss_val: 1.1290 acc_val: 0.6556 time: 0.0058s\n",
      "Epoch: 0485 loss_train: 0.1932 acc_train: 0.9000 loss_val: 1.1657 acc_val: 0.6556 time: 0.0035s\n",
      "Epoch: 0486 loss_train: 0.1292 acc_train: 0.9000 loss_val: 1.1998 acc_val: 0.6444 time: 0.0080s\n",
      "Epoch: 0487 loss_train: 0.1332 acc_train: 1.0000 loss_val: 1.1861 acc_val: 0.6444 time: 0.0036s\n",
      "Epoch: 0488 loss_train: 0.1202 acc_train: 1.0000 loss_val: 1.1742 acc_val: 0.6444 time: 0.0054s\n",
      "Epoch: 0489 loss_train: 0.0760 acc_train: 1.0000 loss_val: 1.1746 acc_val: 0.6444 time: 0.0031s\n",
      "Epoch: 0490 loss_train: 0.0693 acc_train: 1.0000 loss_val: 1.1829 acc_val: 0.6444 time: 0.0030s\n",
      "Epoch: 0491 loss_train: 0.1205 acc_train: 0.9000 loss_val: 1.1626 acc_val: 0.6556 time: 0.0030s\n",
      "Epoch: 0492 loss_train: 0.1561 acc_train: 1.0000 loss_val: 1.1347 acc_val: 0.6556 time: 0.0031s\n",
      "Epoch: 0493 loss_train: 0.2226 acc_train: 0.9000 loss_val: 1.1068 acc_val: 0.6667 time: 0.0050s\n",
      "Epoch: 0494 loss_train: 0.2163 acc_train: 0.8000 loss_val: 1.0956 acc_val: 0.6556 time: 0.0069s\n",
      "Epoch: 0495 loss_train: 0.1639 acc_train: 0.9000 loss_val: 1.1025 acc_val: 0.6556 time: 0.0037s\n",
      "Epoch: 0496 loss_train: 0.1084 acc_train: 1.0000 loss_val: 1.1330 acc_val: 0.6444 time: 0.0039s\n",
      "Epoch: 0497 loss_train: 0.1314 acc_train: 0.9000 loss_val: 1.1599 acc_val: 0.6222 time: 0.0052s\n",
      "Epoch: 0498 loss_train: 0.0837 acc_train: 1.0000 loss_val: 1.1822 acc_val: 0.6222 time: 0.0043s\n",
      "Epoch: 0499 loss_train: 0.1969 acc_train: 0.9000 loss_val: 1.1293 acc_val: 0.6333 time: 0.0059s\n",
      "Epoch: 0500 loss_train: 0.1409 acc_train: 0.9000 loss_val: 1.0650 acc_val: 0.6667 time: 0.0045s\n",
      "Optimization Finished!\n",
      "Total time elapsed: 2.1658s\n",
      "Test set results: loss= 1.1017 accuracy= 0.6727\n"
     ]
    }
   ],
   "source": [
    "cuda = False\n",
    "hidden = 16\n",
    "dropout = 0.5\n",
    "lr = 0.01\n",
    "weight_decay = 5e-4\n",
    "fastmode = False\n",
    "epochs = 500\n",
    "\n",
    "# Model and optimizer\n",
    "model = GCN(nfeat=features.shape[1],\n",
    "            nhid=hidden,\n",
    "            nclass=labels.max().item() + 1,\n",
    "            dropout=dropout)\n",
    "optimizer = optim.Adam(model.parameters(),\n",
    "                       lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "if cuda:\n",
    "    model.cuda()\n",
    "    features = features.cuda()\n",
    "    adj = adj.cuda()\n",
    "    labels = labels.cuda()\n",
    "    idx_train = idx_train.cuda()\n",
    "    idx_val = idx_val.cuda()\n",
    "    idx_test = idx_test.cuda()\n",
    "\n",
    "\n",
    "def train(epoch):\n",
    "    t = time.time()\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(features, adj)\n",
    "    loss_train = F.nll_loss(output[idx_train], all_labels[idx_train])\n",
    "    acc_train = accuracy(output[idx_train], all_labels[idx_train])\n",
    "    loss_train.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if not fastmode:\n",
    "        # Evaluate validation set performance separately,\n",
    "        # deactivates dropout during validation run.\n",
    "        model.eval()\n",
    "        output = model(features, adj)\n",
    "\n",
    "    loss_val = F.nll_loss(output[idx_val], all_labels[idx_val])\n",
    "    acc_val = accuracy(output[idx_val], all_labels[idx_val])\n",
    "    print('Epoch: {:04d}'.format(epoch+1),\n",
    "          'loss_train: {:.4f}'.format(loss_train.item()),\n",
    "          'acc_train: {:.4f}'.format(acc_train.item()),\n",
    "          'loss_val: {:.4f}'.format(loss_val.item()),\n",
    "          'acc_val: {:.4f}'.format(acc_val.item()),\n",
    "          'time: {:.4f}s'.format(time.time() - t))\n",
    "\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    output = model(features, adj)\n",
    "    loss_test = F.nll_loss(output[idx_test], all_labels[idx_test])\n",
    "    acc_test = accuracy(output[idx_test], all_labels[idx_test])\n",
    "    print(\"Test set results:\",\n",
    "          \"loss= {:.4f}\".format(loss_test.item()),\n",
    "          \"accuracy= {:.4f}\".format(acc_test.item()))\n",
    "\n",
    "\n",
    "# Train model\n",
    "t_total = time.time()\n",
    "for epoch in range(epochs):\n",
    "    train(epoch)\n",
    "print(\"Optimization Finished!\")\n",
    "print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))\n",
    "\n",
    "# Testing\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
