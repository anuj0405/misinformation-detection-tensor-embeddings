{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.ArticlesHandler import ArticlesHandler\n",
    "from utils import solve, embedding_matrix_2_kNN, get_rate, accuracy, precision, recall, f1_score\n",
    "from utils import Config\n",
    "import time\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "from postprocessing.SelectLabelsPostprocessor import SelectLabelsPostprocessor\n",
    "from pygcn.utils import encode_onehot, load_from_features, accuracy\n",
    "from pygcn.models import GCN\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import config file and check some values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method of decomposition: parafac\n"
     ]
    }
   ],
   "source": [
    "config = Config(file='config')\n",
    "\n",
    "assert (config.num_fake_articles + config.num_real_articles > \n",
    "        config.num_nearest_neighbours), \"Can't have more neighbours than nodes!\"\n",
    "\n",
    "print(\"Method of decomposition:\", config.method_decomposition_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the articles and decompose the tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset Buzzfeed Political News Dataset\n",
      "Performing decomposition...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/run/media/bdvllrs/Data/Documents/Supelec/OMA/Projet/FakeNews/build_matrix/tensorly/decomposition/_tucker.py:60: Warning: Given only one int for 'rank' intead of a list of 3 modes. Using this rank for all modes.\n",
      "  warnings.warn(message, Warning)\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading dataset\", config.dataset_name)\n",
    "articles = ArticlesHandler(config)\n",
    "\n",
    "print(\"Performing decomposition...\")\n",
    "C = articles.get_tensor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.set(\"num_unknown_labels\", 195)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = articles.articles.labels\n",
    "all_labels = articles.articles.labels_untouched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj, features, all_labels = load_from_features(C, all_labels, config)\n",
    "_, _, labels = load_from_features(C, labels, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 2, 1, 2, 1, 2])\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "# idx_train = range(150)\n",
    "# idx_val = range(150, 175)\n",
    "# idx_test = range(175, 200)\n",
    "print(labels)\n",
    "idx_train = np.where(labels)[0]\n",
    "idx_val = np.where(1 - abs(labels))[0][:90]\n",
    "idx_test = np.where(1 - abs(labels))[0][90:]\n",
    "\n",
    "print(len(idx_train))\n",
    "\n",
    "idx_train = torch.LongTensor(idx_train)\n",
    "idx_val = torch.LongTensor(idx_val)\n",
    "idx_test = torch.LongTensor(idx_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 loss_train: 1.2130 acc_train: 0.2000 loss_val: 1.8251 acc_val: 0.0222 time: 0.0020s\n",
      "Epoch: 0002 loss_train: 1.1146 acc_train: 0.2000 loss_val: 1.8305 acc_val: 0.0333 time: 0.0023s\n",
      "Epoch: 0003 loss_train: 1.0282 acc_train: 0.2000 loss_val: 1.8341 acc_val: 0.0333 time: 0.0024s\n",
      "Epoch: 0004 loss_train: 0.9751 acc_train: 0.2000 loss_val: 1.8525 acc_val: 0.0333 time: 0.0022s\n",
      "Epoch: 0005 loss_train: 1.4190 acc_train: 0.2000 loss_val: 1.8542 acc_val: 0.0667 time: 0.0020s\n",
      "Epoch: 0006 loss_train: 0.8860 acc_train: 0.4000 loss_val: 1.8623 acc_val: 0.0889 time: 0.0024s\n",
      "Epoch: 0007 loss_train: 0.7977 acc_train: 0.6000 loss_val: 1.8735 acc_val: 0.1556 time: 0.0024s\n",
      "Epoch: 0008 loss_train: 0.7891 acc_train: 0.6000 loss_val: 1.8866 acc_val: 0.1889 time: 0.0024s\n",
      "Epoch: 0009 loss_train: 1.9639 acc_train: 0.2000 loss_val: 1.8836 acc_val: 0.2444 time: 0.0028s\n",
      "Epoch: 0010 loss_train: 0.8002 acc_train: 0.6000 loss_val: 1.8824 acc_val: 0.3111 time: 0.0028s\n",
      "Epoch: 0011 loss_train: 0.7446 acc_train: 0.6000 loss_val: 1.8822 acc_val: 0.3222 time: 0.0038s\n",
      "Epoch: 0012 loss_train: 0.6765 acc_train: 0.6000 loss_val: 1.8828 acc_val: 0.3444 time: 0.0033s\n",
      "Epoch: 0013 loss_train: 0.8047 acc_train: 0.6000 loss_val: 1.8826 acc_val: 0.3667 time: 0.0025s\n",
      "Epoch: 0014 loss_train: 0.7209 acc_train: 0.6000 loss_val: 1.8837 acc_val: 0.4556 time: 0.0021s\n",
      "Epoch: 0015 loss_train: 0.6223 acc_train: 0.6000 loss_val: 1.8865 acc_val: 0.4667 time: 0.0023s\n",
      "Epoch: 0016 loss_train: 0.6007 acc_train: 0.6000 loss_val: 1.8952 acc_val: 0.5000 time: 0.0021s\n",
      "Epoch: 0017 loss_train: 0.5820 acc_train: 0.6000 loss_val: 1.9037 acc_val: 0.4667 time: 0.0023s\n",
      "Epoch: 0018 loss_train: 0.5591 acc_train: 0.6000 loss_val: 1.9173 acc_val: 0.4444 time: 0.0027s\n",
      "Epoch: 0019 loss_train: 0.5827 acc_train: 0.8000 loss_val: 1.9340 acc_val: 0.4889 time: 0.0024s\n",
      "Epoch: 0020 loss_train: 0.5113 acc_train: 0.8000 loss_val: 1.9512 acc_val: 0.5111 time: 0.0024s\n",
      "Epoch: 0021 loss_train: 0.5583 acc_train: 0.6000 loss_val: 1.9737 acc_val: 0.5111 time: 0.0027s\n",
      "Epoch: 0022 loss_train: 0.4752 acc_train: 0.8000 loss_val: 1.9994 acc_val: 0.5111 time: 0.0025s\n",
      "Epoch: 0023 loss_train: 0.4500 acc_train: 1.0000 loss_val: 2.0276 acc_val: 0.4889 time: 0.0024s\n",
      "Epoch: 0024 loss_train: 0.4355 acc_train: 1.0000 loss_val: 2.0569 acc_val: 0.4667 time: 0.0024s\n",
      "Epoch: 0025 loss_train: 0.4378 acc_train: 0.8000 loss_val: 2.0882 acc_val: 0.4889 time: 0.0023s\n",
      "Epoch: 0026 loss_train: 0.4183 acc_train: 1.0000 loss_val: 2.1300 acc_val: 0.4889 time: 0.0020s\n",
      "Epoch: 0027 loss_train: 0.4201 acc_train: 1.0000 loss_val: 2.1797 acc_val: 0.5333 time: 0.0025s\n",
      "Epoch: 0028 loss_train: 0.3867 acc_train: 1.0000 loss_val: 2.2358 acc_val: 0.5333 time: 0.0024s\n",
      "Epoch: 0029 loss_train: 0.3485 acc_train: 1.0000 loss_val: 2.2950 acc_val: 0.5333 time: 0.0027s\n",
      "Epoch: 0030 loss_train: 0.3609 acc_train: 1.0000 loss_val: 2.3584 acc_val: 0.5333 time: 0.0025s\n",
      "Epoch: 0031 loss_train: 0.3986 acc_train: 1.0000 loss_val: 2.4351 acc_val: 0.5333 time: 0.0023s\n",
      "Epoch: 0032 loss_train: 0.3200 acc_train: 1.0000 loss_val: 2.5171 acc_val: 0.5556 time: 0.0021s\n",
      "Epoch: 0033 loss_train: 0.3205 acc_train: 1.0000 loss_val: 2.6035 acc_val: 0.5333 time: 0.0020s\n",
      "Epoch: 0034 loss_train: 0.3364 acc_train: 1.0000 loss_val: 2.6986 acc_val: 0.5333 time: 0.0020s\n",
      "Epoch: 0035 loss_train: 0.3276 acc_train: 1.0000 loss_val: 2.7980 acc_val: 0.5333 time: 0.0025s\n",
      "Epoch: 0036 loss_train: 0.2770 acc_train: 1.0000 loss_val: 2.8983 acc_val: 0.5111 time: 0.0023s\n",
      "Epoch: 0037 loss_train: 0.3513 acc_train: 1.0000 loss_val: 3.0109 acc_val: 0.5111 time: 0.0021s\n",
      "Epoch: 0038 loss_train: 0.2070 acc_train: 1.0000 loss_val: 3.1265 acc_val: 0.5111 time: 0.0021s\n",
      "Epoch: 0039 loss_train: 0.2420 acc_train: 1.0000 loss_val: 3.2492 acc_val: 0.5111 time: 0.0020s\n",
      "Epoch: 0040 loss_train: 0.3430 acc_train: 1.0000 loss_val: 3.3833 acc_val: 0.5111 time: 0.0020s\n",
      "Epoch: 0041 loss_train: 0.2065 acc_train: 1.0000 loss_val: 3.5210 acc_val: 0.5111 time: 0.0020s\n",
      "Epoch: 0042 loss_train: 0.1767 acc_train: 1.0000 loss_val: 3.6611 acc_val: 0.5111 time: 0.0021s\n",
      "Epoch: 0043 loss_train: 0.1450 acc_train: 1.0000 loss_val: 3.8070 acc_val: 0.5111 time: 0.0021s\n",
      "Epoch: 0044 loss_train: 0.2011 acc_train: 1.0000 loss_val: 3.9530 acc_val: 0.5000 time: 0.0021s\n",
      "Epoch: 0045 loss_train: 0.1100 acc_train: 1.0000 loss_val: 4.0989 acc_val: 0.5000 time: 0.0020s\n",
      "Epoch: 0046 loss_train: 0.1072 acc_train: 1.0000 loss_val: 4.2516 acc_val: 0.5000 time: 0.0020s\n",
      "Epoch: 0047 loss_train: 0.0742 acc_train: 1.0000 loss_val: 4.4014 acc_val: 0.5000 time: 0.0020s\n",
      "Epoch: 0048 loss_train: 0.1604 acc_train: 1.0000 loss_val: 4.5604 acc_val: 0.5111 time: 0.0022s\n",
      "Epoch: 0049 loss_train: 0.1920 acc_train: 1.0000 loss_val: 4.7184 acc_val: 0.5111 time: 0.0024s\n",
      "Epoch: 0050 loss_train: 0.0831 acc_train: 1.0000 loss_val: 4.8744 acc_val: 0.5111 time: 0.0025s\n",
      "Epoch: 0051 loss_train: 0.1141 acc_train: 1.0000 loss_val: 5.0277 acc_val: 0.5111 time: 0.0027s\n",
      "Epoch: 0052 loss_train: 0.0777 acc_train: 1.0000 loss_val: 5.1781 acc_val: 0.5111 time: 0.0023s\n",
      "Epoch: 0053 loss_train: 0.0934 acc_train: 1.0000 loss_val: 5.3234 acc_val: 0.5111 time: 0.0023s\n",
      "Epoch: 0054 loss_train: 0.0785 acc_train: 1.0000 loss_val: 5.4667 acc_val: 0.5111 time: 0.0020s\n",
      "Epoch: 0055 loss_train: 0.1512 acc_train: 1.0000 loss_val: 5.6182 acc_val: 0.5111 time: 0.0020s\n",
      "Epoch: 0056 loss_train: 0.0982 acc_train: 1.0000 loss_val: 5.7688 acc_val: 0.5111 time: 0.0020s\n",
      "Epoch: 0057 loss_train: 0.0810 acc_train: 1.0000 loss_val: 5.9200 acc_val: 0.5111 time: 0.0020s\n",
      "Epoch: 0058 loss_train: 0.0923 acc_train: 1.0000 loss_val: 6.0710 acc_val: 0.5111 time: 0.0021s\n",
      "Epoch: 0059 loss_train: 0.0368 acc_train: 1.0000 loss_val: 6.2170 acc_val: 0.5111 time: 0.0022s\n",
      "Epoch: 0060 loss_train: 0.0422 acc_train: 1.0000 loss_val: 6.3569 acc_val: 0.5111 time: 0.0022s\n",
      "Epoch: 0061 loss_train: 0.1307 acc_train: 1.0000 loss_val: 6.4915 acc_val: 0.5111 time: 0.0023s\n",
      "Epoch: 0062 loss_train: 0.0361 acc_train: 1.0000 loss_val: 6.6218 acc_val: 0.5111 time: 0.0023s\n",
      "Epoch: 0063 loss_train: 0.0699 acc_train: 1.0000 loss_val: 6.7495 acc_val: 0.5111 time: 0.0023s\n",
      "Epoch: 0064 loss_train: 0.0538 acc_train: 1.0000 loss_val: 6.8771 acc_val: 0.5111 time: 0.0023s\n",
      "Epoch: 0065 loss_train: 0.0188 acc_train: 1.0000 loss_val: 6.9993 acc_val: 0.5111 time: 0.0023s\n",
      "Epoch: 0066 loss_train: 0.0957 acc_train: 1.0000 loss_val: 7.1306 acc_val: 0.5111 time: 0.0023s\n",
      "Epoch: 0067 loss_train: 0.0208 acc_train: 1.0000 loss_val: 7.2563 acc_val: 0.5222 time: 0.0023s\n",
      "Epoch: 0068 loss_train: 0.0562 acc_train: 1.0000 loss_val: 7.3856 acc_val: 0.5333 time: 0.0023s\n",
      "Epoch: 0069 loss_train: 0.0269 acc_train: 1.0000 loss_val: 7.5101 acc_val: 0.5333 time: 0.0023s\n",
      "Epoch: 0070 loss_train: 0.0276 acc_train: 1.0000 loss_val: 7.6334 acc_val: 0.5333 time: 0.0023s\n",
      "Epoch: 0071 loss_train: 0.0159 acc_train: 1.0000 loss_val: 7.7502 acc_val: 0.5333 time: 0.0023s\n",
      "Epoch: 0072 loss_train: 0.0283 acc_train: 1.0000 loss_val: 7.8620 acc_val: 0.5333 time: 0.0020s\n",
      "Epoch: 0073 loss_train: 0.0128 acc_train: 1.0000 loss_val: 7.9675 acc_val: 0.5333 time: 0.0020s\n",
      "Epoch: 0074 loss_train: 0.0138 acc_train: 1.0000 loss_val: 8.0669 acc_val: 0.5333 time: 0.0020s\n",
      "Epoch: 0075 loss_train: 0.0143 acc_train: 1.0000 loss_val: 8.1619 acc_val: 0.5333 time: 0.0022s\n",
      "Epoch: 0076 loss_train: 0.0262 acc_train: 1.0000 loss_val: 8.2543 acc_val: 0.5333 time: 0.0021s\n",
      "Epoch: 0077 loss_train: 0.0378 acc_train: 1.0000 loss_val: 8.3490 acc_val: 0.5333 time: 0.0020s\n",
      "Epoch: 0078 loss_train: 0.0371 acc_train: 1.0000 loss_val: 8.4544 acc_val: 0.5111 time: 0.0020s\n",
      "Epoch: 0079 loss_train: 0.0206 acc_train: 1.0000 loss_val: 8.5565 acc_val: 0.5111 time: 0.0020s\n",
      "Epoch: 0080 loss_train: 0.0168 acc_train: 1.0000 loss_val: 8.6547 acc_val: 0.5111 time: 0.0023s\n",
      "Epoch: 0081 loss_train: 0.0254 acc_train: 1.0000 loss_val: 8.7514 acc_val: 0.5111 time: 0.0027s\n",
      "Epoch: 0082 loss_train: 0.0201 acc_train: 1.0000 loss_val: 8.8478 acc_val: 0.5111 time: 0.0025s\n",
      "Epoch: 0083 loss_train: 0.0156 acc_train: 1.0000 loss_val: 8.9401 acc_val: 0.5111 time: 0.0025s\n",
      "Epoch: 0084 loss_train: 0.0177 acc_train: 1.0000 loss_val: 9.0285 acc_val: 0.5111 time: 0.0023s\n",
      "Epoch: 0085 loss_train: 0.0204 acc_train: 1.0000 loss_val: 9.1160 acc_val: 0.5111 time: 0.0022s\n",
      "Epoch: 0086 loss_train: 0.0245 acc_train: 1.0000 loss_val: 9.2014 acc_val: 0.5111 time: 0.0023s\n",
      "Epoch: 0087 loss_train: 0.0317 acc_train: 1.0000 loss_val: 9.2905 acc_val: 0.5111 time: 0.0020s\n",
      "Epoch: 0088 loss_train: 0.0137 acc_train: 1.0000 loss_val: 9.3770 acc_val: 0.5111 time: 0.0019s\n",
      "Epoch: 0089 loss_train: 0.0356 acc_train: 1.0000 loss_val: 9.4684 acc_val: 0.5111 time: 0.0021s\n",
      "Epoch: 0090 loss_train: 0.0034 acc_train: 1.0000 loss_val: 9.5526 acc_val: 0.5111 time: 0.0019s\n",
      "Epoch: 0091 loss_train: 0.0054 acc_train: 1.0000 loss_val: 9.6317 acc_val: 0.5111 time: 0.0019s\n",
      "Epoch: 0092 loss_train: 0.0211 acc_train: 1.0000 loss_val: 9.7107 acc_val: 0.5111 time: 0.0019s\n",
      "Epoch: 0093 loss_train: 0.0221 acc_train: 1.0000 loss_val: 9.7891 acc_val: 0.5111 time: 0.0023s\n",
      "Epoch: 0094 loss_train: 0.0226 acc_train: 1.0000 loss_val: 9.8725 acc_val: 0.5111 time: 0.0035s\n",
      "Epoch: 0095 loss_train: 0.0060 acc_train: 1.0000 loss_val: 9.9515 acc_val: 0.5111 time: 0.0027s\n",
      "Epoch: 0096 loss_train: 0.0268 acc_train: 1.0000 loss_val: 10.0327 acc_val: 0.5111 time: 0.0020s\n",
      "Epoch: 0097 loss_train: 0.0107 acc_train: 1.0000 loss_val: 10.1102 acc_val: 0.5111 time: 0.0021s\n",
      "Epoch: 0098 loss_train: 0.0150 acc_train: 1.0000 loss_val: 10.1884 acc_val: 0.5111 time: 0.0024s\n",
      "Epoch: 0099 loss_train: 0.0095 acc_train: 1.0000 loss_val: 10.2628 acc_val: 0.5111 time: 0.0030s\n",
      "Epoch: 0100 loss_train: 0.0150 acc_train: 1.0000 loss_val: 10.3363 acc_val: 0.5111 time: 0.0033s\n",
      "Epoch: 0101 loss_train: 0.0346 acc_train: 1.0000 loss_val: 10.4152 acc_val: 0.5111 time: 0.0038s\n",
      "Epoch: 0102 loss_train: 0.0299 acc_train: 1.0000 loss_val: 10.4954 acc_val: 0.5111 time: 0.0034s\n",
      "Epoch: 0103 loss_train: 0.0128 acc_train: 1.0000 loss_val: 10.5716 acc_val: 0.5111 time: 0.0026s\n",
      "Epoch: 0104 loss_train: 0.0065 acc_train: 1.0000 loss_val: 10.6430 acc_val: 0.5111 time: 0.0027s\n",
      "Epoch: 0105 loss_train: 0.0075 acc_train: 1.0000 loss_val: 10.7125 acc_val: 0.5111 time: 0.0022s\n",
      "Epoch: 0106 loss_train: 0.0066 acc_train: 1.0000 loss_val: 10.7792 acc_val: 0.5222 time: 0.0023s\n",
      "Epoch: 0107 loss_train: 0.1201 acc_train: 1.0000 loss_val: 10.8455 acc_val: 0.5333 time: 0.0023s\n",
      "Epoch: 0108 loss_train: 0.0511 acc_train: 1.0000 loss_val: 10.9143 acc_val: 0.5333 time: 0.0025s\n",
      "Epoch: 0109 loss_train: 0.0048 acc_train: 1.0000 loss_val: 10.9789 acc_val: 0.5333 time: 0.0023s\n",
      "Epoch: 0110 loss_train: 0.0131 acc_train: 1.0000 loss_val: 11.0413 acc_val: 0.5333 time: 0.0023s\n",
      "Epoch: 0111 loss_train: 0.0196 acc_train: 1.0000 loss_val: 11.1048 acc_val: 0.5333 time: 0.0023s\n",
      "Epoch: 0112 loss_train: 0.0828 acc_train: 1.0000 loss_val: 11.1707 acc_val: 0.5333 time: 0.0023s\n",
      "Epoch: 0113 loss_train: 0.0067 acc_train: 1.0000 loss_val: 11.2334 acc_val: 0.5333 time: 0.0021s\n",
      "Epoch: 0114 loss_train: 0.0137 acc_train: 1.0000 loss_val: 11.2954 acc_val: 0.5333 time: 0.0020s\n",
      "Epoch: 0115 loss_train: 0.0068 acc_train: 1.0000 loss_val: 11.3535 acc_val: 0.5333 time: 0.0020s\n",
      "Epoch: 0116 loss_train: 0.0097 acc_train: 1.0000 loss_val: 11.4109 acc_val: 0.5333 time: 0.0019s\n",
      "Epoch: 0117 loss_train: 0.0029 acc_train: 1.0000 loss_val: 11.4639 acc_val: 0.5333 time: 0.0020s\n",
      "Epoch: 0118 loss_train: 0.0044 acc_train: 1.0000 loss_val: 11.5142 acc_val: 0.5333 time: 0.0020s\n",
      "Epoch: 0119 loss_train: 0.0154 acc_train: 1.0000 loss_val: 11.5709 acc_val: 0.5333 time: 0.0021s\n",
      "Epoch: 0120 loss_train: 0.0081 acc_train: 1.0000 loss_val: 11.6258 acc_val: 0.5333 time: 0.0022s\n",
      "Epoch: 0121 loss_train: 0.0055 acc_train: 1.0000 loss_val: 11.6786 acc_val: 0.5333 time: 0.0021s\n",
      "Epoch: 0122 loss_train: 0.0093 acc_train: 1.0000 loss_val: 11.7292 acc_val: 0.5333 time: 0.0020s\n",
      "Epoch: 0123 loss_train: 0.0018 acc_train: 1.0000 loss_val: 11.7756 acc_val: 0.5333 time: 0.0021s\n",
      "Epoch: 0124 loss_train: 0.0032 acc_train: 1.0000 loss_val: 11.8191 acc_val: 0.5333 time: 0.0025s\n",
      "Epoch: 0125 loss_train: 0.0085 acc_train: 1.0000 loss_val: 11.8620 acc_val: 0.5333 time: 0.0024s\n",
      "Epoch: 0126 loss_train: 0.0035 acc_train: 1.0000 loss_val: 11.9022 acc_val: 0.5333 time: 0.0023s\n",
      "Epoch: 0127 loss_train: 0.0816 acc_train: 1.0000 loss_val: 11.9438 acc_val: 0.5333 time: 0.0022s\n",
      "Epoch: 0128 loss_train: 0.0057 acc_train: 1.0000 loss_val: 11.9837 acc_val: 0.5333 time: 0.0023s\n",
      "Epoch: 0129 loss_train: 0.0035 acc_train: 1.0000 loss_val: 12.0209 acc_val: 0.5333 time: 0.0023s\n",
      "Epoch: 0130 loss_train: 0.0052 acc_train: 1.0000 loss_val: 12.0568 acc_val: 0.5333 time: 0.0020s\n",
      "Epoch: 0131 loss_train: 0.0063 acc_train: 1.0000 loss_val: 12.0910 acc_val: 0.5333 time: 0.0020s\n",
      "Epoch: 0132 loss_train: 0.0064 acc_train: 1.0000 loss_val: 12.1249 acc_val: 0.5333 time: 0.0020s\n",
      "Epoch: 0133 loss_train: 0.0103 acc_train: 1.0000 loss_val: 12.1606 acc_val: 0.5333 time: 0.0020s\n",
      "Epoch: 0134 loss_train: 0.0025 acc_train: 1.0000 loss_val: 12.1936 acc_val: 0.5333 time: 0.0020s\n",
      "Epoch: 0135 loss_train: 0.0433 acc_train: 1.0000 loss_val: 12.2429 acc_val: 0.5333 time: 0.0021s\n",
      "Epoch: 0136 loss_train: 0.0211 acc_train: 1.0000 loss_val: 12.2939 acc_val: 0.5333 time: 0.0022s\n",
      "Epoch: 0137 loss_train: 0.0055 acc_train: 1.0000 loss_val: 12.3431 acc_val: 0.5333 time: 0.0020s\n",
      "Epoch: 0138 loss_train: 0.0066 acc_train: 1.0000 loss_val: 12.3894 acc_val: 0.5333 time: 0.0028s\n",
      "Epoch: 0139 loss_train: 0.0086 acc_train: 1.0000 loss_val: 12.4356 acc_val: 0.5333 time: 0.0019s\n",
      "Epoch: 0140 loss_train: 0.0070 acc_train: 1.0000 loss_val: 12.4797 acc_val: 0.5333 time: 0.0020s\n",
      "Epoch: 0141 loss_train: 0.0029 acc_train: 1.0000 loss_val: 12.5203 acc_val: 0.5333 time: 0.0019s\n",
      "Epoch: 0142 loss_train: 0.0084 acc_train: 1.0000 loss_val: 12.5601 acc_val: 0.5333 time: 0.0020s\n",
      "Epoch: 0143 loss_train: 0.0131 acc_train: 1.0000 loss_val: 12.6004 acc_val: 0.5333 time: 0.0020s\n",
      "Epoch: 0144 loss_train: 0.0066 acc_train: 1.0000 loss_val: 12.6383 acc_val: 0.5333 time: 0.0020s\n",
      "Epoch: 0145 loss_train: 0.0011 acc_train: 1.0000 loss_val: 12.6722 acc_val: 0.5333 time: 0.0019s\n",
      "Epoch: 0146 loss_train: 0.0381 acc_train: 1.0000 loss_val: 12.7172 acc_val: 0.5333 time: 0.0022s\n",
      "Epoch: 0147 loss_train: 0.0095 acc_train: 1.0000 loss_val: 12.7604 acc_val: 0.5333 time: 0.0020s\n",
      "Epoch: 0148 loss_train: 0.0094 acc_train: 1.0000 loss_val: 12.8014 acc_val: 0.5333 time: 0.0020s\n",
      "Epoch: 0149 loss_train: 0.0009 acc_train: 1.0000 loss_val: 12.8376 acc_val: 0.5333 time: 0.0020s\n",
      "Epoch: 0150 loss_train: 0.0063 acc_train: 1.0000 loss_val: 12.8728 acc_val: 0.5333 time: 0.0020s\n",
      "Optimization Finished!\n",
      "Total time elapsed: 0.3632s\n",
      "Test set results: loss= 26.0648 accuracy= 0.5926\n"
     ]
    }
   ],
   "source": [
    "cuda = False\n",
    "hidden = 16\n",
    "dropout = 0.5\n",
    "lr = 0.01\n",
    "weight_decay = 5e-4\n",
    "fastmode = False\n",
    "epochs = 150\n",
    "\n",
    "# Model and optimizer\n",
    "model = GCN(nfeat=features.shape[1],\n",
    "            nhid=hidden,\n",
    "            nclass=labels.max().item() + 1,\n",
    "            dropout=dropout)\n",
    "optimizer = optim.Adam(model.parameters(),\n",
    "                       lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "if cuda:\n",
    "    model.cuda()\n",
    "    features = features.cuda()\n",
    "    adj = adj.cuda()\n",
    "    labels = labels.cuda()\n",
    "    idx_train = idx_train.cuda()\n",
    "    idx_val = idx_val.cuda()\n",
    "    idx_test = idx_test.cuda()\n",
    "\n",
    "\n",
    "def train(epoch):\n",
    "    t = time.time()\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(features, adj)\n",
    "    loss_train = F.nll_loss(output[idx_train], all_labels[idx_train])\n",
    "    acc_train = accuracy(output[idx_train], all_labels[idx_train])\n",
    "    loss_train.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if not fastmode:\n",
    "        # Evaluate validation set performance separately,\n",
    "        # deactivates dropout during validation run.\n",
    "        model.eval()\n",
    "        output = model(features, adj)\n",
    "\n",
    "    loss_val = F.nll_loss(output[idx_val], all_labels[idx_val])\n",
    "    acc_val = accuracy(output[idx_val], all_labels[idx_val])\n",
    "    print('Epoch: {:04d}'.format(epoch+1),\n",
    "          'loss_train: {:.4f}'.format(loss_train.item()),\n",
    "          'acc_train: {:.4f}'.format(acc_train.item()),\n",
    "          'loss_val: {:.4f}'.format(loss_val.item()),\n",
    "          'acc_val: {:.4f}'.format(acc_val.item()),\n",
    "          'time: {:.4f}s'.format(time.time() - t))\n",
    "\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    output = model(features, adj)\n",
    "    loss_test = F.nll_loss(output[idx_test], all_labels[idx_test])\n",
    "    acc_test = accuracy(output[idx_test], all_labels[idx_test])\n",
    "    print(\"Test set results:\",\n",
    "          \"loss= {:.4f}\".format(loss_test.item()),\n",
    "          \"accuracy= {:.4f}\".format(acc_test.item()))\n",
    "\n",
    "\n",
    "# Train model\n",
    "t_total = time.time()\n",
    "for epoch in range(epochs):\n",
    "    train(epoch)\n",
    "print(\"Optimization Finished!\")\n",
    "print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))\n",
    "\n",
    "# Testing\n",
    "test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
