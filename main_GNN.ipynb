{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.ArticlesHandler import ArticlesHandler\n",
    "from utils import solve, embedding_matrix_2_kNN, get_rate, accuracy, precision, recall, f1_score\n",
    "from utils import Config\n",
    "import time\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "from postprocessing.SelectLabelsPostprocessor import SelectLabelsPostprocessor\n",
    "from pygcn.utils import encode_onehot, load_from_features, accuracy\n",
    "from pygcn.models import GCN\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import config file and check some values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method of decomposition: parafac\n"
     ]
    }
   ],
   "source": [
    "config = Config(file='config')\n",
    "\n",
    "assert (config.num_fake_articles + config.num_real_articles > \n",
    "        config.num_nearest_neighbours), \"Can't have more neighbours than nodes!\"\n",
    "\n",
    "print(\"Method of decomposition:\", config.method_decomposition_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the articles and decompose the tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset Random Poltical News Dataset\n",
      "Performing decomposition...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/benamira/19793564030D4273/MCsBackup/3A/OMA/Projet/misinformation-detection-tensor-embeddings/tensorly/decomposition/_tucker.py:60: Warning: Given only one int for 'rank' intead of a list of 3 modes. Using this rank for all modes.\n",
      "  warnings.warn(message, Warning)\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading dataset\", config.dataset_name)\n",
    "articles = ArticlesHandler(config)\n",
    "\n",
    "print(\"Performing decomposition...\")\n",
    "C = articles.get_tensor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.set(\"num_unknown_labels\", 195)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = articles.articles.labels\n",
    "all_labels = articles.articles.labels_untouched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj, features, all_labels = load_from_features(C, all_labels, config)\n",
    "_, _, labels = load_from_features(C, labels, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2,\n",
      "        2, 0, 0, 2, 0, 0, 2, 0, 2, 1, 2, 2, 2, 2, 1, 1, 2, 2, 1, 1, 1, 2, 1, 1,\n",
      "        1, 1, 1, 1, 2, 1, 2, 1, 1, 1, 2, 1, 2, 1, 2, 2, 2, 2, 2, 1, 1, 1, 2, 1,\n",
      "        2, 1, 1, 2, 2, 1, 2, 2, 1, 2, 2, 1, 1, 1, 1, 1, 2, 2, 1, 2, 1, 1, 2, 1,\n",
      "        1, 2, 2, 2, 1, 2])\n",
      "75\n"
     ]
    }
   ],
   "source": [
    "# idx_train = range(150)\n",
    "# idx_val = range(150, 175)\n",
    "# idx_test = range(175, 200)\n",
    "print(labels)\n",
    "idx_train = np.where(labels)[0]\n",
    "idx_val = np.where(1 - abs(labels))[0][:90]\n",
    "idx_test = np.where(1 - abs(labels))[0][90:]\n",
    "\n",
    "print(len(idx_train))\n",
    "\n",
    "idx_train = torch.LongTensor(idx_train)\n",
    "idx_val = torch.LongTensor(idx_val)\n",
    "idx_test = torch.LongTensor(idx_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 loss_train: 1.5240 acc_train: 0.4267 loss_val: 0.9701 acc_val: 0.5333 time: 0.0042s\n",
      "Epoch: 0002 loss_train: 1.0529 acc_train: 0.5467 loss_val: 0.9428 acc_val: 0.5333 time: 0.0051s\n",
      "Epoch: 0003 loss_train: 1.1073 acc_train: 0.5733 loss_val: 0.9221 acc_val: 0.5333 time: 0.0066s\n",
      "Epoch: 0004 loss_train: 0.8769 acc_train: 0.5867 loss_val: 0.9013 acc_val: 0.5333 time: 0.0055s\n",
      "Epoch: 0005 loss_train: 0.8511 acc_train: 0.5733 loss_val: 0.8857 acc_val: 0.5444 time: 0.0052s\n",
      "Epoch: 0006 loss_train: 0.8511 acc_train: 0.5600 loss_val: 0.8675 acc_val: 0.5444 time: 0.0076s\n",
      "Epoch: 0007 loss_train: 0.8288 acc_train: 0.5733 loss_val: 0.8500 acc_val: 0.5556 time: 0.0052s\n",
      "Epoch: 0008 loss_train: 0.8886 acc_train: 0.6000 loss_val: 0.8333 acc_val: 0.5667 time: 0.0062s\n",
      "Epoch: 0009 loss_train: 0.9678 acc_train: 0.5467 loss_val: 0.8222 acc_val: 0.5667 time: 0.0052s\n",
      "Epoch: 0010 loss_train: 0.7250 acc_train: 0.5867 loss_val: 0.8113 acc_val: 0.5667 time: 0.0035s\n",
      "Epoch: 0011 loss_train: 0.7796 acc_train: 0.6000 loss_val: 0.7998 acc_val: 0.5556 time: 0.0037s\n",
      "Epoch: 0012 loss_train: 0.7380 acc_train: 0.6133 loss_val: 0.7878 acc_val: 0.5556 time: 0.0090s\n",
      "Epoch: 0013 loss_train: 0.7409 acc_train: 0.6267 loss_val: 0.7775 acc_val: 0.5556 time: 0.0059s\n",
      "Epoch: 0014 loss_train: 0.7370 acc_train: 0.5733 loss_val: 0.7664 acc_val: 0.5556 time: 0.0037s\n",
      "Epoch: 0015 loss_train: 0.6804 acc_train: 0.6400 loss_val: 0.7570 acc_val: 0.5667 time: 0.0035s\n",
      "Epoch: 0016 loss_train: 0.8781 acc_train: 0.5600 loss_val: 0.7496 acc_val: 0.5667 time: 0.0033s\n",
      "Epoch: 0017 loss_train: 0.6908 acc_train: 0.6533 loss_val: 0.7426 acc_val: 0.5556 time: 0.0032s\n",
      "Epoch: 0018 loss_train: 0.7045 acc_train: 0.5733 loss_val: 0.7360 acc_val: 0.5556 time: 0.0034s\n",
      "Epoch: 0019 loss_train: 0.6953 acc_train: 0.6267 loss_val: 0.7295 acc_val: 0.5556 time: 0.0030s\n",
      "Epoch: 0020 loss_train: 0.6327 acc_train: 0.5867 loss_val: 0.7225 acc_val: 0.5667 time: 0.0032s\n",
      "Epoch: 0021 loss_train: 0.6791 acc_train: 0.6267 loss_val: 0.7138 acc_val: 0.5667 time: 0.0035s\n",
      "Epoch: 0022 loss_train: 0.6286 acc_train: 0.6667 loss_val: 0.7049 acc_val: 0.5778 time: 0.0042s\n",
      "Epoch: 0023 loss_train: 0.5911 acc_train: 0.6933 loss_val: 0.6958 acc_val: 0.5778 time: 0.0037s\n",
      "Epoch: 0024 loss_train: 0.7215 acc_train: 0.6000 loss_val: 0.6872 acc_val: 0.5778 time: 0.0036s\n",
      "Epoch: 0025 loss_train: 0.5887 acc_train: 0.6533 loss_val: 0.6795 acc_val: 0.5889 time: 0.0033s\n",
      "Epoch: 0026 loss_train: 0.6421 acc_train: 0.6933 loss_val: 0.6731 acc_val: 0.6000 time: 0.0034s\n",
      "Epoch: 0027 loss_train: 0.6032 acc_train: 0.6533 loss_val: 0.6672 acc_val: 0.6222 time: 0.0036s\n",
      "Epoch: 0028 loss_train: 0.5563 acc_train: 0.7867 loss_val: 0.6621 acc_val: 0.6333 time: 0.0031s\n",
      "Epoch: 0029 loss_train: 0.5769 acc_train: 0.7067 loss_val: 0.6576 acc_val: 0.6444 time: 0.0035s\n",
      "Epoch: 0030 loss_train: 0.5569 acc_train: 0.7467 loss_val: 0.6538 acc_val: 0.6556 time: 0.0029s\n",
      "Epoch: 0031 loss_train: 0.5684 acc_train: 0.6800 loss_val: 0.6499 acc_val: 0.6333 time: 0.0029s\n",
      "Epoch: 0032 loss_train: 0.5187 acc_train: 0.7067 loss_val: 0.6454 acc_val: 0.6222 time: 0.0029s\n",
      "Epoch: 0033 loss_train: 0.5575 acc_train: 0.7733 loss_val: 0.6407 acc_val: 0.6444 time: 0.0028s\n",
      "Epoch: 0034 loss_train: 0.5460 acc_train: 0.7733 loss_val: 0.6371 acc_val: 0.7000 time: 0.0030s\n",
      "Epoch: 0035 loss_train: 0.5067 acc_train: 0.8000 loss_val: 0.6340 acc_val: 0.7444 time: 0.0029s\n",
      "Epoch: 0036 loss_train: 0.5215 acc_train: 0.7867 loss_val: 0.6322 acc_val: 0.7667 time: 0.0030s\n",
      "Epoch: 0037 loss_train: 0.5189 acc_train: 0.8400 loss_val: 0.6326 acc_val: 0.7778 time: 0.0028s\n",
      "Epoch: 0038 loss_train: 0.4750 acc_train: 0.8533 loss_val: 0.6319 acc_val: 0.7778 time: 0.0028s\n",
      "Epoch: 0039 loss_train: 0.5101 acc_train: 0.8000 loss_val: 0.6295 acc_val: 0.7778 time: 0.0028s\n",
      "Epoch: 0040 loss_train: 0.5246 acc_train: 0.8267 loss_val: 0.6260 acc_val: 0.7778 time: 0.0028s\n",
      "Epoch: 0041 loss_train: 0.5068 acc_train: 0.7733 loss_val: 0.6227 acc_val: 0.7778 time: 0.0029s\n",
      "Epoch: 0042 loss_train: 0.4794 acc_train: 0.8667 loss_val: 0.6202 acc_val: 0.7778 time: 0.0028s\n",
      "Epoch: 0043 loss_train: 0.5205 acc_train: 0.7200 loss_val: 0.6175 acc_val: 0.7778 time: 0.0030s\n",
      "Epoch: 0044 loss_train: 0.5642 acc_train: 0.7067 loss_val: 0.6144 acc_val: 0.7778 time: 0.0029s\n",
      "Epoch: 0045 loss_train: 0.5032 acc_train: 0.7733 loss_val: 0.6116 acc_val: 0.7778 time: 0.0028s\n",
      "Epoch: 0046 loss_train: 0.5124 acc_train: 0.7733 loss_val: 0.6092 acc_val: 0.7778 time: 0.0028s\n",
      "Epoch: 0047 loss_train: 0.4976 acc_train: 0.7733 loss_val: 0.6070 acc_val: 0.7778 time: 0.0029s\n",
      "Epoch: 0048 loss_train: 0.4666 acc_train: 0.8667 loss_val: 0.6053 acc_val: 0.7556 time: 0.0029s\n",
      "Epoch: 0049 loss_train: 0.4801 acc_train: 0.8133 loss_val: 0.6036 acc_val: 0.7556 time: 0.0030s\n",
      "Epoch: 0050 loss_train: 0.4715 acc_train: 0.8267 loss_val: 0.6021 acc_val: 0.7556 time: 0.0028s\n",
      "Epoch: 0051 loss_train: 0.4862 acc_train: 0.8000 loss_val: 0.6005 acc_val: 0.7556 time: 0.0028s\n",
      "Epoch: 0052 loss_train: 0.4695 acc_train: 0.7733 loss_val: 0.5990 acc_val: 0.7556 time: 0.0055s\n",
      "Epoch: 0053 loss_train: 0.4615 acc_train: 0.8533 loss_val: 0.5993 acc_val: 0.7556 time: 0.0038s\n",
      "Epoch: 0054 loss_train: 0.4557 acc_train: 0.8000 loss_val: 0.5989 acc_val: 0.7556 time: 0.0042s\n",
      "Epoch: 0055 loss_train: 0.4469 acc_train: 0.8400 loss_val: 0.5990 acc_val: 0.7556 time: 0.0042s\n",
      "Epoch: 0056 loss_train: 0.4514 acc_train: 0.8133 loss_val: 0.5990 acc_val: 0.7556 time: 0.0053s\n",
      "Epoch: 0057 loss_train: 0.5048 acc_train: 0.7867 loss_val: 0.5978 acc_val: 0.7556 time: 0.0050s\n",
      "Epoch: 0058 loss_train: 0.4872 acc_train: 0.7733 loss_val: 0.5957 acc_val: 0.7556 time: 0.0038s\n",
      "Epoch: 0059 loss_train: 0.4597 acc_train: 0.8533 loss_val: 0.5935 acc_val: 0.7556 time: 0.0035s\n",
      "Epoch: 0060 loss_train: 0.4414 acc_train: 0.8267 loss_val: 0.5915 acc_val: 0.7667 time: 0.0045s\n",
      "Epoch: 0061 loss_train: 0.4594 acc_train: 0.8133 loss_val: 0.5897 acc_val: 0.7667 time: 0.0073s\n",
      "Epoch: 0062 loss_train: 0.4369 acc_train: 0.8400 loss_val: 0.5890 acc_val: 0.7667 time: 0.0044s\n",
      "Epoch: 0063 loss_train: 0.4359 acc_train: 0.7867 loss_val: 0.5881 acc_val: 0.7667 time: 0.0042s\n",
      "Epoch: 0064 loss_train: 0.4376 acc_train: 0.8267 loss_val: 0.5869 acc_val: 0.7667 time: 0.0051s\n",
      "Epoch: 0065 loss_train: 0.4234 acc_train: 0.8400 loss_val: 0.5858 acc_val: 0.7667 time: 0.0049s\n",
      "Epoch: 0066 loss_train: 0.4474 acc_train: 0.8000 loss_val: 0.5840 acc_val: 0.7556 time: 0.0046s\n",
      "Epoch: 0067 loss_train: 0.4230 acc_train: 0.8800 loss_val: 0.5835 acc_val: 0.7556 time: 0.0041s\n",
      "Epoch: 0068 loss_train: 0.4213 acc_train: 0.8400 loss_val: 0.5865 acc_val: 0.7556 time: 0.0036s\n",
      "Epoch: 0069 loss_train: 0.4838 acc_train: 0.8133 loss_val: 0.5908 acc_val: 0.7556 time: 0.0035s\n",
      "Epoch: 0070 loss_train: 0.4215 acc_train: 0.7600 loss_val: 0.5971 acc_val: 0.7556 time: 0.0036s\n",
      "Epoch: 0071 loss_train: 0.4857 acc_train: 0.7867 loss_val: 0.5959 acc_val: 0.7556 time: 0.0043s\n",
      "Epoch: 0072 loss_train: 0.5790 acc_train: 0.7600 loss_val: 0.5928 acc_val: 0.7556 time: 0.0037s\n",
      "Epoch: 0073 loss_train: 0.4648 acc_train: 0.7733 loss_val: 0.5904 acc_val: 0.7556 time: 0.0034s\n",
      "Epoch: 0074 loss_train: 0.4082 acc_train: 0.8133 loss_val: 0.5884 acc_val: 0.7556 time: 0.0038s\n",
      "Epoch: 0075 loss_train: 0.3953 acc_train: 0.8267 loss_val: 0.5890 acc_val: 0.7556 time: 0.0036s\n",
      "Epoch: 0076 loss_train: 0.4414 acc_train: 0.8000 loss_val: 0.5881 acc_val: 0.7556 time: 0.0034s\n",
      "Epoch: 0077 loss_train: 0.4741 acc_train: 0.8000 loss_val: 0.5849 acc_val: 0.7556 time: 0.0032s\n",
      "Epoch: 0078 loss_train: 0.4314 acc_train: 0.8267 loss_val: 0.5805 acc_val: 0.7556 time: 0.0035s\n",
      "Epoch: 0079 loss_train: 0.4150 acc_train: 0.8133 loss_val: 0.5753 acc_val: 0.7556 time: 0.0034s\n",
      "Epoch: 0080 loss_train: 0.3774 acc_train: 0.8400 loss_val: 0.5713 acc_val: 0.7556 time: 0.0031s\n",
      "Epoch: 0081 loss_train: 0.4614 acc_train: 0.8133 loss_val: 0.5676 acc_val: 0.7556 time: 0.0038s\n",
      "Epoch: 0082 loss_train: 0.3694 acc_train: 0.8800 loss_val: 0.5646 acc_val: 0.7556 time: 0.0035s\n",
      "Epoch: 0083 loss_train: 0.4493 acc_train: 0.8267 loss_val: 0.5620 acc_val: 0.7556 time: 0.0032s\n",
      "Epoch: 0084 loss_train: 0.3976 acc_train: 0.8267 loss_val: 0.5598 acc_val: 0.7556 time: 0.0036s\n",
      "Epoch: 0085 loss_train: 0.3757 acc_train: 0.8267 loss_val: 0.5570 acc_val: 0.7667 time: 0.0035s\n",
      "Epoch: 0086 loss_train: 0.3857 acc_train: 0.8533 loss_val: 0.5543 acc_val: 0.7667 time: 0.0034s\n",
      "Epoch: 0087 loss_train: 0.3677 acc_train: 0.8933 loss_val: 0.5524 acc_val: 0.7667 time: 0.0036s\n",
      "Epoch: 0088 loss_train: 0.5040 acc_train: 0.8000 loss_val: 0.5513 acc_val: 0.7667 time: 0.0032s\n",
      "Epoch: 0089 loss_train: 0.4126 acc_train: 0.7867 loss_val: 0.5513 acc_val: 0.7556 time: 0.0031s\n",
      "Epoch: 0090 loss_train: 0.4684 acc_train: 0.7733 loss_val: 0.5535 acc_val: 0.7556 time: 0.0031s\n",
      "Epoch: 0091 loss_train: 0.4045 acc_train: 0.8000 loss_val: 0.5556 acc_val: 0.7556 time: 0.0032s\n",
      "Epoch: 0092 loss_train: 0.4395 acc_train: 0.8000 loss_val: 0.5576 acc_val: 0.7556 time: 0.0032s\n",
      "Epoch: 0093 loss_train: 0.3997 acc_train: 0.8667 loss_val: 0.5580 acc_val: 0.7556 time: 0.0033s\n",
      "Epoch: 0094 loss_train: 0.3818 acc_train: 0.7867 loss_val: 0.5577 acc_val: 0.7556 time: 0.0033s\n",
      "Epoch: 0095 loss_train: 0.3624 acc_train: 0.8533 loss_val: 0.5577 acc_val: 0.7556 time: 0.0033s\n",
      "Epoch: 0096 loss_train: 0.3762 acc_train: 0.8267 loss_val: 0.5578 acc_val: 0.7556 time: 0.0034s\n",
      "Epoch: 0097 loss_train: 0.3848 acc_train: 0.8533 loss_val: 0.5588 acc_val: 0.7556 time: 0.0032s\n",
      "Epoch: 0098 loss_train: 0.3945 acc_train: 0.8667 loss_val: 0.5625 acc_val: 0.7667 time: 0.0046s\n",
      "Epoch: 0099 loss_train: 0.3940 acc_train: 0.8133 loss_val: 0.5662 acc_val: 0.7667 time: 0.0038s\n",
      "Epoch: 0100 loss_train: 0.4709 acc_train: 0.8533 loss_val: 0.5666 acc_val: 0.7667 time: 0.0029s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0101 loss_train: 0.4237 acc_train: 0.7867 loss_val: 0.5659 acc_val: 0.7667 time: 0.0033s\n",
      "Epoch: 0102 loss_train: 0.3521 acc_train: 0.8800 loss_val: 0.5653 acc_val: 0.7667 time: 0.0043s\n",
      "Epoch: 0103 loss_train: 0.4065 acc_train: 0.8267 loss_val: 0.5664 acc_val: 0.7556 time: 0.0039s\n",
      "Epoch: 0104 loss_train: 0.3808 acc_train: 0.8267 loss_val: 0.5678 acc_val: 0.7556 time: 0.0031s\n",
      "Epoch: 0105 loss_train: 0.3440 acc_train: 0.8400 loss_val: 0.5692 acc_val: 0.7556 time: 0.0029s\n",
      "Epoch: 0106 loss_train: 0.4217 acc_train: 0.8133 loss_val: 0.5689 acc_val: 0.7556 time: 0.0028s\n",
      "Epoch: 0107 loss_train: 0.4270 acc_train: 0.8133 loss_val: 0.5701 acc_val: 0.7556 time: 0.0039s\n",
      "Epoch: 0108 loss_train: 0.3631 acc_train: 0.8267 loss_val: 0.5706 acc_val: 0.7556 time: 0.0059s\n",
      "Epoch: 0109 loss_train: 0.3906 acc_train: 0.8267 loss_val: 0.5710 acc_val: 0.7556 time: 0.0059s\n",
      "Epoch: 0110 loss_train: 0.4640 acc_train: 0.7467 loss_val: 0.5689 acc_val: 0.7556 time: 0.0054s\n",
      "Epoch: 0111 loss_train: 0.3800 acc_train: 0.8133 loss_val: 0.5668 acc_val: 0.7556 time: 0.0039s\n",
      "Epoch: 0112 loss_train: 0.3714 acc_train: 0.8000 loss_val: 0.5636 acc_val: 0.7556 time: 0.0038s\n",
      "Epoch: 0113 loss_train: 0.4526 acc_train: 0.8000 loss_val: 0.5578 acc_val: 0.7556 time: 0.0037s\n",
      "Epoch: 0114 loss_train: 0.3295 acc_train: 0.8267 loss_val: 0.5523 acc_val: 0.7556 time: 0.0052s\n",
      "Epoch: 0115 loss_train: 0.3360 acc_train: 0.8267 loss_val: 0.5462 acc_val: 0.7556 time: 0.0061s\n",
      "Epoch: 0116 loss_train: 0.3594 acc_train: 0.8000 loss_val: 0.5417 acc_val: 0.7667 time: 0.0069s\n",
      "Epoch: 0117 loss_train: 0.3902 acc_train: 0.8133 loss_val: 0.5394 acc_val: 0.7667 time: 0.0033s\n",
      "Epoch: 0118 loss_train: 0.3786 acc_train: 0.8267 loss_val: 0.5380 acc_val: 0.7667 time: 0.0029s\n",
      "Epoch: 0119 loss_train: 0.3684 acc_train: 0.8533 loss_val: 0.5375 acc_val: 0.7667 time: 0.0039s\n",
      "Epoch: 0120 loss_train: 0.3806 acc_train: 0.8000 loss_val: 0.5367 acc_val: 0.7667 time: 0.0037s\n",
      "Epoch: 0121 loss_train: 0.3488 acc_train: 0.8400 loss_val: 0.5354 acc_val: 0.7667 time: 0.0031s\n",
      "Epoch: 0122 loss_train: 0.3460 acc_train: 0.8400 loss_val: 0.5350 acc_val: 0.7667 time: 0.0032s\n",
      "Epoch: 0123 loss_train: 0.3723 acc_train: 0.8267 loss_val: 0.5331 acc_val: 0.7667 time: 0.0041s\n",
      "Epoch: 0124 loss_train: 0.3931 acc_train: 0.7867 loss_val: 0.5321 acc_val: 0.7667 time: 0.0034s\n",
      "Epoch: 0125 loss_train: 0.3488 acc_train: 0.8400 loss_val: 0.5311 acc_val: 0.7667 time: 0.0035s\n",
      "Epoch: 0126 loss_train: 0.3891 acc_train: 0.8000 loss_val: 0.5291 acc_val: 0.7667 time: 0.0035s\n",
      "Epoch: 0127 loss_train: 0.3630 acc_train: 0.8533 loss_val: 0.5275 acc_val: 0.7667 time: 0.0039s\n",
      "Epoch: 0128 loss_train: 0.3656 acc_train: 0.8000 loss_val: 0.5250 acc_val: 0.7667 time: 0.0037s\n",
      "Epoch: 0129 loss_train: 0.3685 acc_train: 0.8133 loss_val: 0.5243 acc_val: 0.7667 time: 0.0039s\n",
      "Epoch: 0130 loss_train: 0.4381 acc_train: 0.8267 loss_val: 0.5253 acc_val: 0.7667 time: 0.0038s\n",
      "Epoch: 0131 loss_train: 0.3668 acc_train: 0.8400 loss_val: 0.5257 acc_val: 0.7667 time: 0.0042s\n",
      "Epoch: 0132 loss_train: 0.3973 acc_train: 0.8000 loss_val: 0.5268 acc_val: 0.7667 time: 0.0037s\n",
      "Epoch: 0133 loss_train: 0.3391 acc_train: 0.8800 loss_val: 0.5284 acc_val: 0.7667 time: 0.0043s\n",
      "Epoch: 0134 loss_train: 0.3447 acc_train: 0.8400 loss_val: 0.5292 acc_val: 0.7667 time: 0.0036s\n",
      "Epoch: 0135 loss_train: 0.3393 acc_train: 0.8533 loss_val: 0.5304 acc_val: 0.7667 time: 0.0039s\n",
      "Epoch: 0136 loss_train: 0.3751 acc_train: 0.8133 loss_val: 0.5302 acc_val: 0.7667 time: 0.0037s\n",
      "Epoch: 0137 loss_train: 0.4163 acc_train: 0.8267 loss_val: 0.5290 acc_val: 0.7667 time: 0.0036s\n",
      "Epoch: 0138 loss_train: 0.3599 acc_train: 0.8267 loss_val: 0.5283 acc_val: 0.7667 time: 0.0037s\n",
      "Epoch: 0139 loss_train: 0.4189 acc_train: 0.7867 loss_val: 0.5264 acc_val: 0.7667 time: 0.0032s\n",
      "Epoch: 0140 loss_train: 0.3997 acc_train: 0.8400 loss_val: 0.5263 acc_val: 0.7667 time: 0.0039s\n",
      "Epoch: 0141 loss_train: 0.3387 acc_train: 0.8000 loss_val: 0.5266 acc_val: 0.7667 time: 0.0040s\n",
      "Epoch: 0142 loss_train: 0.3548 acc_train: 0.8133 loss_val: 0.5273 acc_val: 0.7667 time: 0.0039s\n",
      "Epoch: 0143 loss_train: 0.3713 acc_train: 0.8133 loss_val: 0.5271 acc_val: 0.7667 time: 0.0047s\n",
      "Epoch: 0144 loss_train: 0.3631 acc_train: 0.8133 loss_val: 0.5269 acc_val: 0.7667 time: 0.0036s\n",
      "Epoch: 0145 loss_train: 0.3366 acc_train: 0.8133 loss_val: 0.5258 acc_val: 0.7667 time: 0.0042s\n",
      "Epoch: 0146 loss_train: 0.4012 acc_train: 0.8267 loss_val: 0.5252 acc_val: 0.7667 time: 0.0041s\n",
      "Epoch: 0147 loss_train: 0.3620 acc_train: 0.8133 loss_val: 0.5256 acc_val: 0.7667 time: 0.0049s\n",
      "Epoch: 0148 loss_train: 0.3209 acc_train: 0.8400 loss_val: 0.5257 acc_val: 0.7667 time: 0.0037s\n",
      "Epoch: 0149 loss_train: 0.4604 acc_train: 0.7333 loss_val: 0.5262 acc_val: 0.7667 time: 0.0049s\n",
      "Epoch: 0150 loss_train: 0.3874 acc_train: 0.8267 loss_val: 0.5272 acc_val: 0.7667 time: 0.0041s\n",
      "Epoch: 0151 loss_train: 0.3803 acc_train: 0.8000 loss_val: 0.5290 acc_val: 0.7667 time: 0.0038s\n",
      "Epoch: 0152 loss_train: 0.3130 acc_train: 0.8533 loss_val: 0.5319 acc_val: 0.7667 time: 0.0030s\n",
      "Epoch: 0153 loss_train: 0.3710 acc_train: 0.8400 loss_val: 0.5338 acc_val: 0.7667 time: 0.0051s\n",
      "Epoch: 0154 loss_train: 0.3434 acc_train: 0.8133 loss_val: 0.5363 acc_val: 0.7667 time: 0.0047s\n",
      "Epoch: 0155 loss_train: 0.3442 acc_train: 0.8133 loss_val: 0.5379 acc_val: 0.7667 time: 0.0047s\n",
      "Epoch: 0156 loss_train: 0.3456 acc_train: 0.8267 loss_val: 0.5398 acc_val: 0.7667 time: 0.0047s\n",
      "Epoch: 0157 loss_train: 0.3556 acc_train: 0.8533 loss_val: 0.5415 acc_val: 0.7667 time: 0.0042s\n",
      "Epoch: 0158 loss_train: 0.3526 acc_train: 0.8400 loss_val: 0.5580 acc_val: 0.7667 time: 0.0068s\n",
      "Epoch: 0159 loss_train: 0.3734 acc_train: 0.8533 loss_val: 0.5439 acc_val: 0.7667 time: 0.0048s\n",
      "Epoch: 0160 loss_train: 0.3500 acc_train: 0.8133 loss_val: 0.5438 acc_val: 0.7667 time: 0.0044s\n",
      "Epoch: 0161 loss_train: 0.3768 acc_train: 0.7867 loss_val: 0.5427 acc_val: 0.7667 time: 0.0036s\n",
      "Epoch: 0162 loss_train: 0.3426 acc_train: 0.8400 loss_val: 0.5417 acc_val: 0.7667 time: 0.0041s\n",
      "Epoch: 0163 loss_train: 0.3073 acc_train: 0.8267 loss_val: 0.5413 acc_val: 0.7778 time: 0.0053s\n",
      "Epoch: 0164 loss_train: 0.3527 acc_train: 0.8400 loss_val: 0.5406 acc_val: 0.7778 time: 0.0092s\n",
      "Epoch: 0165 loss_train: 0.3629 acc_train: 0.8133 loss_val: 0.5396 acc_val: 0.7778 time: 0.0034s\n",
      "Epoch: 0166 loss_train: 0.3516 acc_train: 0.8400 loss_val: 0.5389 acc_val: 0.7889 time: 0.0032s\n",
      "Epoch: 0167 loss_train: 0.3020 acc_train: 0.8933 loss_val: 0.5382 acc_val: 0.7778 time: 0.0033s\n",
      "Epoch: 0168 loss_train: 0.3879 acc_train: 0.7733 loss_val: 0.5369 acc_val: 0.7778 time: 0.0032s\n",
      "Epoch: 0169 loss_train: 0.3617 acc_train: 0.8267 loss_val: 0.5367 acc_val: 0.7667 time: 0.0033s\n",
      "Epoch: 0170 loss_train: 0.3203 acc_train: 0.8267 loss_val: 0.5363 acc_val: 0.7667 time: 0.0033s\n",
      "Epoch: 0171 loss_train: 0.3464 acc_train: 0.8267 loss_val: 0.5354 acc_val: 0.7667 time: 0.0034s\n",
      "Epoch: 0172 loss_train: 0.3779 acc_train: 0.8267 loss_val: 0.5340 acc_val: 0.7667 time: 0.0034s\n",
      "Epoch: 0173 loss_train: 0.3316 acc_train: 0.8533 loss_val: 0.5335 acc_val: 0.7667 time: 0.0035s\n",
      "Epoch: 0174 loss_train: 0.3648 acc_train: 0.8533 loss_val: 0.5342 acc_val: 0.7667 time: 0.0035s\n",
      "Epoch: 0175 loss_train: 0.3310 acc_train: 0.8533 loss_val: 0.5350 acc_val: 0.7667 time: 0.0033s\n",
      "Epoch: 0176 loss_train: 0.3182 acc_train: 0.8400 loss_val: 0.5359 acc_val: 0.7667 time: 0.0036s\n",
      "Epoch: 0177 loss_train: 0.3766 acc_train: 0.8133 loss_val: 0.5371 acc_val: 0.7333 time: 0.0034s\n",
      "Epoch: 0178 loss_train: 0.3590 acc_train: 0.8533 loss_val: 0.5400 acc_val: 0.7222 time: 0.0033s\n",
      "Epoch: 0179 loss_train: 0.3678 acc_train: 0.8000 loss_val: 0.5437 acc_val: 0.7222 time: 0.0036s\n",
      "Epoch: 0180 loss_train: 0.3234 acc_train: 0.8400 loss_val: 0.5461 acc_val: 0.7222 time: 0.0034s\n",
      "Epoch: 0181 loss_train: 0.2849 acc_train: 0.8800 loss_val: 0.5479 acc_val: 0.7222 time: 0.0034s\n",
      "Epoch: 0182 loss_train: 0.3622 acc_train: 0.8133 loss_val: 0.5484 acc_val: 0.7222 time: 0.0032s\n",
      "Epoch: 0183 loss_train: 0.3301 acc_train: 0.8533 loss_val: 0.5485 acc_val: 0.7333 time: 0.0033s\n",
      "Epoch: 0184 loss_train: 0.4018 acc_train: 0.8133 loss_val: 0.5465 acc_val: 0.7667 time: 0.0034s\n",
      "Epoch: 0185 loss_train: 0.2971 acc_train: 0.8800 loss_val: 0.5442 acc_val: 0.7667 time: 0.0032s\n",
      "Epoch: 0186 loss_train: 0.3245 acc_train: 0.8533 loss_val: 0.5426 acc_val: 0.7667 time: 0.0034s\n",
      "Epoch: 0187 loss_train: 0.3298 acc_train: 0.8400 loss_val: 0.5409 acc_val: 0.7667 time: 0.0034s\n",
      "Epoch: 0188 loss_train: 0.3298 acc_train: 0.8400 loss_val: 0.5392 acc_val: 0.7667 time: 0.0035s\n",
      "Epoch: 0189 loss_train: 0.3487 acc_train: 0.8933 loss_val: 0.5381 acc_val: 0.7667 time: 0.0035s\n",
      "Epoch: 0190 loss_train: 0.3946 acc_train: 0.7867 loss_val: 0.5367 acc_val: 0.7889 time: 0.0032s\n",
      "Epoch: 0191 loss_train: 0.3538 acc_train: 0.8400 loss_val: 0.5365 acc_val: 0.7889 time: 0.0035s\n",
      "Epoch: 0192 loss_train: 0.3871 acc_train: 0.8267 loss_val: 0.5365 acc_val: 0.7889 time: 0.0037s\n",
      "Epoch: 0193 loss_train: 0.3652 acc_train: 0.8267 loss_val: 0.5364 acc_val: 0.7667 time: 0.0038s\n",
      "Epoch: 0194 loss_train: 0.3698 acc_train: 0.8000 loss_val: 0.5365 acc_val: 0.7667 time: 0.0039s\n",
      "Epoch: 0195 loss_train: 0.3839 acc_train: 0.8133 loss_val: 0.5356 acc_val: 0.7667 time: 0.0030s\n",
      "Epoch: 0196 loss_train: 0.3236 acc_train: 0.8267 loss_val: 0.5351 acc_val: 0.7667 time: 0.0029s\n",
      "Epoch: 0197 loss_train: 0.3287 acc_train: 0.8667 loss_val: 0.5350 acc_val: 0.7667 time: 0.0048s\n",
      "Epoch: 0198 loss_train: 0.3555 acc_train: 0.8267 loss_val: 0.5349 acc_val: 0.7667 time: 0.0037s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0199 loss_train: 0.2934 acc_train: 0.8933 loss_val: 0.5348 acc_val: 0.7667 time: 0.0054s\n",
      "Epoch: 0200 loss_train: 0.3460 acc_train: 0.8533 loss_val: 0.5354 acc_val: 0.7667 time: 0.0041s\n",
      "Epoch: 0201 loss_train: 0.3837 acc_train: 0.8267 loss_val: 0.5357 acc_val: 0.7667 time: 0.0050s\n",
      "Epoch: 0202 loss_train: 0.3708 acc_train: 0.7867 loss_val: 0.5359 acc_val: 0.7667 time: 0.0083s\n",
      "Epoch: 0203 loss_train: 0.3515 acc_train: 0.8400 loss_val: 0.5346 acc_val: 0.7667 time: 0.0047s\n",
      "Epoch: 0204 loss_train: 0.3782 acc_train: 0.8133 loss_val: 0.5383 acc_val: 0.7778 time: 0.0077s\n",
      "Epoch: 0205 loss_train: 0.7697 acc_train: 0.7600 loss_val: 0.5351 acc_val: 0.7778 time: 0.0108s\n",
      "Epoch: 0206 loss_train: 0.3344 acc_train: 0.8533 loss_val: 0.5371 acc_val: 0.7889 time: 0.0087s\n",
      "Epoch: 0207 loss_train: 0.3571 acc_train: 0.8667 loss_val: 0.5398 acc_val: 0.7889 time: 0.0117s\n",
      "Epoch: 0208 loss_train: 0.3344 acc_train: 0.8267 loss_val: 0.5425 acc_val: 0.8000 time: 0.0047s\n",
      "Epoch: 0209 loss_train: 0.2997 acc_train: 0.8667 loss_val: 0.5453 acc_val: 0.8000 time: 0.0121s\n",
      "Epoch: 0210 loss_train: 0.4601 acc_train: 0.7733 loss_val: 0.5466 acc_val: 0.8000 time: 0.0069s\n",
      "Epoch: 0211 loss_train: 0.4285 acc_train: 0.8133 loss_val: 0.5473 acc_val: 0.7889 time: 0.0090s\n",
      "Epoch: 0212 loss_train: 0.3942 acc_train: 0.8267 loss_val: 0.5490 acc_val: 0.7778 time: 0.0045s\n",
      "Epoch: 0213 loss_train: 0.3974 acc_train: 0.8000 loss_val: 0.5517 acc_val: 0.7667 time: 0.0048s\n",
      "Epoch: 0214 loss_train: 0.3598 acc_train: 0.8133 loss_val: 0.5556 acc_val: 0.7667 time: 0.0042s\n",
      "Epoch: 0215 loss_train: 0.3576 acc_train: 0.8400 loss_val: 0.5598 acc_val: 0.7444 time: 0.0140s\n",
      "Epoch: 0216 loss_train: 0.4031 acc_train: 0.8000 loss_val: 0.5642 acc_val: 0.7444 time: 0.0044s\n",
      "Epoch: 0217 loss_train: 0.3338 acc_train: 0.8000 loss_val: 0.5684 acc_val: 0.7222 time: 0.0089s\n",
      "Epoch: 0218 loss_train: 0.3576 acc_train: 0.8133 loss_val: 0.5708 acc_val: 0.7222 time: 0.0092s\n",
      "Epoch: 0219 loss_train: 0.3395 acc_train: 0.8000 loss_val: 0.5698 acc_val: 0.7222 time: 0.0152s\n",
      "Epoch: 0220 loss_train: 0.3098 acc_train: 0.8667 loss_val: 0.5684 acc_val: 0.7222 time: 0.0106s\n",
      "Epoch: 0221 loss_train: 0.3809 acc_train: 0.8133 loss_val: 0.5659 acc_val: 0.7444 time: 0.0066s\n",
      "Epoch: 0222 loss_train: 0.3325 acc_train: 0.8400 loss_val: 0.5629 acc_val: 0.7444 time: 0.0121s\n",
      "Epoch: 0223 loss_train: 0.3245 acc_train: 0.8667 loss_val: 0.5596 acc_val: 0.7667 time: 0.0105s\n",
      "Epoch: 0224 loss_train: 0.3132 acc_train: 0.8533 loss_val: 0.5564 acc_val: 0.7667 time: 0.0068s\n",
      "Epoch: 0225 loss_train: 0.3100 acc_train: 0.8533 loss_val: 0.5533 acc_val: 0.7667 time: 0.0122s\n",
      "Epoch: 0226 loss_train: 0.3682 acc_train: 0.8133 loss_val: 0.5502 acc_val: 0.7778 time: 0.0056s\n",
      "Epoch: 0227 loss_train: 0.3629 acc_train: 0.8267 loss_val: 0.5480 acc_val: 0.7778 time: 0.0101s\n",
      "Epoch: 0228 loss_train: 0.3370 acc_train: 0.8400 loss_val: 0.5468 acc_val: 0.7889 time: 0.0047s\n",
      "Epoch: 0229 loss_train: 0.3978 acc_train: 0.7333 loss_val: 0.5460 acc_val: 0.7667 time: 0.0101s\n",
      "Epoch: 0230 loss_train: 0.3224 acc_train: 0.8267 loss_val: 0.5447 acc_val: 0.7556 time: 0.0051s\n",
      "Epoch: 0231 loss_train: 0.3614 acc_train: 0.8400 loss_val: 0.5431 acc_val: 0.7556 time: 0.0106s\n",
      "Epoch: 0232 loss_train: 0.3051 acc_train: 0.8667 loss_val: 0.5409 acc_val: 0.7556 time: 0.0059s\n",
      "Epoch: 0233 loss_train: 0.2957 acc_train: 0.8933 loss_val: 0.5380 acc_val: 0.7667 time: 0.0096s\n",
      "Epoch: 0234 loss_train: 0.3273 acc_train: 0.8533 loss_val: 0.5347 acc_val: 0.7667 time: 0.0035s\n",
      "Epoch: 0235 loss_train: 0.3375 acc_train: 0.8267 loss_val: 0.5315 acc_val: 0.8000 time: 0.0042s\n",
      "Epoch: 0236 loss_train: 0.3642 acc_train: 0.8267 loss_val: 0.5282 acc_val: 0.8000 time: 0.0075s\n",
      "Epoch: 0237 loss_train: 0.3328 acc_train: 0.8533 loss_val: 0.5255 acc_val: 0.7778 time: 0.0035s\n",
      "Epoch: 0238 loss_train: 0.3739 acc_train: 0.7867 loss_val: 0.5225 acc_val: 0.7778 time: 0.0033s\n",
      "Epoch: 0239 loss_train: 0.3891 acc_train: 0.8267 loss_val: 0.5193 acc_val: 0.8000 time: 0.0032s\n",
      "Epoch: 0240 loss_train: 0.3349 acc_train: 0.8267 loss_val: 0.5165 acc_val: 0.8000 time: 0.0038s\n",
      "Epoch: 0241 loss_train: 0.3537 acc_train: 0.8400 loss_val: 0.5138 acc_val: 0.8000 time: 0.0033s\n",
      "Epoch: 0242 loss_train: 0.2838 acc_train: 0.8667 loss_val: 0.5122 acc_val: 0.7889 time: 0.0038s\n",
      "Epoch: 0243 loss_train: 0.3656 acc_train: 0.8267 loss_val: 0.5120 acc_val: 0.7889 time: 0.0040s\n",
      "Epoch: 0244 loss_train: 0.3392 acc_train: 0.8400 loss_val: 0.5118 acc_val: 0.7889 time: 0.0034s\n",
      "Epoch: 0245 loss_train: 0.3641 acc_train: 0.8000 loss_val: 0.5110 acc_val: 0.7889 time: 0.0035s\n",
      "Epoch: 0246 loss_train: 0.3399 acc_train: 0.8533 loss_val: 0.5102 acc_val: 0.7444 time: 0.0040s\n",
      "Epoch: 0247 loss_train: 0.2807 acc_train: 0.8800 loss_val: 0.5099 acc_val: 0.7444 time: 0.0031s\n",
      "Epoch: 0248 loss_train: 0.3398 acc_train: 0.8400 loss_val: 0.5097 acc_val: 0.7333 time: 0.0033s\n",
      "Epoch: 0249 loss_train: 0.3208 acc_train: 0.8400 loss_val: 0.5098 acc_val: 0.7444 time: 0.0029s\n",
      "Epoch: 0250 loss_train: 0.3597 acc_train: 0.8400 loss_val: 0.5084 acc_val: 0.7667 time: 0.0028s\n",
      "Epoch: 0251 loss_train: 0.3900 acc_train: 0.8000 loss_val: 0.5072 acc_val: 0.7889 time: 0.0032s\n",
      "Epoch: 0252 loss_train: 0.3980 acc_train: 0.7467 loss_val: 0.5067 acc_val: 0.7889 time: 0.0027s\n",
      "Epoch: 0253 loss_train: 0.3508 acc_train: 0.8267 loss_val: 0.5074 acc_val: 0.7889 time: 0.0029s\n",
      "Epoch: 0254 loss_train: 0.3249 acc_train: 0.8400 loss_val: 0.5080 acc_val: 0.7889 time: 0.0029s\n",
      "Epoch: 0255 loss_train: 0.3178 acc_train: 0.8533 loss_val: 0.5089 acc_val: 0.7889 time: 0.0031s\n",
      "Epoch: 0256 loss_train: 0.3675 acc_train: 0.8533 loss_val: 0.5101 acc_val: 0.7889 time: 0.0029s\n",
      "Epoch: 0257 loss_train: 0.3195 acc_train: 0.8267 loss_val: 0.5112 acc_val: 0.7889 time: 0.0029s\n",
      "Epoch: 0258 loss_train: 0.3503 acc_train: 0.8267 loss_val: 0.5125 acc_val: 0.8111 time: 0.0030s\n",
      "Epoch: 0259 loss_train: 0.3981 acc_train: 0.7867 loss_val: 0.5132 acc_val: 0.8111 time: 0.0076s\n",
      "Epoch: 0260 loss_train: 0.3240 acc_train: 0.8667 loss_val: 0.5130 acc_val: 0.8111 time: 0.0034s\n",
      "Epoch: 0261 loss_train: 0.3289 acc_train: 0.8667 loss_val: 0.5125 acc_val: 0.8111 time: 0.0035s\n",
      "Epoch: 0262 loss_train: 0.3187 acc_train: 0.8800 loss_val: 0.5120 acc_val: 0.7889 time: 0.0038s\n",
      "Epoch: 0263 loss_train: 0.3563 acc_train: 0.8400 loss_val: 0.5114 acc_val: 0.7889 time: 0.0059s\n",
      "Epoch: 0264 loss_train: 0.3483 acc_train: 0.8400 loss_val: 0.5113 acc_val: 0.7889 time: 0.0051s\n",
      "Epoch: 0265 loss_train: 0.3485 acc_train: 0.8533 loss_val: 0.5115 acc_val: 0.7889 time: 0.0041s\n",
      "Epoch: 0266 loss_train: 0.3396 acc_train: 0.8000 loss_val: 0.5115 acc_val: 0.7889 time: 0.0082s\n",
      "Epoch: 0267 loss_train: 0.3337 acc_train: 0.8267 loss_val: 0.5116 acc_val: 0.7889 time: 0.0040s\n",
      "Epoch: 0268 loss_train: 0.3520 acc_train: 0.8267 loss_val: 0.5114 acc_val: 0.7889 time: 0.0042s\n",
      "Epoch: 0269 loss_train: 0.3285 acc_train: 0.8667 loss_val: 0.5109 acc_val: 0.7889 time: 0.0079s\n",
      "Epoch: 0270 loss_train: 0.3188 acc_train: 0.8267 loss_val: 0.5100 acc_val: 0.7889 time: 0.0035s\n",
      "Epoch: 0271 loss_train: 0.2936 acc_train: 0.8533 loss_val: 0.5093 acc_val: 0.7889 time: 0.0029s\n",
      "Epoch: 0272 loss_train: 0.2944 acc_train: 0.8667 loss_val: 0.5087 acc_val: 0.7889 time: 0.0039s\n",
      "Epoch: 0273 loss_train: 0.3727 acc_train: 0.8400 loss_val: 0.5080 acc_val: 0.7889 time: 0.0034s\n",
      "Epoch: 0274 loss_train: 0.3666 acc_train: 0.8133 loss_val: 0.5071 acc_val: 0.7889 time: 0.0035s\n",
      "Epoch: 0275 loss_train: 0.3579 acc_train: 0.8267 loss_val: 0.5062 acc_val: 0.7889 time: 0.0037s\n",
      "Epoch: 0276 loss_train: 0.3400 acc_train: 0.8400 loss_val: 0.5056 acc_val: 0.7889 time: 0.0042s\n",
      "Epoch: 0277 loss_train: 0.3371 acc_train: 0.8267 loss_val: 0.5049 acc_val: 0.7889 time: 0.0041s\n",
      "Epoch: 0278 loss_train: 0.3183 acc_train: 0.8267 loss_val: 0.5045 acc_val: 0.7889 time: 0.0035s\n",
      "Epoch: 0279 loss_train: 0.2899 acc_train: 0.8800 loss_val: 0.5049 acc_val: 0.7889 time: 0.0039s\n",
      "Epoch: 0280 loss_train: 0.2888 acc_train: 0.8800 loss_val: 0.5057 acc_val: 0.8000 time: 0.0043s\n",
      "Epoch: 0281 loss_train: 0.3357 acc_train: 0.8400 loss_val: 0.5067 acc_val: 0.8000 time: 0.0043s\n",
      "Epoch: 0282 loss_train: 0.3094 acc_train: 0.8533 loss_val: 0.5078 acc_val: 0.8000 time: 0.0046s\n",
      "Epoch: 0283 loss_train: 0.3074 acc_train: 0.8533 loss_val: 0.5096 acc_val: 0.8111 time: 0.0035s\n",
      "Epoch: 0284 loss_train: 0.3404 acc_train: 0.8267 loss_val: 0.5123 acc_val: 0.8222 time: 0.0035s\n",
      "Epoch: 0285 loss_train: 0.3449 acc_train: 0.8667 loss_val: 0.5151 acc_val: 0.8222 time: 0.0041s\n",
      "Epoch: 0286 loss_train: 0.3405 acc_train: 0.8533 loss_val: 0.5166 acc_val: 0.8333 time: 0.0046s\n",
      "Epoch: 0287 loss_train: 0.4039 acc_train: 0.8400 loss_val: 0.5137 acc_val: 0.8222 time: 0.0041s\n",
      "Epoch: 0288 loss_train: 0.3693 acc_train: 0.8133 loss_val: 0.5110 acc_val: 0.8111 time: 0.0037s\n",
      "Epoch: 0289 loss_train: 0.3440 acc_train: 0.8800 loss_val: 0.5086 acc_val: 0.8000 time: 0.0033s\n",
      "Epoch: 0290 loss_train: 0.3268 acc_train: 0.8800 loss_val: 0.5074 acc_val: 0.7889 time: 0.0036s\n",
      "Epoch: 0291 loss_train: 0.3215 acc_train: 0.8400 loss_val: 0.5073 acc_val: 0.7889 time: 0.0035s\n",
      "Epoch: 0292 loss_train: 0.3945 acc_train: 0.7733 loss_val: 0.5077 acc_val: 0.7667 time: 0.0034s\n",
      "Epoch: 0293 loss_train: 0.3523 acc_train: 0.8400 loss_val: 0.5097 acc_val: 0.7444 time: 0.0036s\n",
      "Epoch: 0294 loss_train: 0.2875 acc_train: 0.8533 loss_val: 0.5127 acc_val: 0.7444 time: 0.0031s\n",
      "Epoch: 0295 loss_train: 0.3018 acc_train: 0.8667 loss_val: 0.5150 acc_val: 0.7222 time: 0.0036s\n",
      "Epoch: 0296 loss_train: 0.3337 acc_train: 0.8400 loss_val: 0.5169 acc_val: 0.7444 time: 0.0033s\n",
      "Epoch: 0297 loss_train: 0.3339 acc_train: 0.8133 loss_val: 0.5181 acc_val: 0.7667 time: 0.0034s\n",
      "Epoch: 0298 loss_train: 0.3419 acc_train: 0.8533 loss_val: 0.5185 acc_val: 0.7667 time: 0.0034s\n",
      "Epoch: 0299 loss_train: 0.3176 acc_train: 0.8667 loss_val: 0.5218 acc_val: 0.7667 time: 0.0036s\n",
      "Epoch: 0300 loss_train: 0.2739 acc_train: 0.8800 loss_val: 0.5277 acc_val: 0.7667 time: 0.0038s\n",
      "Epoch: 0301 loss_train: 0.3343 acc_train: 0.8533 loss_val: 0.5274 acc_val: 0.7889 time: 0.0037s\n",
      "Epoch: 0302 loss_train: 0.3609 acc_train: 0.8667 loss_val: 0.5195 acc_val: 0.7889 time: 0.0035s\n",
      "Epoch: 0303 loss_train: 0.2861 acc_train: 0.9067 loss_val: 0.5186 acc_val: 0.7889 time: 0.0036s\n",
      "Epoch: 0304 loss_train: 0.3065 acc_train: 0.8533 loss_val: 0.5179 acc_val: 0.7778 time: 0.0043s\n",
      "Epoch: 0305 loss_train: 0.3242 acc_train: 0.8267 loss_val: 0.5168 acc_val: 0.7778 time: 0.0036s\n",
      "Epoch: 0306 loss_train: 0.3100 acc_train: 0.8400 loss_val: 0.5160 acc_val: 0.7778 time: 0.0038s\n",
      "Epoch: 0307 loss_train: 0.3692 acc_train: 0.8133 loss_val: 0.5150 acc_val: 0.7778 time: 0.0032s\n",
      "Epoch: 0308 loss_train: 0.3198 acc_train: 0.8667 loss_val: 0.5142 acc_val: 0.7778 time: 0.0043s\n",
      "Epoch: 0309 loss_train: 0.3080 acc_train: 0.8667 loss_val: 0.5137 acc_val: 0.7778 time: 0.0036s\n",
      "Epoch: 0310 loss_train: 0.3323 acc_train: 0.8533 loss_val: 0.5133 acc_val: 0.7778 time: 0.0034s\n",
      "Epoch: 0311 loss_train: 0.3185 acc_train: 0.8533 loss_val: 0.5127 acc_val: 0.7889 time: 0.0033s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0312 loss_train: 0.3141 acc_train: 0.8533 loss_val: 0.5124 acc_val: 0.7889 time: 0.0047s\n",
      "Epoch: 0313 loss_train: 0.2806 acc_train: 0.8667 loss_val: 0.5121 acc_val: 0.7889 time: 0.0037s\n",
      "Epoch: 0314 loss_train: 0.3337 acc_train: 0.8667 loss_val: 0.5118 acc_val: 0.7889 time: 0.0046s\n",
      "Epoch: 0315 loss_train: 0.3038 acc_train: 0.8533 loss_val: 0.5113 acc_val: 0.7889 time: 0.0039s\n",
      "Epoch: 0316 loss_train: 0.3075 acc_train: 0.8667 loss_val: 0.5104 acc_val: 0.7889 time: 0.0041s\n",
      "Epoch: 0317 loss_train: 0.2898 acc_train: 0.8800 loss_val: 0.5099 acc_val: 0.7889 time: 0.0029s\n",
      "Epoch: 0318 loss_train: 0.3197 acc_train: 0.8667 loss_val: 0.5098 acc_val: 0.7889 time: 0.0053s\n",
      "Epoch: 0319 loss_train: 0.3836 acc_train: 0.8400 loss_val: 0.5089 acc_val: 0.7667 time: 0.0049s\n",
      "Epoch: 0320 loss_train: 0.2975 acc_train: 0.8800 loss_val: 0.5083 acc_val: 0.7667 time: 0.0037s\n",
      "Epoch: 0321 loss_train: 0.2877 acc_train: 0.8933 loss_val: 0.5078 acc_val: 0.7667 time: 0.0031s\n",
      "Epoch: 0322 loss_train: 0.3210 acc_train: 0.8667 loss_val: 0.5075 acc_val: 0.7556 time: 0.0030s\n",
      "Epoch: 0323 loss_train: 0.3067 acc_train: 0.8400 loss_val: 0.5071 acc_val: 0.7444 time: 0.0029s\n",
      "Epoch: 0324 loss_train: 0.3665 acc_train: 0.8133 loss_val: 0.5060 acc_val: 0.7444 time: 0.0029s\n",
      "Epoch: 0325 loss_train: 0.3470 acc_train: 0.8533 loss_val: 0.5054 acc_val: 0.7444 time: 0.0031s\n",
      "Epoch: 0326 loss_train: 0.2977 acc_train: 0.8533 loss_val: 0.5045 acc_val: 0.7444 time: 0.0031s\n",
      "Epoch: 0327 loss_train: 0.3801 acc_train: 0.8667 loss_val: 0.5041 acc_val: 0.7556 time: 0.0029s\n",
      "Epoch: 0328 loss_train: 0.3382 acc_train: 0.8133 loss_val: 0.5035 acc_val: 0.7556 time: 0.0029s\n",
      "Epoch: 0329 loss_train: 0.3168 acc_train: 0.8800 loss_val: 0.5035 acc_val: 0.7444 time: 0.0029s\n",
      "Epoch: 0330 loss_train: 0.2847 acc_train: 0.8533 loss_val: 0.5034 acc_val: 0.7444 time: 0.0030s\n",
      "Epoch: 0331 loss_train: 0.3380 acc_train: 0.8133 loss_val: 0.5034 acc_val: 0.7444 time: 0.0028s\n",
      "Epoch: 0332 loss_train: 0.2979 acc_train: 0.8667 loss_val: 0.5041 acc_val: 0.7556 time: 0.0042s\n",
      "Epoch: 0333 loss_train: 0.3455 acc_train: 0.8533 loss_val: 0.5053 acc_val: 0.7667 time: 0.0035s\n",
      "Epoch: 0334 loss_train: 0.3610 acc_train: 0.8000 loss_val: 0.5069 acc_val: 0.7667 time: 0.0035s\n",
      "Epoch: 0335 loss_train: 0.3403 acc_train: 0.8667 loss_val: 0.5087 acc_val: 0.7889 time: 0.0035s\n",
      "Epoch: 0336 loss_train: 0.3554 acc_train: 0.8400 loss_val: 0.5105 acc_val: 0.7889 time: 0.0033s\n",
      "Epoch: 0337 loss_train: 0.3243 acc_train: 0.8267 loss_val: 0.5119 acc_val: 0.8111 time: 0.0032s\n",
      "Epoch: 0338 loss_train: 0.3369 acc_train: 0.8267 loss_val: 0.5118 acc_val: 0.8000 time: 0.0035s\n",
      "Epoch: 0339 loss_train: 0.3342 acc_train: 0.8400 loss_val: 0.5115 acc_val: 0.7889 time: 0.0034s\n",
      "Epoch: 0340 loss_train: 0.3341 acc_train: 0.8667 loss_val: 0.5105 acc_val: 0.7778 time: 0.0034s\n",
      "Epoch: 0341 loss_train: 0.3244 acc_train: 0.8400 loss_val: 0.5092 acc_val: 0.7556 time: 0.0039s\n",
      "Epoch: 0342 loss_train: 0.2759 acc_train: 0.8800 loss_val: 0.5080 acc_val: 0.7556 time: 0.0033s\n",
      "Epoch: 0343 loss_train: 0.3150 acc_train: 0.8533 loss_val: 0.5073 acc_val: 0.7667 time: 0.0034s\n",
      "Epoch: 0344 loss_train: 0.3316 acc_train: 0.8933 loss_val: 0.5071 acc_val: 0.7556 time: 0.0033s\n",
      "Epoch: 0345 loss_train: 0.3088 acc_train: 0.8133 loss_val: 0.5083 acc_val: 0.7556 time: 0.0034s\n",
      "Epoch: 0346 loss_train: 0.2938 acc_train: 0.8533 loss_val: 0.5099 acc_val: 0.7444 time: 0.0035s\n",
      "Epoch: 0347 loss_train: 0.3215 acc_train: 0.8667 loss_val: 0.5112 acc_val: 0.7444 time: 0.0034s\n",
      "Epoch: 0348 loss_train: 0.3282 acc_train: 0.8133 loss_val: 0.5118 acc_val: 0.7556 time: 0.0036s\n",
      "Epoch: 0349 loss_train: 0.3856 acc_train: 0.8133 loss_val: 0.5103 acc_val: 0.7556 time: 0.0036s\n",
      "Epoch: 0350 loss_train: 0.3583 acc_train: 0.8400 loss_val: 0.5087 acc_val: 0.7556 time: 0.0034s\n",
      "Epoch: 0351 loss_train: 0.3176 acc_train: 0.8400 loss_val: 0.5078 acc_val: 0.7556 time: 0.0033s\n",
      "Epoch: 0352 loss_train: 0.2977 acc_train: 0.8533 loss_val: 0.5082 acc_val: 0.7556 time: 0.0033s\n",
      "Epoch: 0353 loss_train: 0.3898 acc_train: 0.7867 loss_val: 0.5095 acc_val: 0.7778 time: 0.0032s\n",
      "Epoch: 0354 loss_train: 0.3229 acc_train: 0.8400 loss_val: 0.5115 acc_val: 0.7778 time: 0.0037s\n",
      "Epoch: 0355 loss_train: 0.3537 acc_train: 0.8533 loss_val: 0.5139 acc_val: 0.7889 time: 0.0032s\n",
      "Epoch: 0356 loss_train: 0.3340 acc_train: 0.8267 loss_val: 0.5176 acc_val: 0.8000 time: 0.0034s\n",
      "Epoch: 0357 loss_train: 0.3506 acc_train: 0.8400 loss_val: 0.5194 acc_val: 0.8000 time: 0.0036s\n",
      "Epoch: 0358 loss_train: 0.3160 acc_train: 0.8533 loss_val: 0.5204 acc_val: 0.8111 time: 0.0034s\n",
      "Epoch: 0359 loss_train: 0.3050 acc_train: 0.8667 loss_val: 0.5191 acc_val: 0.8111 time: 0.0036s\n",
      "Epoch: 0360 loss_train: 0.3564 acc_train: 0.8800 loss_val: 0.5160 acc_val: 0.8111 time: 0.0035s\n",
      "Epoch: 0361 loss_train: 0.3487 acc_train: 0.8133 loss_val: 0.5131 acc_val: 0.8000 time: 0.0032s\n",
      "Epoch: 0362 loss_train: 0.3513 acc_train: 0.8533 loss_val: 0.5099 acc_val: 0.7778 time: 0.0032s\n",
      "Epoch: 0363 loss_train: 0.2742 acc_train: 0.9467 loss_val: 0.5088 acc_val: 0.7778 time: 0.0034s\n",
      "Epoch: 0364 loss_train: 0.3380 acc_train: 0.8133 loss_val: 0.5080 acc_val: 0.7556 time: 0.0034s\n",
      "Epoch: 0365 loss_train: 0.3742 acc_train: 0.8133 loss_val: 0.5083 acc_val: 0.7556 time: 0.0038s\n",
      "Epoch: 0366 loss_train: 0.2613 acc_train: 0.9333 loss_val: 0.5098 acc_val: 0.7556 time: 0.0033s\n",
      "Epoch: 0367 loss_train: 0.2965 acc_train: 0.8667 loss_val: 0.5110 acc_val: 0.7556 time: 0.0032s\n",
      "Epoch: 0368 loss_train: 0.3072 acc_train: 0.8533 loss_val: 0.5116 acc_val: 0.7556 time: 0.0055s\n",
      "Epoch: 0369 loss_train: 0.2940 acc_train: 0.8933 loss_val: 0.5121 acc_val: 0.7556 time: 0.0057s\n",
      "Epoch: 0370 loss_train: 0.3770 acc_train: 0.8000 loss_val: 0.5117 acc_val: 0.7556 time: 0.0038s\n",
      "Epoch: 0371 loss_train: 0.2897 acc_train: 0.8667 loss_val: 0.5116 acc_val: 0.7556 time: 0.0034s\n",
      "Epoch: 0372 loss_train: 0.3353 acc_train: 0.8533 loss_val: 0.5118 acc_val: 0.7778 time: 0.0042s\n",
      "Epoch: 0373 loss_train: 0.3098 acc_train: 0.8533 loss_val: 0.5130 acc_val: 0.7778 time: 0.0054s\n",
      "Epoch: 0374 loss_train: 0.3573 acc_train: 0.8400 loss_val: 0.5144 acc_val: 0.7778 time: 0.0059s\n",
      "Epoch: 0375 loss_train: 0.3458 acc_train: 0.7733 loss_val: 0.5150 acc_val: 0.7778 time: 0.0064s\n",
      "Epoch: 0376 loss_train: 0.2818 acc_train: 0.8800 loss_val: 0.5156 acc_val: 0.7889 time: 0.0061s\n",
      "Epoch: 0377 loss_train: 0.2954 acc_train: 0.8667 loss_val: 0.5158 acc_val: 0.7889 time: 0.0055s\n",
      "Epoch: 0378 loss_train: 0.3418 acc_train: 0.8267 loss_val: 0.5153 acc_val: 0.7889 time: 0.0057s\n",
      "Epoch: 0379 loss_train: 0.3262 acc_train: 0.8533 loss_val: 0.5145 acc_val: 0.7778 time: 0.0116s\n",
      "Epoch: 0380 loss_train: 0.4246 acc_train: 0.7600 loss_val: 0.5145 acc_val: 0.7889 time: 0.0043s\n",
      "Epoch: 0381 loss_train: 0.3363 acc_train: 0.8267 loss_val: 0.5140 acc_val: 0.7778 time: 0.0036s\n",
      "Epoch: 0382 loss_train: 0.2903 acc_train: 0.8667 loss_val: 0.5129 acc_val: 0.7556 time: 0.0099s\n",
      "Epoch: 0383 loss_train: 0.2938 acc_train: 0.8667 loss_val: 0.5116 acc_val: 0.7556 time: 0.0061s\n",
      "Epoch: 0384 loss_train: 0.3756 acc_train: 0.8400 loss_val: 0.5108 acc_val: 0.7444 time: 0.0125s\n",
      "Epoch: 0385 loss_train: 0.3074 acc_train: 0.8667 loss_val: 0.5106 acc_val: 0.7444 time: 0.0068s\n",
      "Epoch: 0386 loss_train: 0.3301 acc_train: 0.8533 loss_val: 0.5107 acc_val: 0.7444 time: 0.0051s\n",
      "Epoch: 0387 loss_train: 0.3320 acc_train: 0.8533 loss_val: 0.5107 acc_val: 0.7556 time: 0.0036s\n",
      "Epoch: 0388 loss_train: 0.3469 acc_train: 0.8133 loss_val: 0.5109 acc_val: 0.7444 time: 0.0098s\n",
      "Epoch: 0389 loss_train: 0.3175 acc_train: 0.8533 loss_val: 0.5108 acc_val: 0.7444 time: 0.0046s\n",
      "Epoch: 0390 loss_train: 0.3698 acc_train: 0.8133 loss_val: 0.5100 acc_val: 0.7444 time: 0.0045s\n",
      "Epoch: 0391 loss_train: 0.3225 acc_train: 0.8400 loss_val: 0.5097 acc_val: 0.7444 time: 0.0109s\n",
      "Epoch: 0392 loss_train: 0.3411 acc_train: 0.8400 loss_val: 0.5100 acc_val: 0.7556 time: 0.0041s\n",
      "Epoch: 0393 loss_train: 0.3137 acc_train: 0.8400 loss_val: 0.5117 acc_val: 0.7778 time: 0.0102s\n",
      "Epoch: 0394 loss_train: 0.3137 acc_train: 0.8267 loss_val: 0.5141 acc_val: 0.7778 time: 0.0048s\n",
      "Epoch: 0395 loss_train: 0.3300 acc_train: 0.8267 loss_val: 0.5166 acc_val: 0.8000 time: 0.0112s\n",
      "Epoch: 0396 loss_train: 0.3228 acc_train: 0.8400 loss_val: 0.5191 acc_val: 0.8000 time: 0.0051s\n",
      "Epoch: 0397 loss_train: 0.3374 acc_train: 0.8667 loss_val: 0.5203 acc_val: 0.8000 time: 0.0130s\n",
      "Epoch: 0398 loss_train: 0.2911 acc_train: 0.9200 loss_val: 0.5210 acc_val: 0.8000 time: 0.0059s\n",
      "Epoch: 0399 loss_train: 0.3571 acc_train: 0.8667 loss_val: 0.5206 acc_val: 0.8000 time: 0.0116s\n",
      "Epoch: 0400 loss_train: 0.3503 acc_train: 0.8667 loss_val: 0.5184 acc_val: 0.8000 time: 0.0078s\n",
      "Epoch: 0401 loss_train: 0.3392 acc_train: 0.8533 loss_val: 0.5144 acc_val: 0.7778 time: 0.0087s\n",
      "Epoch: 0402 loss_train: 0.3296 acc_train: 0.8533 loss_val: 0.5123 acc_val: 0.7889 time: 0.0109s\n",
      "Epoch: 0403 loss_train: 0.3108 acc_train: 0.8933 loss_val: 0.5105 acc_val: 0.7778 time: 0.0069s\n",
      "Epoch: 0404 loss_train: 0.3038 acc_train: 0.8800 loss_val: 0.5083 acc_val: 0.7556 time: 0.0120s\n",
      "Epoch: 0405 loss_train: 0.3542 acc_train: 0.8533 loss_val: 0.5065 acc_val: 0.7556 time: 0.0117s\n",
      "Epoch: 0406 loss_train: 0.3255 acc_train: 0.8267 loss_val: 0.5059 acc_val: 0.7444 time: 0.0050s\n",
      "Epoch: 0407 loss_train: 0.3661 acc_train: 0.8000 loss_val: 0.5060 acc_val: 0.7444 time: 0.0105s\n",
      "Epoch: 0408 loss_train: 0.2808 acc_train: 0.8800 loss_val: 0.5059 acc_val: 0.7444 time: 0.0042s\n",
      "Epoch: 0409 loss_train: 0.2997 acc_train: 0.8933 loss_val: 0.5058 acc_val: 0.7556 time: 0.0085s\n",
      "Epoch: 0410 loss_train: 0.3432 acc_train: 0.8400 loss_val: 0.5065 acc_val: 0.7556 time: 0.0077s\n",
      "Epoch: 0411 loss_train: 0.3106 acc_train: 0.7867 loss_val: 0.5076 acc_val: 0.7556 time: 0.0114s\n",
      "Epoch: 0412 loss_train: 0.3383 acc_train: 0.8133 loss_val: 0.5093 acc_val: 0.7889 time: 0.0046s\n",
      "Epoch: 0413 loss_train: 0.3123 acc_train: 0.8267 loss_val: 0.5109 acc_val: 0.7889 time: 0.0083s\n",
      "Epoch: 0414 loss_train: 0.3423 acc_train: 0.8533 loss_val: 0.5118 acc_val: 0.7889 time: 0.0044s\n",
      "Epoch: 0415 loss_train: 0.3108 acc_train: 0.8533 loss_val: 0.5120 acc_val: 0.7889 time: 0.0033s\n",
      "Epoch: 0416 loss_train: 0.3000 acc_train: 0.8667 loss_val: 0.5151 acc_val: 0.7889 time: 0.0069s\n",
      "Epoch: 0417 loss_train: 0.3340 acc_train: 0.8667 loss_val: 0.5133 acc_val: 0.7778 time: 0.0043s\n",
      "Epoch: 0418 loss_train: 0.3044 acc_train: 0.8400 loss_val: 0.5125 acc_val: 0.7778 time: 0.0030s\n",
      "Epoch: 0419 loss_train: 0.3369 acc_train: 0.8533 loss_val: 0.5116 acc_val: 0.7778 time: 0.0060s\n",
      "Epoch: 0420 loss_train: 0.3091 acc_train: 0.8533 loss_val: 0.5109 acc_val: 0.7778 time: 0.0044s\n",
      "Epoch: 0421 loss_train: 0.3336 acc_train: 0.8667 loss_val: 0.5102 acc_val: 0.7778 time: 0.0035s\n",
      "Epoch: 0422 loss_train: 0.3216 acc_train: 0.8800 loss_val: 0.5096 acc_val: 0.7778 time: 0.0036s\n",
      "Epoch: 0423 loss_train: 0.2907 acc_train: 0.8533 loss_val: 0.5096 acc_val: 0.7778 time: 0.0065s\n",
      "Epoch: 0424 loss_train: 0.3051 acc_train: 0.8933 loss_val: 0.5099 acc_val: 0.7556 time: 0.0037s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0425 loss_train: 0.3305 acc_train: 0.8533 loss_val: 0.5103 acc_val: 0.7556 time: 0.0046s\n",
      "Epoch: 0426 loss_train: 0.3136 acc_train: 0.8400 loss_val: 0.5111 acc_val: 0.7556 time: 0.0043s\n",
      "Epoch: 0427 loss_train: 0.3356 acc_train: 0.8267 loss_val: 0.5120 acc_val: 0.7556 time: 0.0052s\n",
      "Epoch: 0428 loss_train: 0.3063 acc_train: 0.8667 loss_val: 0.5132 acc_val: 0.7778 time: 0.0053s\n",
      "Epoch: 0429 loss_train: 0.2976 acc_train: 0.8400 loss_val: 0.5146 acc_val: 0.7778 time: 0.0063s\n",
      "Epoch: 0430 loss_train: 0.3513 acc_train: 0.8267 loss_val: 0.5154 acc_val: 0.7778 time: 0.0037s\n",
      "Optimization Finished!\n",
      "Total time elapsed: 2.0845s\n",
      "Test set results: loss= 0.2259 accuracy= 0.8696\n"
     ]
    }
   ],
   "source": [
    "cuda = False\n",
    "hidden = 16\n",
    "dropout = 0.5\n",
    "lr = 0.01\n",
    "weight_decay = 5e-4\n",
    "fastmode = False\n",
    "epochs = 430\n",
    "\n",
    "# Model and optimizer\n",
    "model = GCN(nfeat=features.shape[1],\n",
    "            nhid=hidden,\n",
    "            nclass=labels.max().item() + 1,\n",
    "            dropout=dropout)\n",
    "optimizer = optim.Adam(model.parameters(),\n",
    "                       lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "if cuda:\n",
    "    model.cuda()\n",
    "    features = features.cuda()\n",
    "    adj = adj.cuda()\n",
    "    labels = labels.cuda()\n",
    "    idx_train = idx_train.cuda()\n",
    "    idx_val = idx_val.cuda()\n",
    "    idx_test = idx_test.cuda()\n",
    "\n",
    "\n",
    "def train(epoch):\n",
    "    t = time.time()\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(features, adj)\n",
    "    loss_train = F.nll_loss(output[idx_train], all_labels[idx_train])\n",
    "    acc_train = accuracy(output[idx_train], all_labels[idx_train])\n",
    "    loss_train.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if not fastmode:\n",
    "        # Evaluate validation set performance separately,\n",
    "        # deactivates dropout during validation run.\n",
    "        model.eval()\n",
    "        output = model(features, adj)\n",
    "\n",
    "    loss_val = F.nll_loss(output[idx_val], all_labels[idx_val])\n",
    "    acc_val = accuracy(output[idx_val], all_labels[idx_val])\n",
    "    print('Epoch: {:04d}'.format(epoch+1),\n",
    "          'loss_train: {:.4f}'.format(loss_train.item()),\n",
    "          'acc_train: {:.4f}'.format(acc_train.item()),\n",
    "          'loss_val: {:.4f}'.format(loss_val.item()),\n",
    "          'acc_val: {:.4f}'.format(acc_val.item()),\n",
    "          'time: {:.4f}s'.format(time.time() - t))\n",
    "\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    output = model(features, adj)\n",
    "    loss_test = F.nll_loss(output[idx_test], all_labels[idx_test])\n",
    "    acc_test = accuracy(output[idx_test], all_labels[idx_test])\n",
    "    print(\"Test set results:\",\n",
    "          \"loss= {:.4f}\".format(loss_test.item()),\n",
    "          \"accuracy= {:.4f}\".format(acc_test.item()))\n",
    "\n",
    "\n",
    "# Train model\n",
    "t_total = time.time()\n",
    "for epoch in range(epochs):\n",
    "    train(epoch)\n",
    "print(\"Optimization Finished!\")\n",
    "print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))\n",
    "\n",
    "# Testing\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
