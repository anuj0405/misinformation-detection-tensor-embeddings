{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying AGNN From PyTorch Geometric to Fake News Dataset (BuzzFeed Political News Dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.ArticlesHandler import ArticlesHandler\n",
    "from utils import solve, embedding_matrix_2_kNN, get_rate, accuracy, precision, recall, f1_score\n",
    "from utils import Config\n",
    "import time\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "from postprocessing.SelectLabelsPostprocessor import SelectLabelsPostprocessor\n",
    "from pygcn.utils import encode_onehot, accuracy, load_from_features\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.utils import sparse as S\n",
    "#from model import AGNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method of decomposition: GloVe\n"
     ]
    }
   ],
   "source": [
    "config = Config(file='config')\n",
    "\n",
    "assert (config.num_fake_articles + config.num_real_articles > \n",
    "        config.num_nearest_neighbours), \"Can't have more neighbours than nodes!\"\n",
    "\n",
    "print(\"Method of decomposition:\", config.method_decomposition_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset Buzzfeed Political News Dataset\n",
      "Performing decomposition...\n",
      "Loading Glove Model\n",
      "Done. 400000  words loaded!\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading dataset\", config.dataset_name)\n",
    "articles = ArticlesHandler(config)\n",
    "\n",
    "print(\"Performing decomposition...\")\n",
    "C = articles.get_tensor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.set(\"num_unknown_labels\", 195)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = articles.articles.labels\n",
    "all_labels = articles.articles.labels_untouched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj, features, all_labels = load_from_features(C, all_labels, config)\n",
    "_, _, labels = load_from_features(C, labels, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(labels)\n",
    "idx_train = np.where(labels)[0]\n",
    "idx_val = np.where(1 - abs(labels))[0][:90]\n",
    "idx_test = np.where(1 - abs(labels))[0][90:]\n",
    "\n",
    "#print(len(idx_train))\n",
    "\n",
    "idx_train = torch.LongTensor(idx_train)\n",
    "idx_val = torch.LongTensor(idx_val)\n",
    "idx_test = torch.LongTensor(idx_test)\n",
    "\n",
    "num_features = features.shape[1]\n",
    "num_classes = 2\n",
    "\n",
    "#--------Converting Adjacency Matrix into the form that is compatible with PyTorch Geometric ------------------------------\n",
    "adj_dense = adj.to_dense()\n",
    "index, value = S.dense_to_sparse(adj_dense)\n",
    "\n",
    "#print(index.shape)\n",
    "#print(num_features)\n",
    "\n",
    "data = Data(x=features, edge_index=index)\n",
    "\n",
    "#print(data.x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch_Geometric AGNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.nn import AGNNConv\n",
    "import torch.nn as nn\n",
    "from torch.nn.parameter import Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.lin1 = torch.nn.Linear(num_features, 16)\n",
    "            \n",
    "        self.prop1 = AGNNConv(requires_grad=False)\n",
    "        self.prop2 = AGNNConv(requires_grad=True)\n",
    "        self.lin2 = torch.nn.Linear(16, num_classes)\n",
    "\n",
    "        self.prop1_mat_weight = torch.tensor(0) \n",
    "        self.prop2_mat_weight = torch.tensor(0)\n",
    "\n",
    "        self.edge_index_with_self_lopp = torch.tensor(0)\n",
    "\n",
    "    def forward(self):\n",
    "        x = F.dropout(data.x,0.5,training=self.training)\n",
    "        x = F.relu(self.lin1(data.x))\n",
    "\n",
    "        self.edge_index_with_self_loop , self.prop1_mat_weight = self.prop1.propagation_matrix(x, data.edge_index)\n",
    "        x = self.prop1(x, data.edge_index)\n",
    "\n",
    "        _ , self.prop2_mat_weight = self.prop2.propagation_matrix(x, data.edge_index)\n",
    "        x = self.prop2(x, data.edge_index)\n",
    "        \n",
    "        x = F.dropout(x,0.5,training=self.training)\n",
    "        x = self.lin2(x)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#device = 'cpu'\n",
    "model = Net().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "\n",
    "\n",
    "def train(epoch):\n",
    "    t = time.time()\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model()\n",
    "    \n",
    "    loss_train = F.nll_loss(output[idx_train], all_labels[idx_train])\n",
    "    acc_train = accuracy(output[idx_train], all_labels[idx_train])\n",
    "    loss_train.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    loss_val = F.nll_loss(output[idx_val], all_labels[idx_val])\n",
    "    acc_val = accuracy(output[idx_val], all_labels[idx_val])\n",
    "    print('Epoch: {:04d}'.format(epoch+1),\n",
    "          'loss_train: {:.4f}'.format(loss_train.item()),\n",
    "          'acc_train: {:.4f}'.format(acc_train.item()),\n",
    "          'loss_val: {:.4f}'.format(loss_val.item()),\n",
    "          'acc_val: {:.4f}'.format(acc_val.item()),\n",
    "          'time: {:.4f}s'.format(time.time() - t))\n",
    "\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    output = model()\n",
    "    loss_test = F.nll_loss(output[idx_test], all_labels[idx_test])\n",
    "    acc_test = accuracy(output[idx_test], all_labels[idx_test])\n",
    "    print(\"Test set results:\",\n",
    "          \"loss= {:.4f}\".format(loss_test.item()),\n",
    "          \"accuracy= {:.4f}\".format(acc_test.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training on FakeNews Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 loss_train: 0.6939 acc_train: 0.5000 loss_val: 0.6913 acc_val: 0.5000 time: 0.0064s\n",
      "Epoch: 0002 loss_train: 0.6991 acc_train: 0.4000 loss_val: 0.6967 acc_val: 0.4667 time: 0.0074s\n",
      "Epoch: 0003 loss_train: 0.6960 acc_train: 0.4500 loss_val: 0.7003 acc_val: 0.4556 time: 0.0077s\n",
      "Epoch: 0004 loss_train: 0.6940 acc_train: 0.5000 loss_val: 0.6963 acc_val: 0.4667 time: 0.0106s\n",
      "Epoch: 0005 loss_train: 0.7012 acc_train: 0.4500 loss_val: 0.7036 acc_val: 0.4111 time: 0.0066s\n",
      "Epoch: 0006 loss_train: 0.6916 acc_train: 0.5250 loss_val: 0.7054 acc_val: 0.3778 time: 0.0063s\n",
      "Epoch: 0007 loss_train: 0.6973 acc_train: 0.4750 loss_val: 0.6909 acc_val: 0.5444 time: 0.0071s\n",
      "Epoch: 0008 loss_train: 0.6901 acc_train: 0.6000 loss_val: 0.6910 acc_val: 0.5222 time: 0.0068s\n",
      "Epoch: 0009 loss_train: 0.6979 acc_train: 0.4250 loss_val: 0.6925 acc_val: 0.4778 time: 0.0066s\n",
      "Epoch: 0010 loss_train: 0.6894 acc_train: 0.5500 loss_val: 0.6924 acc_val: 0.5000 time: 0.0067s\n",
      "Epoch: 0011 loss_train: 0.6870 acc_train: 0.5250 loss_val: 0.6904 acc_val: 0.5222 time: 0.0066s\n",
      "Epoch: 0012 loss_train: 0.6885 acc_train: 0.5500 loss_val: 0.6946 acc_val: 0.5111 time: 0.0068s\n",
      "Epoch: 0013 loss_train: 0.6967 acc_train: 0.5250 loss_val: 0.6882 acc_val: 0.5556 time: 0.0067s\n",
      "Epoch: 0014 loss_train: 0.6933 acc_train: 0.5500 loss_val: 0.6867 acc_val: 0.6000 time: 0.0068s\n",
      "Epoch: 0015 loss_train: 0.7049 acc_train: 0.4000 loss_val: 0.6937 acc_val: 0.5222 time: 0.0067s\n",
      "Epoch: 0016 loss_train: 0.7124 acc_train: 0.3500 loss_val: 0.7106 acc_val: 0.4444 time: 0.0067s\n",
      "Epoch: 0017 loss_train: 0.6833 acc_train: 0.6000 loss_val: 0.6914 acc_val: 0.5000 time: 0.0068s\n",
      "Epoch: 0018 loss_train: 0.6953 acc_train: 0.5500 loss_val: 0.6949 acc_val: 0.5333 time: 0.0067s\n",
      "Epoch: 0019 loss_train: 0.6865 acc_train: 0.5000 loss_val: 0.6942 acc_val: 0.4667 time: 0.0073s\n",
      "Epoch: 0020 loss_train: 0.6887 acc_train: 0.5000 loss_val: 0.6904 acc_val: 0.5111 time: 0.0067s\n",
      "Epoch: 0021 loss_train: 0.6873 acc_train: 0.5000 loss_val: 0.6897 acc_val: 0.5111 time: 0.0067s\n",
      "Epoch: 0022 loss_train: 0.6780 acc_train: 0.6500 loss_val: 0.6964 acc_val: 0.4667 time: 0.0067s\n",
      "Epoch: 0023 loss_train: 0.6851 acc_train: 0.5250 loss_val: 0.6921 acc_val: 0.5444 time: 0.0067s\n",
      "Epoch: 0024 loss_train: 0.7068 acc_train: 0.4000 loss_val: 0.6888 acc_val: 0.5000 time: 0.0070s\n",
      "Epoch: 0025 loss_train: 0.6896 acc_train: 0.5250 loss_val: 0.7041 acc_val: 0.4111 time: 0.0070s\n",
      "Epoch: 0026 loss_train: 0.6909 acc_train: 0.4500 loss_val: 0.7083 acc_val: 0.4778 time: 0.0067s\n",
      "Epoch: 0027 loss_train: 0.6991 acc_train: 0.4750 loss_val: 0.6987 acc_val: 0.5111 time: 0.0067s\n",
      "Epoch: 0028 loss_train: 0.6915 acc_train: 0.5000 loss_val: 0.6897 acc_val: 0.5444 time: 0.0067s\n",
      "Epoch: 0029 loss_train: 0.6897 acc_train: 0.5000 loss_val: 0.7014 acc_val: 0.4667 time: 0.0068s\n",
      "Epoch: 0030 loss_train: 0.6943 acc_train: 0.4750 loss_val: 0.6960 acc_val: 0.4667 time: 0.0114s\n",
      "Epoch: 0031 loss_train: 0.6717 acc_train: 0.6500 loss_val: 0.6979 acc_val: 0.5111 time: 0.0117s\n",
      "Epoch: 0032 loss_train: 0.7001 acc_train: 0.3750 loss_val: 0.6859 acc_val: 0.5556 time: 0.0080s\n",
      "Epoch: 0033 loss_train: 0.6862 acc_train: 0.5250 loss_val: 0.6925 acc_val: 0.4778 time: 0.0071s\n",
      "Epoch: 0034 loss_train: 0.6922 acc_train: 0.5000 loss_val: 0.6920 acc_val: 0.5222 time: 0.0069s\n",
      "Epoch: 0035 loss_train: 0.6893 acc_train: 0.5500 loss_val: 0.6825 acc_val: 0.6000 time: 0.0067s\n",
      "Epoch: 0036 loss_train: 0.6801 acc_train: 0.5750 loss_val: 0.6907 acc_val: 0.5000 time: 0.0067s\n",
      "Epoch: 0037 loss_train: 0.7049 acc_train: 0.4250 loss_val: 0.6880 acc_val: 0.4667 time: 0.0067s\n",
      "Epoch: 0038 loss_train: 0.6840 acc_train: 0.5500 loss_val: 0.6784 acc_val: 0.5667 time: 0.0067s\n",
      "Epoch: 0039 loss_train: 0.6766 acc_train: 0.6000 loss_val: 0.6780 acc_val: 0.5333 time: 0.0067s\n",
      "Epoch: 0040 loss_train: 0.6760 acc_train: 0.5250 loss_val: 0.6903 acc_val: 0.4889 time: 0.0067s\n",
      "Epoch: 0041 loss_train: 0.6879 acc_train: 0.5250 loss_val: 0.6948 acc_val: 0.4667 time: 0.0068s\n",
      "Epoch: 0042 loss_train: 0.6700 acc_train: 0.6250 loss_val: 0.6886 acc_val: 0.4889 time: 0.0067s\n",
      "Epoch: 0043 loss_train: 0.6965 acc_train: 0.4500 loss_val: 0.6946 acc_val: 0.4222 time: 0.0067s\n",
      "Epoch: 0044 loss_train: 0.6850 acc_train: 0.5000 loss_val: 0.6755 acc_val: 0.5444 time: 0.0067s\n",
      "Epoch: 0045 loss_train: 0.6833 acc_train: 0.5500 loss_val: 0.6803 acc_val: 0.5222 time: 0.0067s\n",
      "Epoch: 0046 loss_train: 0.6570 acc_train: 0.7000 loss_val: 0.6869 acc_val: 0.5333 time: 0.0067s\n",
      "Epoch: 0047 loss_train: 0.6988 acc_train: 0.4750 loss_val: 0.6819 acc_val: 0.5444 time: 0.0066s\n",
      "Epoch: 0048 loss_train: 0.6877 acc_train: 0.5000 loss_val: 0.7160 acc_val: 0.4222 time: 0.0068s\n",
      "Epoch: 0049 loss_train: 0.6815 acc_train: 0.5750 loss_val: 0.7257 acc_val: 0.3889 time: 0.0069s\n",
      "Epoch: 0050 loss_train: 0.7096 acc_train: 0.4500 loss_val: 0.6940 acc_val: 0.4778 time: 0.0067s\n",
      "Epoch: 0051 loss_train: 0.6940 acc_train: 0.5000 loss_val: 0.6751 acc_val: 0.5444 time: 0.0067s\n",
      "Epoch: 0052 loss_train: 0.6568 acc_train: 0.6250 loss_val: 0.7114 acc_val: 0.4333 time: 0.0067s\n",
      "Epoch: 0053 loss_train: 0.6776 acc_train: 0.5250 loss_val: 0.7179 acc_val: 0.4222 time: 0.0067s\n",
      "Epoch: 0054 loss_train: 0.6713 acc_train: 0.5000 loss_val: 0.7037 acc_val: 0.4667 time: 0.0067s\n",
      "Epoch: 0055 loss_train: 0.6690 acc_train: 0.6000 loss_val: 0.7117 acc_val: 0.4556 time: 0.0067s\n",
      "Epoch: 0056 loss_train: 0.7171 acc_train: 0.4250 loss_val: 0.6847 acc_val: 0.5444 time: 0.0093s\n",
      "Epoch: 0057 loss_train: 0.7067 acc_train: 0.4500 loss_val: 0.6862 acc_val: 0.5667 time: 0.0068s\n",
      "Epoch: 0058 loss_train: 0.6671 acc_train: 0.6000 loss_val: 0.7046 acc_val: 0.4778 time: 0.0072s\n",
      "Epoch: 0059 loss_train: 0.6875 acc_train: 0.4750 loss_val: 0.7020 acc_val: 0.4778 time: 0.0072s\n",
      "Epoch: 0060 loss_train: 0.6894 acc_train: 0.4750 loss_val: 0.6712 acc_val: 0.5889 time: 0.0068s\n",
      "Epoch: 0061 loss_train: 0.6828 acc_train: 0.5500 loss_val: 0.6732 acc_val: 0.5889 time: 0.0079s\n",
      "Epoch: 0062 loss_train: 0.6953 acc_train: 0.5000 loss_val: 0.6870 acc_val: 0.5111 time: 0.0070s\n",
      "Epoch: 0063 loss_train: 0.7068 acc_train: 0.4500 loss_val: 0.6970 acc_val: 0.5111 time: 0.0080s\n",
      "Epoch: 0064 loss_train: 0.6870 acc_train: 0.5250 loss_val: 0.6894 acc_val: 0.5778 time: 0.0068s\n",
      "Epoch: 0065 loss_train: 0.6883 acc_train: 0.6000 loss_val: 0.6978 acc_val: 0.5222 time: 0.0067s\n",
      "Epoch: 0066 loss_train: 0.6916 acc_train: 0.5250 loss_val: 0.6920 acc_val: 0.5444 time: 0.0068s\n",
      "Epoch: 0067 loss_train: 0.6823 acc_train: 0.5750 loss_val: 0.6939 acc_val: 0.5000 time: 0.0067s\n",
      "Epoch: 0068 loss_train: 0.6963 acc_train: 0.5250 loss_val: 0.6979 acc_val: 0.4222 time: 0.0068s\n",
      "Epoch: 0069 loss_train: 0.6823 acc_train: 0.5500 loss_val: 0.6956 acc_val: 0.4667 time: 0.0066s\n",
      "Epoch: 0070 loss_train: 0.6817 acc_train: 0.5500 loss_val: 0.6939 acc_val: 0.5000 time: 0.0066s\n",
      "Epoch: 0071 loss_train: 0.6878 acc_train: 0.5500 loss_val: 0.6941 acc_val: 0.4667 time: 0.0067s\n",
      "Epoch: 0072 loss_train: 0.6820 acc_train: 0.6000 loss_val: 0.6923 acc_val: 0.4778 time: 0.0067s\n",
      "Epoch: 0073 loss_train: 0.6747 acc_train: 0.6000 loss_val: 0.6899 acc_val: 0.5333 time: 0.0067s\n",
      "Epoch: 0074 loss_train: 0.6700 acc_train: 0.7250 loss_val: 0.6750 acc_val: 0.6333 time: 0.0067s\n",
      "Epoch: 0075 loss_train: 0.6682 acc_train: 0.6250 loss_val: 0.6772 acc_val: 0.6667 time: 0.0067s\n",
      "Epoch: 0076 loss_train: 0.6562 acc_train: 0.7250 loss_val: 0.6859 acc_val: 0.5556 time: 0.0066s\n",
      "Epoch: 0077 loss_train: 0.6966 acc_train: 0.4750 loss_val: 0.6759 acc_val: 0.5667 time: 0.0066s\n",
      "Epoch: 0078 loss_train: 0.6843 acc_train: 0.5750 loss_val: 0.6704 acc_val: 0.6333 time: 0.0066s\n",
      "Epoch: 0079 loss_train: 0.6703 acc_train: 0.5000 loss_val: 0.6616 acc_val: 0.6111 time: 0.0066s\n",
      "Epoch: 0080 loss_train: 0.6831 acc_train: 0.5000 loss_val: 0.6753 acc_val: 0.5333 time: 0.0067s\n",
      "Epoch: 0081 loss_train: 0.6529 acc_train: 0.5750 loss_val: 0.6912 acc_val: 0.5444 time: 0.0066s\n",
      "Epoch: 0082 loss_train: 0.6626 acc_train: 0.5750 loss_val: 0.6617 acc_val: 0.6000 time: 0.0067s\n",
      "Epoch: 0083 loss_train: 0.6345 acc_train: 0.6750 loss_val: 0.6894 acc_val: 0.5556 time: 0.0066s\n",
      "Epoch: 0084 loss_train: 0.6242 acc_train: 0.7250 loss_val: 0.7105 acc_val: 0.5111 time: 0.0067s\n",
      "Epoch: 0085 loss_train: 0.6558 acc_train: 0.6000 loss_val: 0.7079 acc_val: 0.4444 time: 0.0094s\n",
      "Epoch: 0086 loss_train: 0.6305 acc_train: 0.7000 loss_val: 0.7445 acc_val: 0.4111 time: 0.0067s\n",
      "Epoch: 0087 loss_train: 0.6448 acc_train: 0.6250 loss_val: 0.7115 acc_val: 0.5111 time: 0.0070s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0088 loss_train: 0.6483 acc_train: 0.6250 loss_val: 0.7515 acc_val: 0.4333 time: 0.0101s\n",
      "Epoch: 0089 loss_train: 0.6702 acc_train: 0.5500 loss_val: 0.6842 acc_val: 0.5444 time: 0.0113s\n",
      "Epoch: 0090 loss_train: 0.6673 acc_train: 0.5250 loss_val: 0.7050 acc_val: 0.5444 time: 0.0070s\n",
      "Epoch: 0091 loss_train: 0.7229 acc_train: 0.4750 loss_val: 0.7302 acc_val: 0.4556 time: 0.0066s\n",
      "Epoch: 0092 loss_train: 0.7466 acc_train: 0.4500 loss_val: 0.6828 acc_val: 0.5222 time: 0.0068s\n",
      "Epoch: 0093 loss_train: 0.6565 acc_train: 0.6750 loss_val: 0.6942 acc_val: 0.5000 time: 0.0066s\n",
      "Epoch: 0094 loss_train: 0.7141 acc_train: 0.5000 loss_val: 0.7402 acc_val: 0.4889 time: 0.0066s\n",
      "Epoch: 0095 loss_train: 0.6676 acc_train: 0.6250 loss_val: 0.7074 acc_val: 0.5333 time: 0.0067s\n",
      "Epoch: 0096 loss_train: 0.7233 acc_train: 0.4250 loss_val: 0.6627 acc_val: 0.6111 time: 0.0072s\n",
      "Epoch: 0097 loss_train: 0.6886 acc_train: 0.4750 loss_val: 0.6747 acc_val: 0.6000 time: 0.0069s\n",
      "Epoch: 0098 loss_train: 0.6744 acc_train: 0.5500 loss_val: 0.6953 acc_val: 0.5444 time: 0.0069s\n",
      "Epoch: 0099 loss_train: 0.6685 acc_train: 0.6750 loss_val: 0.6906 acc_val: 0.5556 time: 0.0072s\n",
      "Epoch: 0100 loss_train: 0.6668 acc_train: 0.6000 loss_val: 0.6897 acc_val: 0.5444 time: 0.0075s\n",
      "Epoch: 0101 loss_train: 0.6706 acc_train: 0.5500 loss_val: 0.6979 acc_val: 0.4667 time: 0.0070s\n",
      "Epoch: 0102 loss_train: 0.6636 acc_train: 0.6250 loss_val: 0.6937 acc_val: 0.4556 time: 0.0073s\n",
      "Epoch: 0103 loss_train: 0.6879 acc_train: 0.5750 loss_val: 0.7067 acc_val: 0.4667 time: 0.0071s\n",
      "Epoch: 0104 loss_train: 0.6590 acc_train: 0.6750 loss_val: 0.6921 acc_val: 0.5111 time: 0.0073s\n",
      "Epoch: 0105 loss_train: 0.6680 acc_train: 0.6000 loss_val: 0.7153 acc_val: 0.4889 time: 0.0068s\n",
      "Epoch: 0106 loss_train: 0.6854 acc_train: 0.4750 loss_val: 0.7111 acc_val: 0.4444 time: 0.0072s\n",
      "Epoch: 0107 loss_train: 0.6925 acc_train: 0.5000 loss_val: 0.6883 acc_val: 0.5556 time: 0.0071s\n",
      "Epoch: 0108 loss_train: 0.6875 acc_train: 0.4750 loss_val: 0.6628 acc_val: 0.6000 time: 0.0070s\n",
      "Epoch: 0109 loss_train: 0.6946 acc_train: 0.4250 loss_val: 0.6793 acc_val: 0.5444 time: 0.0071s\n",
      "Epoch: 0110 loss_train: 0.6789 acc_train: 0.4500 loss_val: 0.6718 acc_val: 0.5667 time: 0.0071s\n",
      "Epoch: 0111 loss_train: 0.6791 acc_train: 0.5250 loss_val: 0.6778 acc_val: 0.5667 time: 0.0073s\n",
      "Epoch: 0112 loss_train: 0.6814 acc_train: 0.5000 loss_val: 0.6759 acc_val: 0.5667 time: 0.0068s\n",
      "Epoch: 0113 loss_train: 0.6966 acc_train: 0.5000 loss_val: 0.6751 acc_val: 0.5667 time: 0.0069s\n",
      "Epoch: 0114 loss_train: 0.6902 acc_train: 0.5000 loss_val: 0.6909 acc_val: 0.5667 time: 0.0072s\n",
      "Epoch: 0115 loss_train: 0.6993 acc_train: 0.3500 loss_val: 0.6756 acc_val: 0.5111 time: 0.0066s\n",
      "Epoch: 0116 loss_train: 0.6911 acc_train: 0.4250 loss_val: 0.6763 acc_val: 0.5778 time: 0.0072s\n",
      "Epoch: 0117 loss_train: 0.6803 acc_train: 0.6750 loss_val: 0.6936 acc_val: 0.4222 time: 0.0063s\n",
      "Epoch: 0118 loss_train: 0.6787 acc_train: 0.6250 loss_val: 0.6827 acc_val: 0.5778 time: 0.0113s\n",
      "Epoch: 0119 loss_train: 0.6893 acc_train: 0.4750 loss_val: 0.6882 acc_val: 0.5667 time: 0.0073s\n",
      "Epoch: 0120 loss_train: 0.6788 acc_train: 0.5750 loss_val: 0.6924 acc_val: 0.4778 time: 0.0076s\n",
      "Epoch: 0121 loss_train: 0.6834 acc_train: 0.5750 loss_val: 0.6941 acc_val: 0.4778 time: 0.0073s\n",
      "Epoch: 0122 loss_train: 0.6775 acc_train: 0.5750 loss_val: 0.6878 acc_val: 0.5444 time: 0.0128s\n",
      "Epoch: 0123 loss_train: 0.6868 acc_train: 0.5000 loss_val: 0.6894 acc_val: 0.5222 time: 0.0070s\n",
      "Epoch: 0124 loss_train: 0.6898 acc_train: 0.5250 loss_val: 0.6919 acc_val: 0.5000 time: 0.0071s\n",
      "Epoch: 0125 loss_train: 0.6638 acc_train: 0.7250 loss_val: 0.6891 acc_val: 0.5444 time: 0.0063s\n",
      "Epoch: 0126 loss_train: 0.6846 acc_train: 0.5750 loss_val: 0.6932 acc_val: 0.4667 time: 0.0063s\n",
      "Epoch: 0127 loss_train: 0.6627 acc_train: 0.6750 loss_val: 0.6985 acc_val: 0.3889 time: 0.0071s\n",
      "Epoch: 0128 loss_train: 0.6708 acc_train: 0.6500 loss_val: 0.6882 acc_val: 0.4333 time: 0.0068s\n",
      "Epoch: 0129 loss_train: 0.6660 acc_train: 0.6000 loss_val: 0.6508 acc_val: 0.5667 time: 0.0070s\n",
      "Epoch: 0130 loss_train: 0.6959 acc_train: 0.4000 loss_val: 0.6873 acc_val: 0.4667 time: 0.0068s\n",
      "Epoch: 0131 loss_train: 0.6686 acc_train: 0.5250 loss_val: 0.6769 acc_val: 0.5111 time: 0.0071s\n",
      "Epoch: 0132 loss_train: 0.6762 acc_train: 0.5000 loss_val: 0.6670 acc_val: 0.5444 time: 0.0068s\n",
      "Epoch: 0133 loss_train: 0.6442 acc_train: 0.6750 loss_val: 0.6966 acc_val: 0.4222 time: 0.0075s\n",
      "Epoch: 0134 loss_train: 0.6889 acc_train: 0.5000 loss_val: 0.6659 acc_val: 0.5111 time: 0.0069s\n",
      "Epoch: 0135 loss_train: 0.6896 acc_train: 0.4500 loss_val: 0.6634 acc_val: 0.5444 time: 0.0068s\n",
      "Epoch: 0136 loss_train: 0.6891 acc_train: 0.5250 loss_val: 0.6952 acc_val: 0.4778 time: 0.0062s\n",
      "Epoch: 0137 loss_train: 0.6619 acc_train: 0.5750 loss_val: 0.6789 acc_val: 0.5444 time: 0.0061s\n",
      "Epoch: 0138 loss_train: 0.6837 acc_train: 0.6000 loss_val: 0.6699 acc_val: 0.5556 time: 0.0063s\n",
      "Epoch: 0139 loss_train: 0.7104 acc_train: 0.4750 loss_val: 0.6893 acc_val: 0.5111 time: 0.0062s\n",
      "Epoch: 0140 loss_train: 0.6755 acc_train: 0.6250 loss_val: 0.6799 acc_val: 0.5778 time: 0.0072s\n",
      "Epoch: 0141 loss_train: 0.6528 acc_train: 0.6250 loss_val: 0.6880 acc_val: 0.6000 time: 0.0071s\n",
      "Epoch: 0142 loss_train: 0.6757 acc_train: 0.5500 loss_val: 0.7019 acc_val: 0.5222 time: 0.0073s\n",
      "Epoch: 0143 loss_train: 0.6832 acc_train: 0.5000 loss_val: 0.6976 acc_val: 0.5111 time: 0.0085s\n",
      "Epoch: 0144 loss_train: 0.6802 acc_train: 0.5750 loss_val: 0.6838 acc_val: 0.5889 time: 0.0070s\n",
      "Epoch: 0145 loss_train: 0.6831 acc_train: 0.6000 loss_val: 0.6906 acc_val: 0.5333 time: 0.0095s\n",
      "Epoch: 0146 loss_train: 0.6979 acc_train: 0.5000 loss_val: 0.6875 acc_val: 0.5556 time: 0.0081s\n",
      "Epoch: 0147 loss_train: 0.6531 acc_train: 0.7000 loss_val: 0.6779 acc_val: 0.6111 time: 0.0064s\n",
      "Epoch: 0148 loss_train: 0.6477 acc_train: 0.7750 loss_val: 0.7068 acc_val: 0.4556 time: 0.0069s\n",
      "Epoch: 0149 loss_train: 0.6649 acc_train: 0.6000 loss_val: 0.6848 acc_val: 0.5222 time: 0.0075s\n",
      "Epoch: 0150 loss_train: 0.6470 acc_train: 0.6000 loss_val: 0.6580 acc_val: 0.5889 time: 0.0062s\n",
      "Epoch: 0151 loss_train: 0.6994 acc_train: 0.4500 loss_val: 0.6938 acc_val: 0.4556 time: 0.0085s\n",
      "Epoch: 0152 loss_train: 0.6400 acc_train: 0.6250 loss_val: 0.6563 acc_val: 0.5778 time: 0.0071s\n",
      "Epoch: 0153 loss_train: 0.6797 acc_train: 0.5000 loss_val: 0.6652 acc_val: 0.5556 time: 0.0087s\n",
      "Epoch: 0154 loss_train: 0.6909 acc_train: 0.5000 loss_val: 0.7005 acc_val: 0.4333 time: 0.0073s\n",
      "Epoch: 0155 loss_train: 0.6681 acc_train: 0.5500 loss_val: 0.6874 acc_val: 0.4778 time: 0.0095s\n",
      "Epoch: 0156 loss_train: 0.6834 acc_train: 0.5750 loss_val: 0.6913 acc_val: 0.4778 time: 0.0067s\n",
      "Epoch: 0157 loss_train: 0.6786 acc_train: 0.5750 loss_val: 0.6836 acc_val: 0.5000 time: 0.0088s\n",
      "Epoch: 0158 loss_train: 0.6836 acc_train: 0.5250 loss_val: 0.6826 acc_val: 0.5333 time: 0.0069s\n",
      "Epoch: 0159 loss_train: 0.6601 acc_train: 0.7000 loss_val: 0.6956 acc_val: 0.4556 time: 0.0071s\n",
      "Epoch: 0160 loss_train: 0.6747 acc_train: 0.6250 loss_val: 0.6759 acc_val: 0.6000 time: 0.0065s\n",
      "Epoch: 0161 loss_train: 0.6807 acc_train: 0.6250 loss_val: 0.6878 acc_val: 0.5222 time: 0.0067s\n",
      "Epoch: 0162 loss_train: 0.6951 acc_train: 0.5750 loss_val: 0.6798 acc_val: 0.5444 time: 0.0064s\n",
      "Epoch: 0163 loss_train: 0.6759 acc_train: 0.6500 loss_val: 0.6838 acc_val: 0.5556 time: 0.0064s\n",
      "Epoch: 0164 loss_train: 0.6696 acc_train: 0.6250 loss_val: 0.6837 acc_val: 0.5778 time: 0.0066s\n",
      "Epoch: 0165 loss_train: 0.6638 acc_train: 0.6500 loss_val: 0.6703 acc_val: 0.5778 time: 0.0074s\n",
      "Epoch: 0166 loss_train: 0.6748 acc_train: 0.5750 loss_val: 0.6740 acc_val: 0.5333 time: 0.0091s\n",
      "Epoch: 0167 loss_train: 0.6382 acc_train: 0.7000 loss_val: 0.6733 acc_val: 0.5222 time: 0.0077s\n",
      "Epoch: 0168 loss_train: 0.6870 acc_train: 0.5500 loss_val: 0.6932 acc_val: 0.4778 time: 0.0088s\n",
      "Epoch: 0169 loss_train: 0.7064 acc_train: 0.4500 loss_val: 0.6694 acc_val: 0.5222 time: 0.0067s\n",
      "Epoch: 0170 loss_train: 0.6549 acc_train: 0.6000 loss_val: 0.6852 acc_val: 0.4556 time: 0.0103s\n",
      "Epoch: 0171 loss_train: 0.6709 acc_train: 0.5750 loss_val: 0.6646 acc_val: 0.5667 time: 0.0071s\n",
      "Epoch: 0172 loss_train: 0.6329 acc_train: 0.6750 loss_val: 0.6767 acc_val: 0.5222 time: 0.0074s\n",
      "Epoch: 0173 loss_train: 0.6068 acc_train: 0.8250 loss_val: 0.7094 acc_val: 0.4111 time: 0.0071s\n",
      "Epoch: 0174 loss_train: 0.6520 acc_train: 0.5750 loss_val: 0.6717 acc_val: 0.5667 time: 0.0094s\n",
      "Epoch: 0175 loss_train: 0.6459 acc_train: 0.6250 loss_val: 0.6783 acc_val: 0.5000 time: 0.0073s\n",
      "Epoch: 0176 loss_train: 0.6618 acc_train: 0.5500 loss_val: 0.6890 acc_val: 0.4889 time: 0.0086s\n",
      "Epoch: 0177 loss_train: 0.6169 acc_train: 0.6750 loss_val: 0.6780 acc_val: 0.5111 time: 0.0072s\n",
      "Epoch: 0178 loss_train: 0.6724 acc_train: 0.4750 loss_val: 0.6899 acc_val: 0.4667 time: 0.0096s\n",
      "Epoch: 0179 loss_train: 0.6219 acc_train: 0.6500 loss_val: 0.6756 acc_val: 0.5444 time: 0.0073s\n",
      "Epoch: 0180 loss_train: 0.7034 acc_train: 0.4500 loss_val: 0.6793 acc_val: 0.5333 time: 0.0088s\n",
      "Epoch: 0181 loss_train: 0.6619 acc_train: 0.6000 loss_val: 0.6933 acc_val: 0.4444 time: 0.0069s\n",
      "Epoch: 0182 loss_train: 0.6695 acc_train: 0.6000 loss_val: 0.6717 acc_val: 0.5667 time: 0.0071s\n",
      "Epoch: 0183 loss_train: 0.6364 acc_train: 0.7000 loss_val: 0.6832 acc_val: 0.5778 time: 0.0065s\n",
      "Epoch: 0184 loss_train: 0.6703 acc_train: 0.6250 loss_val: 0.7011 acc_val: 0.5111 time: 0.0068s\n",
      "Epoch: 0185 loss_train: 0.6598 acc_train: 0.6500 loss_val: 0.6919 acc_val: 0.5333 time: 0.0068s\n",
      "Epoch: 0186 loss_train: 0.6753 acc_train: 0.6250 loss_val: 0.7000 acc_val: 0.5000 time: 0.0061s\n",
      "Epoch: 0187 loss_train: 0.6227 acc_train: 0.7250 loss_val: 0.7217 acc_val: 0.4111 time: 0.0068s\n",
      "Epoch: 0188 loss_train: 0.6730 acc_train: 0.6000 loss_val: 0.6671 acc_val: 0.5889 time: 0.0067s\n",
      "Epoch: 0189 loss_train: 0.6526 acc_train: 0.6000 loss_val: 0.6831 acc_val: 0.5556 time: 0.0068s\n",
      "Epoch: 0190 loss_train: 0.7225 acc_train: 0.4500 loss_val: 0.7135 acc_val: 0.4333 time: 0.0067s\n",
      "Epoch: 0191 loss_train: 0.6714 acc_train: 0.6000 loss_val: 0.6862 acc_val: 0.5222 time: 0.0061s\n",
      "Epoch: 0192 loss_train: 0.6888 acc_train: 0.5750 loss_val: 0.6549 acc_val: 0.6111 time: 0.0061s\n",
      "Epoch: 0193 loss_train: 0.6708 acc_train: 0.6250 loss_val: 0.7011 acc_val: 0.5000 time: 0.0061s\n",
      "Epoch: 0194 loss_train: 0.6652 acc_train: 0.6250 loss_val: 0.6900 acc_val: 0.5111 time: 0.0061s\n",
      "Epoch: 0195 loss_train: 0.6630 acc_train: 0.6500 loss_val: 0.7019 acc_val: 0.4667 time: 0.0068s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0196 loss_train: 0.6254 acc_train: 0.7250 loss_val: 0.7059 acc_val: 0.4889 time: 0.0076s\n",
      "Epoch: 0197 loss_train: 0.6073 acc_train: 0.7750 loss_val: 0.6770 acc_val: 0.5778 time: 0.0068s\n",
      "Epoch: 0198 loss_train: 0.6511 acc_train: 0.6250 loss_val: 0.6764 acc_val: 0.5000 time: 0.0067s\n",
      "Epoch: 0199 loss_train: 0.6848 acc_train: 0.5250 loss_val: 0.6836 acc_val: 0.5222 time: 0.0079s\n",
      "Epoch: 0200 loss_train: 0.6662 acc_train: 0.5250 loss_val: 0.6789 acc_val: 0.5333 time: 0.0069s\n",
      "Epoch: 0201 loss_train: 0.6742 acc_train: 0.5500 loss_val: 0.6672 acc_val: 0.5556 time: 0.0069s\n",
      "Epoch: 0202 loss_train: 0.6889 acc_train: 0.4500 loss_val: 0.7032 acc_val: 0.4667 time: 0.0067s\n",
      "Epoch: 0203 loss_train: 0.6538 acc_train: 0.6500 loss_val: 0.6590 acc_val: 0.5778 time: 0.0071s\n",
      "Epoch: 0204 loss_train: 0.6439 acc_train: 0.6500 loss_val: 0.6855 acc_val: 0.5000 time: 0.0070s\n",
      "Epoch: 0205 loss_train: 0.6435 acc_train: 0.6750 loss_val: 0.7037 acc_val: 0.4889 time: 0.0072s\n",
      "Epoch: 0206 loss_train: 0.6222 acc_train: 0.7250 loss_val: 0.6978 acc_val: 0.5000 time: 0.0073s\n",
      "Epoch: 0207 loss_train: 0.6695 acc_train: 0.6000 loss_val: 0.6852 acc_val: 0.5333 time: 0.0069s\n",
      "Epoch: 0208 loss_train: 0.6813 acc_train: 0.6000 loss_val: 0.6611 acc_val: 0.6111 time: 0.0074s\n",
      "Epoch: 0209 loss_train: 0.6714 acc_train: 0.6000 loss_val: 0.6908 acc_val: 0.5111 time: 0.0070s\n",
      "Epoch: 0210 loss_train: 0.6414 acc_train: 0.6500 loss_val: 0.6919 acc_val: 0.4889 time: 0.0065s\n",
      "Epoch: 0211 loss_train: 0.6758 acc_train: 0.5750 loss_val: 0.6764 acc_val: 0.5444 time: 0.0072s\n",
      "Epoch: 0212 loss_train: 0.6688 acc_train: 0.6000 loss_val: 0.6768 acc_val: 0.5444 time: 0.0072s\n",
      "Epoch: 0213 loss_train: 0.6570 acc_train: 0.6500 loss_val: 0.6489 acc_val: 0.6444 time: 0.0072s\n",
      "Epoch: 0214 loss_train: 0.6565 acc_train: 0.6500 loss_val: 0.7078 acc_val: 0.4556 time: 0.0073s\n",
      "Epoch: 0215 loss_train: 0.6644 acc_train: 0.6500 loss_val: 0.6772 acc_val: 0.5667 time: 0.0073s\n",
      "Epoch: 0216 loss_train: 0.6372 acc_train: 0.6750 loss_val: 0.6708 acc_val: 0.5556 time: 0.0072s\n",
      "Epoch: 0217 loss_train: 0.5740 acc_train: 0.8250 loss_val: 0.6689 acc_val: 0.6000 time: 0.0074s\n",
      "Epoch: 0218 loss_train: 0.6439 acc_train: 0.6250 loss_val: 0.6835 acc_val: 0.4778 time: 0.0071s\n",
      "Epoch: 0219 loss_train: 0.6205 acc_train: 0.6500 loss_val: 0.6805 acc_val: 0.4667 time: 0.0069s\n",
      "Epoch: 0220 loss_train: 0.6887 acc_train: 0.4500 loss_val: 0.6833 acc_val: 0.4778 time: 0.0062s\n",
      "Epoch: 0221 loss_train: 0.6299 acc_train: 0.5750 loss_val: 0.6678 acc_val: 0.5222 time: 0.0063s\n",
      "Epoch: 0222 loss_train: 0.7308 acc_train: 0.4500 loss_val: 0.7091 acc_val: 0.4444 time: 0.0067s\n",
      "Epoch: 0223 loss_train: 0.7115 acc_train: 0.5000 loss_val: 0.7000 acc_val: 0.4667 time: 0.0064s\n",
      "Epoch: 0224 loss_train: 0.6672 acc_train: 0.6000 loss_val: 0.6702 acc_val: 0.5778 time: 0.0088s\n",
      "Epoch: 0225 loss_train: 0.6517 acc_train: 0.6750 loss_val: 0.6815 acc_val: 0.6000 time: 0.0077s\n",
      "Epoch: 0226 loss_train: 0.6555 acc_train: 0.6500 loss_val: 0.6981 acc_val: 0.5556 time: 0.0146s\n",
      "Epoch: 0227 loss_train: 0.7040 acc_train: 0.5250 loss_val: 0.7110 acc_val: 0.4556 time: 0.0074s\n",
      "Epoch: 0228 loss_train: 0.6645 acc_train: 0.6000 loss_val: 0.7047 acc_val: 0.4667 time: 0.0089s\n",
      "Epoch: 0229 loss_train: 0.6482 acc_train: 0.6250 loss_val: 0.6853 acc_val: 0.5000 time: 0.0063s\n",
      "Epoch: 0230 loss_train: 0.6863 acc_train: 0.5500 loss_val: 0.7000 acc_val: 0.5222 time: 0.0083s\n",
      "Epoch: 0231 loss_train: 0.6543 acc_train: 0.5750 loss_val: 0.6938 acc_val: 0.5667 time: 0.0064s\n",
      "Epoch: 0232 loss_train: 0.6542 acc_train: 0.6500 loss_val: 0.6904 acc_val: 0.5000 time: 0.0082s\n",
      "Epoch: 0233 loss_train: 0.6775 acc_train: 0.6000 loss_val: 0.6809 acc_val: 0.5222 time: 0.0070s\n",
      "Epoch: 0234 loss_train: 0.6139 acc_train: 0.7000 loss_val: 0.7025 acc_val: 0.4222 time: 0.0100s\n",
      "Epoch: 0235 loss_train: 0.6396 acc_train: 0.6250 loss_val: 0.6336 acc_val: 0.6111 time: 0.0073s\n",
      "Epoch: 0236 loss_train: 0.6220 acc_train: 0.6500 loss_val: 0.6779 acc_val: 0.5000 time: 0.0102s\n",
      "Epoch: 0237 loss_train: 0.6198 acc_train: 0.6500 loss_val: 0.7081 acc_val: 0.4556 time: 0.0072s\n",
      "Epoch: 0238 loss_train: 0.7037 acc_train: 0.5000 loss_val: 0.6768 acc_val: 0.4889 time: 0.0078s\n",
      "Epoch: 0239 loss_train: 0.7071 acc_train: 0.5250 loss_val: 0.6809 acc_val: 0.4889 time: 0.0069s\n",
      "Epoch: 0240 loss_train: 0.7440 acc_train: 0.3750 loss_val: 0.6822 acc_val: 0.5333 time: 0.0097s\n",
      "Epoch: 0241 loss_train: 0.6523 acc_train: 0.5750 loss_val: 0.7207 acc_val: 0.4222 time: 0.0069s\n",
      "Epoch: 0242 loss_train: 0.6370 acc_train: 0.7000 loss_val: 0.6699 acc_val: 0.5667 time: 0.0109s\n",
      "Epoch: 0243 loss_train: 0.6527 acc_train: 0.6250 loss_val: 0.6917 acc_val: 0.5667 time: 0.0068s\n",
      "Epoch: 0244 loss_train: 0.6527 acc_train: 0.6250 loss_val: 0.6990 acc_val: 0.5222 time: 0.0116s\n",
      "Epoch: 0245 loss_train: 0.6645 acc_train: 0.6000 loss_val: 0.6976 acc_val: 0.5111 time: 0.0069s\n",
      "Epoch: 0246 loss_train: 0.6742 acc_train: 0.6000 loss_val: 0.7048 acc_val: 0.4889 time: 0.0066s\n",
      "Epoch: 0247 loss_train: 0.6591 acc_train: 0.6000 loss_val: 0.6785 acc_val: 0.5556 time: 0.0068s\n",
      "Epoch: 0248 loss_train: 0.6427 acc_train: 0.6500 loss_val: 0.7013 acc_val: 0.5000 time: 0.0070s\n",
      "Epoch: 0249 loss_train: 0.6576 acc_train: 0.6250 loss_val: 0.7058 acc_val: 0.5000 time: 0.0073s\n",
      "Epoch: 0250 loss_train: 0.6628 acc_train: 0.6000 loss_val: 0.6656 acc_val: 0.5667 time: 0.0069s\n",
      "Epoch: 0251 loss_train: 0.6963 acc_train: 0.5250 loss_val: 0.6935 acc_val: 0.5111 time: 0.0088s\n",
      "Epoch: 0252 loss_train: 0.6735 acc_train: 0.5750 loss_val: 0.6520 acc_val: 0.6000 time: 0.0083s\n",
      "Epoch: 0253 loss_train: 0.6716 acc_train: 0.5750 loss_val: 0.6792 acc_val: 0.5111 time: 0.0070s\n",
      "Epoch: 0254 loss_train: 0.6525 acc_train: 0.6250 loss_val: 0.6732 acc_val: 0.5222 time: 0.0085s\n",
      "Epoch: 0255 loss_train: 0.6985 acc_train: 0.5250 loss_val: 0.6933 acc_val: 0.4444 time: 0.0063s\n",
      "Epoch: 0256 loss_train: 0.6526 acc_train: 0.6000 loss_val: 0.6743 acc_val: 0.5333 time: 0.0068s\n",
      "Epoch: 0257 loss_train: 0.6737 acc_train: 0.5500 loss_val: 0.6909 acc_val: 0.4556 time: 0.0069s\n",
      "Epoch: 0258 loss_train: 0.6425 acc_train: 0.6250 loss_val: 0.6769 acc_val: 0.5556 time: 0.0067s\n",
      "Epoch: 0259 loss_train: 0.6371 acc_train: 0.6750 loss_val: 0.6845 acc_val: 0.4889 time: 0.0068s\n",
      "Epoch: 0260 loss_train: 0.6377 acc_train: 0.6750 loss_val: 0.6733 acc_val: 0.5333 time: 0.0067s\n",
      "Epoch: 0261 loss_train: 0.6545 acc_train: 0.6250 loss_val: 0.6887 acc_val: 0.5222 time: 0.0068s\n",
      "Epoch: 0262 loss_train: 0.6276 acc_train: 0.7000 loss_val: 0.6805 acc_val: 0.4889 time: 0.0068s\n",
      "Epoch: 0263 loss_train: 0.6197 acc_train: 0.6500 loss_val: 0.6678 acc_val: 0.5222 time: 0.0067s\n",
      "Epoch: 0264 loss_train: 0.6928 acc_train: 0.5250 loss_val: 0.6567 acc_val: 0.5444 time: 0.0068s\n",
      "Epoch: 0265 loss_train: 0.6595 acc_train: 0.6000 loss_val: 0.6432 acc_val: 0.6111 time: 0.0067s\n",
      "Epoch: 0266 loss_train: 0.6037 acc_train: 0.6750 loss_val: 0.6722 acc_val: 0.5556 time: 0.0067s\n",
      "Epoch: 0267 loss_train: 0.6392 acc_train: 0.6500 loss_val: 0.6617 acc_val: 0.5222 time: 0.0067s\n",
      "Epoch: 0268 loss_train: 0.6729 acc_train: 0.5750 loss_val: 0.6819 acc_val: 0.5333 time: 0.0067s\n",
      "Epoch: 0269 loss_train: 0.6515 acc_train: 0.6500 loss_val: 0.6981 acc_val: 0.4667 time: 0.0067s\n",
      "Epoch: 0270 loss_train: 0.6676 acc_train: 0.6000 loss_val: 0.6695 acc_val: 0.5778 time: 0.0067s\n",
      "Epoch: 0271 loss_train: 0.6288 acc_train: 0.7000 loss_val: 0.6769 acc_val: 0.5111 time: 0.0068s\n",
      "Epoch: 0272 loss_train: 0.6422 acc_train: 0.6500 loss_val: 0.6759 acc_val: 0.5556 time: 0.0067s\n",
      "Epoch: 0273 loss_train: 0.6667 acc_train: 0.6000 loss_val: 0.6758 acc_val: 0.5667 time: 0.0068s\n",
      "Epoch: 0274 loss_train: 0.6306 acc_train: 0.6750 loss_val: 0.6851 acc_val: 0.4778 time: 0.0067s\n",
      "Epoch: 0275 loss_train: 0.6830 acc_train: 0.5750 loss_val: 0.6987 acc_val: 0.4556 time: 0.0067s\n",
      "Epoch: 0276 loss_train: 0.6971 acc_train: 0.4750 loss_val: 0.6716 acc_val: 0.5556 time: 0.0067s\n",
      "Epoch: 0277 loss_train: 0.6483 acc_train: 0.6250 loss_val: 0.6943 acc_val: 0.4444 time: 0.0078s\n",
      "Epoch: 0278 loss_train: 0.6640 acc_train: 0.5750 loss_val: 0.6437 acc_val: 0.6000 time: 0.0070s\n",
      "Epoch: 0279 loss_train: 0.6831 acc_train: 0.6000 loss_val: 0.6744 acc_val: 0.5444 time: 0.0068s\n",
      "Epoch: 0280 loss_train: 0.6423 acc_train: 0.6500 loss_val: 0.6697 acc_val: 0.5667 time: 0.0070s\n",
      "Epoch: 0281 loss_train: 0.6608 acc_train: 0.6000 loss_val: 0.6681 acc_val: 0.5889 time: 0.0089s\n",
      "Epoch: 0282 loss_train: 0.6488 acc_train: 0.6500 loss_val: 0.6817 acc_val: 0.5222 time: 0.0069s\n",
      "Epoch: 0283 loss_train: 0.6412 acc_train: 0.6500 loss_val: 0.6636 acc_val: 0.5889 time: 0.0074s\n",
      "Epoch: 0284 loss_train: 0.6144 acc_train: 0.7250 loss_val: 0.6911 acc_val: 0.4889 time: 0.0079s\n",
      "Epoch: 0285 loss_train: 0.6677 acc_train: 0.6000 loss_val: 0.6551 acc_val: 0.6111 time: 0.0073s\n",
      "Epoch: 0286 loss_train: 0.6771 acc_train: 0.5250 loss_val: 0.6609 acc_val: 0.5667 time: 0.0073s\n",
      "Epoch: 0287 loss_train: 0.6886 acc_train: 0.5000 loss_val: 0.6866 acc_val: 0.5000 time: 0.0075s\n",
      "Epoch: 0288 loss_train: 0.6960 acc_train: 0.5250 loss_val: 0.6770 acc_val: 0.5333 time: 0.0073s\n",
      "Epoch: 0289 loss_train: 0.6843 acc_train: 0.5750 loss_val: 0.6487 acc_val: 0.6000 time: 0.0074s\n",
      "Epoch: 0290 loss_train: 0.6218 acc_train: 0.7250 loss_val: 0.6696 acc_val: 0.5444 time: 0.0069s\n",
      "Epoch: 0291 loss_train: 0.6153 acc_train: 0.7250 loss_val: 0.6841 acc_val: 0.5556 time: 0.0068s\n",
      "Epoch: 0292 loss_train: 0.6354 acc_train: 0.7000 loss_val: 0.7108 acc_val: 0.4556 time: 0.0075s\n",
      "Epoch: 0293 loss_train: 0.6726 acc_train: 0.6000 loss_val: 0.6353 acc_val: 0.6111 time: 0.0070s\n",
      "Epoch: 0294 loss_train: 0.6343 acc_train: 0.6500 loss_val: 0.6662 acc_val: 0.5444 time: 0.0102s\n",
      "Epoch: 0295 loss_train: 0.6191 acc_train: 0.6500 loss_val: 0.7037 acc_val: 0.4444 time: 0.0066s\n",
      "Epoch: 0296 loss_train: 0.6691 acc_train: 0.5250 loss_val: 0.6871 acc_val: 0.4778 time: 0.0069s\n",
      "Epoch: 0297 loss_train: 0.6502 acc_train: 0.6250 loss_val: 0.6501 acc_val: 0.5667 time: 0.0070s\n",
      "Epoch: 0298 loss_train: 0.5899 acc_train: 0.7500 loss_val: 0.6775 acc_val: 0.5333 time: 0.0091s\n",
      "Epoch: 0299 loss_train: 0.6246 acc_train: 0.6750 loss_val: 0.6789 acc_val: 0.5333 time: 0.0068s\n",
      "Epoch: 0300 loss_train: 0.6378 acc_train: 0.6500 loss_val: 0.6910 acc_val: 0.5000 time: 0.0081s\n",
      "Epoch: 0301 loss_train: 0.7237 acc_train: 0.4750 loss_val: 0.6943 acc_val: 0.5111 time: 0.0069s\n",
      "Epoch: 0302 loss_train: 0.6094 acc_train: 0.7000 loss_val: 0.6874 acc_val: 0.5222 time: 0.0103s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0303 loss_train: 0.6597 acc_train: 0.6000 loss_val: 0.6792 acc_val: 0.5444 time: 0.0073s\n",
      "Epoch: 0304 loss_train: 0.6111 acc_train: 0.7250 loss_val: 0.7106 acc_val: 0.4778 time: 0.0118s\n",
      "Epoch: 0305 loss_train: 0.6672 acc_train: 0.5750 loss_val: 0.6573 acc_val: 0.6333 time: 0.0066s\n",
      "Epoch: 0306 loss_train: 0.6962 acc_train: 0.5250 loss_val: 0.7026 acc_val: 0.4667 time: 0.0087s\n",
      "Epoch: 0307 loss_train: 0.6403 acc_train: 0.6500 loss_val: 0.6961 acc_val: 0.4778 time: 0.0070s\n",
      "Epoch: 0308 loss_train: 0.6404 acc_train: 0.6250 loss_val: 0.6703 acc_val: 0.5333 time: 0.0095s\n",
      "Epoch: 0309 loss_train: 0.6142 acc_train: 0.6750 loss_val: 0.6525 acc_val: 0.6000 time: 0.0075s\n",
      "Epoch: 0310 loss_train: 0.6289 acc_train: 0.6750 loss_val: 0.6516 acc_val: 0.6222 time: 0.0114s\n",
      "Epoch: 0311 loss_train: 0.6655 acc_train: 0.6000 loss_val: 0.7175 acc_val: 0.4556 time: 0.0079s\n",
      "Epoch: 0312 loss_train: 0.6819 acc_train: 0.5750 loss_val: 0.6894 acc_val: 0.4778 time: 0.0063s\n",
      "Epoch: 0313 loss_train: 0.6117 acc_train: 0.7000 loss_val: 0.6669 acc_val: 0.5667 time: 0.0072s\n",
      "Epoch: 0314 loss_train: 0.5917 acc_train: 0.7250 loss_val: 0.6488 acc_val: 0.6000 time: 0.0066s\n",
      "Epoch: 0315 loss_train: 0.6316 acc_train: 0.6500 loss_val: 0.6934 acc_val: 0.5000 time: 0.0068s\n",
      "Epoch: 0316 loss_train: 0.6855 acc_train: 0.5500 loss_val: 0.6927 acc_val: 0.4778 time: 0.0085s\n",
      "Epoch: 0317 loss_train: 0.6547 acc_train: 0.5750 loss_val: 0.6500 acc_val: 0.5889 time: 0.0068s\n",
      "Epoch: 0318 loss_train: 0.6145 acc_train: 0.7000 loss_val: 0.6985 acc_val: 0.4889 time: 0.0085s\n",
      "Epoch: 0319 loss_train: 0.6373 acc_train: 0.6500 loss_val: 0.6816 acc_val: 0.5111 time: 0.0071s\n",
      "Epoch: 0320 loss_train: 0.6506 acc_train: 0.6000 loss_val: 0.6824 acc_val: 0.5222 time: 0.0094s\n",
      "Epoch: 0321 loss_train: 0.5947 acc_train: 0.7500 loss_val: 0.6882 acc_val: 0.5333 time: 0.0071s\n",
      "Epoch: 0322 loss_train: 0.6090 acc_train: 0.6750 loss_val: 0.7207 acc_val: 0.4222 time: 0.0099s\n",
      "Epoch: 0323 loss_train: 0.6184 acc_train: 0.6750 loss_val: 0.6511 acc_val: 0.5889 time: 0.0069s\n",
      "Epoch: 0324 loss_train: 0.6643 acc_train: 0.6000 loss_val: 0.6760 acc_val: 0.5222 time: 0.0084s\n",
      "Epoch: 0325 loss_train: 0.6021 acc_train: 0.7000 loss_val: 0.7170 acc_val: 0.4778 time: 0.0068s\n",
      "Epoch: 0326 loss_train: 0.6596 acc_train: 0.6000 loss_val: 0.7054 acc_val: 0.4556 time: 0.0100s\n",
      "Epoch: 0327 loss_train: 0.6638 acc_train: 0.6000 loss_val: 0.6680 acc_val: 0.5444 time: 0.0068s\n",
      "Epoch: 0328 loss_train: 0.6460 acc_train: 0.6250 loss_val: 0.6523 acc_val: 0.6111 time: 0.0074s\n",
      "Epoch: 0329 loss_train: 0.6503 acc_train: 0.5750 loss_val: 0.6656 acc_val: 0.5444 time: 0.0084s\n",
      "Epoch: 0330 loss_train: 0.6423 acc_train: 0.6250 loss_val: 0.7029 acc_val: 0.4889 time: 0.0088s\n",
      "Epoch: 0331 loss_train: 0.6146 acc_train: 0.7000 loss_val: 0.6739 acc_val: 0.5333 time: 0.0072s\n",
      "Epoch: 0332 loss_train: 0.6505 acc_train: 0.6500 loss_val: 0.6892 acc_val: 0.5222 time: 0.0089s\n",
      "Epoch: 0333 loss_train: 0.6913 acc_train: 0.5500 loss_val: 0.6993 acc_val: 0.5000 time: 0.0072s\n",
      "Epoch: 0334 loss_train: 0.6739 acc_train: 0.5750 loss_val: 0.6851 acc_val: 0.5222 time: 0.0088s\n",
      "Epoch: 0335 loss_train: 0.6566 acc_train: 0.6250 loss_val: 0.6558 acc_val: 0.6000 time: 0.0069s\n",
      "Epoch: 0336 loss_train: 0.6423 acc_train: 0.6500 loss_val: 0.6777 acc_val: 0.5444 time: 0.0088s\n",
      "Epoch: 0337 loss_train: 0.6773 acc_train: 0.5500 loss_val: 0.6771 acc_val: 0.5444 time: 0.0069s\n",
      "Epoch: 0338 loss_train: 0.6767 acc_train: 0.5250 loss_val: 0.6665 acc_val: 0.5667 time: 0.0100s\n",
      "Epoch: 0339 loss_train: 0.6344 acc_train: 0.6500 loss_val: 0.6906 acc_val: 0.4889 time: 0.0069s\n",
      "Epoch: 0340 loss_train: 0.6566 acc_train: 0.6250 loss_val: 0.6986 acc_val: 0.4778 time: 0.0068s\n",
      "Epoch: 0341 loss_train: 0.6685 acc_train: 0.5500 loss_val: 0.6699 acc_val: 0.5444 time: 0.0070s\n",
      "Epoch: 0342 loss_train: 0.6659 acc_train: 0.6000 loss_val: 0.6881 acc_val: 0.5111 time: 0.0068s\n",
      "Epoch: 0343 loss_train: 0.6190 acc_train: 0.6750 loss_val: 0.6971 acc_val: 0.4556 time: 0.0068s\n",
      "Epoch: 0344 loss_train: 0.6331 acc_train: 0.6750 loss_val: 0.6804 acc_val: 0.5222 time: 0.0067s\n",
      "Epoch: 0345 loss_train: 0.7070 acc_train: 0.5000 loss_val: 0.6986 acc_val: 0.4667 time: 0.0067s\n",
      "Epoch: 0346 loss_train: 0.6519 acc_train: 0.6250 loss_val: 0.7003 acc_val: 0.5222 time: 0.0067s\n",
      "Epoch: 0347 loss_train: 0.6245 acc_train: 0.6250 loss_val: 0.6730 acc_val: 0.5444 time: 0.0073s\n",
      "Epoch: 0348 loss_train: 0.6100 acc_train: 0.7000 loss_val: 0.6617 acc_val: 0.5444 time: 0.0071s\n",
      "Epoch: 0349 loss_train: 0.6220 acc_train: 0.6500 loss_val: 0.6768 acc_val: 0.5222 time: 0.0069s\n",
      "Epoch: 0350 loss_train: 0.6016 acc_train: 0.6750 loss_val: 0.6632 acc_val: 0.5667 time: 0.0068s\n",
      "Epoch: 0351 loss_train: 0.6515 acc_train: 0.6000 loss_val: 0.6921 acc_val: 0.4889 time: 0.0068s\n",
      "Epoch: 0352 loss_train: 0.6700 acc_train: 0.5750 loss_val: 0.6767 acc_val: 0.5000 time: 0.0063s\n",
      "Epoch: 0353 loss_train: 0.6392 acc_train: 0.6000 loss_val: 0.6738 acc_val: 0.5556 time: 0.0072s\n",
      "Epoch: 0354 loss_train: 0.6773 acc_train: 0.5750 loss_val: 0.6980 acc_val: 0.4444 time: 0.0068s\n",
      "Epoch: 0355 loss_train: 0.6255 acc_train: 0.6250 loss_val: 0.6844 acc_val: 0.4889 time: 0.0071s\n",
      "Epoch: 0356 loss_train: 0.6676 acc_train: 0.5750 loss_val: 0.6973 acc_val: 0.5000 time: 0.0087s\n",
      "Epoch: 0357 loss_train: 0.6408 acc_train: 0.6500 loss_val: 0.7050 acc_val: 0.5111 time: 0.0075s\n",
      "Epoch: 0358 loss_train: 0.6072 acc_train: 0.7250 loss_val: 0.6969 acc_val: 0.5222 time: 0.0077s\n",
      "Epoch: 0359 loss_train: 0.6838 acc_train: 0.5500 loss_val: 0.6896 acc_val: 0.4889 time: 0.0072s\n",
      "Epoch: 0360 loss_train: 0.5709 acc_train: 0.7750 loss_val: 0.6777 acc_val: 0.5333 time: 0.0076s\n",
      "Epoch: 0361 loss_train: 0.5987 acc_train: 0.7250 loss_val: 0.7236 acc_val: 0.4444 time: 0.0074s\n",
      "Epoch: 0362 loss_train: 0.6225 acc_train: 0.6000 loss_val: 0.6414 acc_val: 0.6000 time: 0.0069s\n",
      "Epoch: 0363 loss_train: 0.6517 acc_train: 0.5750 loss_val: 0.7094 acc_val: 0.4444 time: 0.0072s\n",
      "Epoch: 0364 loss_train: 0.6487 acc_train: 0.5750 loss_val: 0.7014 acc_val: 0.4889 time: 0.0068s\n",
      "Epoch: 0365 loss_train: 0.6553 acc_train: 0.6250 loss_val: 0.6798 acc_val: 0.5333 time: 0.0068s\n",
      "Epoch: 0366 loss_train: 0.6542 acc_train: 0.6250 loss_val: 0.6838 acc_val: 0.5111 time: 0.0070s\n",
      "Epoch: 0367 loss_train: 0.6086 acc_train: 0.6750 loss_val: 0.6868 acc_val: 0.5111 time: 0.0068s\n",
      "Epoch: 0368 loss_train: 0.6313 acc_train: 0.6750 loss_val: 0.6815 acc_val: 0.5667 time: 0.0071s\n",
      "Epoch: 0369 loss_train: 0.6364 acc_train: 0.6750 loss_val: 0.6931 acc_val: 0.5333 time: 0.0073s\n",
      "Epoch: 0370 loss_train: 0.6507 acc_train: 0.6250 loss_val: 0.6556 acc_val: 0.5889 time: 0.0071s\n",
      "Epoch: 0371 loss_train: 0.6720 acc_train: 0.6250 loss_val: 0.6784 acc_val: 0.5222 time: 0.0069s\n",
      "Epoch: 0372 loss_train: 0.5473 acc_train: 0.8000 loss_val: 0.6978 acc_val: 0.4889 time: 0.0072s\n",
      "Epoch: 0373 loss_train: 0.6125 acc_train: 0.6750 loss_val: 0.6694 acc_val: 0.5778 time: 0.0070s\n",
      "Epoch: 0374 loss_train: 0.6683 acc_train: 0.5750 loss_val: 0.6577 acc_val: 0.5444 time: 0.0068s\n",
      "Epoch: 0375 loss_train: 0.6352 acc_train: 0.6000 loss_val: 0.7328 acc_val: 0.4556 time: 0.0068s\n",
      "Epoch: 0376 loss_train: 0.6740 acc_train: 0.5250 loss_val: 0.6813 acc_val: 0.5556 time: 0.0068s\n",
      "Epoch: 0377 loss_train: 0.6665 acc_train: 0.5500 loss_val: 0.6804 acc_val: 0.5111 time: 0.0070s\n",
      "Epoch: 0378 loss_train: 0.5975 acc_train: 0.7000 loss_val: 0.7157 acc_val: 0.5111 time: 0.0072s\n",
      "Epoch: 0379 loss_train: 0.6037 acc_train: 0.6750 loss_val: 0.7063 acc_val: 0.5222 time: 0.0071s\n",
      "Epoch: 0380 loss_train: 0.5887 acc_train: 0.7000 loss_val: 0.6683 acc_val: 0.5667 time: 0.0069s\n",
      "Epoch: 0381 loss_train: 0.6670 acc_train: 0.6000 loss_val: 0.6982 acc_val: 0.5222 time: 0.0069s\n",
      "Epoch: 0382 loss_train: 0.6381 acc_train: 0.6750 loss_val: 0.6711 acc_val: 0.5667 time: 0.0066s\n",
      "Epoch: 0383 loss_train: 0.6206 acc_train: 0.6750 loss_val: 0.6703 acc_val: 0.5667 time: 0.0071s\n",
      "Epoch: 0384 loss_train: 0.6111 acc_train: 0.7000 loss_val: 0.6604 acc_val: 0.5667 time: 0.0110s\n",
      "Epoch: 0385 loss_train: 0.6710 acc_train: 0.6250 loss_val: 0.6673 acc_val: 0.5778 time: 0.0074s\n",
      "Epoch: 0386 loss_train: 0.5866 acc_train: 0.7500 loss_val: 0.7154 acc_val: 0.4667 time: 0.0072s\n",
      "Epoch: 0387 loss_train: 0.6534 acc_train: 0.6000 loss_val: 0.6809 acc_val: 0.5333 time: 0.0073s\n",
      "Epoch: 0388 loss_train: 0.6948 acc_train: 0.5500 loss_val: 0.6549 acc_val: 0.6222 time: 0.0084s\n",
      "Epoch: 0389 loss_train: 0.6804 acc_train: 0.6000 loss_val: 0.6883 acc_val: 0.5333 time: 0.0073s\n",
      "Epoch: 0390 loss_train: 0.5920 acc_train: 0.7000 loss_val: 0.6596 acc_val: 0.5667 time: 0.0069s\n",
      "Epoch: 0391 loss_train: 0.6399 acc_train: 0.6500 loss_val: 0.6794 acc_val: 0.5444 time: 0.0073s\n",
      "Epoch: 0392 loss_train: 0.6201 acc_train: 0.7000 loss_val: 0.6918 acc_val: 0.5222 time: 0.0065s\n",
      "Epoch: 0393 loss_train: 0.6161 acc_train: 0.7000 loss_val: 0.6985 acc_val: 0.5000 time: 0.0068s\n",
      "Epoch: 0394 loss_train: 0.5993 acc_train: 0.6750 loss_val: 0.7012 acc_val: 0.4889 time: 0.0066s\n",
      "Epoch: 0395 loss_train: 0.6120 acc_train: 0.6500 loss_val: 0.6762 acc_val: 0.5556 time: 0.0071s\n",
      "Epoch: 0396 loss_train: 0.5801 acc_train: 0.7250 loss_val: 0.6793 acc_val: 0.5667 time: 0.0068s\n",
      "Epoch: 0397 loss_train: 0.5914 acc_train: 0.6750 loss_val: 0.7124 acc_val: 0.4778 time: 0.0072s\n",
      "Epoch: 0398 loss_train: 0.5758 acc_train: 0.7250 loss_val: 0.7072 acc_val: 0.4444 time: 0.0072s\n",
      "Epoch: 0399 loss_train: 0.6468 acc_train: 0.5750 loss_val: 0.7178 acc_val: 0.4778 time: 0.0070s\n",
      "Epoch: 0400 loss_train: 0.5853 acc_train: 0.7000 loss_val: 0.7226 acc_val: 0.4667 time: 0.0068s\n",
      "Epoch: 0401 loss_train: 0.7099 acc_train: 0.5000 loss_val: 0.7021 acc_val: 0.5111 time: 0.0070s\n",
      "\n",
      "Done with Training\n",
      "\n",
      "\n",
      "Total time elapsed: 3.0122s\n",
      "\n",
      "Test set results: loss= 0.6573 accuracy= 0.6556\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 400\n",
    "\n",
    "#Train the model\n",
    "t_total = time.time()\n",
    "for epoch in range(0, num_epochs+1):\n",
    "    train(epoch)\n",
    "        \n",
    "print(\"\\nDone with Training\\n\")\n",
    "\n",
    "print(\"\\nTotal time elapsed: {:.4f}s\\n\".format(time.time() - t_total))\n",
    "\n",
    "#Test the model\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print('----Edge Index With Self Loop-------')\n",
    "#print(model.edge_index_with_self_loop)\n",
    "\n",
    "#print('----Edge Index With Self Loop shape---')\n",
    "#print(model.edge_index_with_self_loop.shape)\n",
    "\n",
    "\n",
    "#----Printing the Shapes of the Propagation Matrices for corresponding Propagation Layer\n",
    "\n",
    "#print('Propagation Matrix Weights Shape')\n",
    "\n",
    "#print(model.prop1_mat_weight.shape)\n",
    "#print(model.prop2_mat_weight.shape)\n",
    "#print(model.prop3_mat_weight.shape)\n",
    "\n",
    "#-----------Printing the Propagation Weights for 3 Propagation Layers ----------------------\n",
    "\n",
    "#print('Propagation Matrix Weight')\n",
    "\n",
    "#print(model.prop1_mat_weight)\n",
    "#print(model.prop2_mat_weight)\n",
    "#print(model.prop3_mat_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}