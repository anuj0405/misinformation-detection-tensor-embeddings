{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.ArticlesHandler import ArticlesHandler\n",
    "from utils import solve, embedding_matrix_2_kNN, get_rate, accuracy, precision, recall, f1_score\n",
    "from utils import Config\n",
    "import time\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "from postprocessing.SelectLabelsPostprocessor import SelectLabelsPostprocessor\n",
    "from pygcn.utils import encode_onehot, accuracy, load_from_features\n",
    "from model import AGNN\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import config file and check some values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method of decomposition: GloVe\n"
     ]
    }
   ],
   "source": [
    "config = Config(file='config')\n",
    "\n",
    "assert (config.num_fake_articles + config.num_real_articles > \n",
    "        config.num_nearest_neighbours), \"Can't have more neighbours than nodes!\"\n",
    "\n",
    "print(\"Method of decomposition:\", config.method_decomposition_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the articles and decompose the tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset Random Poltical News Dataset\n",
      "Performing decomposition...\n",
      "Loading Glove Model\n",
      "Done. 400000  words loaded!\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading dataset\", config.dataset_name)\n",
    "articles = ArticlesHandler(config)\n",
    "\n",
    "print(\"Performing decomposition...\")\n",
    "C = articles.get_tensor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.set(\"num_unknown_labels\", 195)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = articles.articles.labels\n",
    "all_labels = articles.articles.labels_untouched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj, features, all_labels = load_from_features(C, all_labels, config)\n",
    "_, _, labels = load_from_features(C, labels, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 1, 1, 2,\n",
      "        1, 1, 2, 2, 2, 1])\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "# idx_train = range(150)\n",
    "# idx_val = range(150, 175)\n",
    "# idx_test = range(175, 200)\n",
    "print(labels)\n",
    "idx_train = np.where(labels)[0]\n",
    "idx_val = np.where(1 - abs(labels))[0][:90]\n",
    "idx_test = np.where(1 - abs(labels))[0][90:]\n",
    "\n",
    "print(len(idx_train))\n",
    "\n",
    "idx_train = torch.LongTensor(idx_train)\n",
    "idx_val = torch.LongTensor(idx_val)\n",
    "idx_test = torch.LongTensor(idx_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class GraphAttentionLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, requires_grad=True):\n",
    "        super(GraphAttentionLayer, self).__init__()\n",
    "        if requires_grad:\n",
    "            # unifrom initialization\n",
    "            self.beta = Parameter(torch.Tensor(1).uniform_(\n",
    "                0, 1), requires_grad=requires_grad)\n",
    "        else:\n",
    "            self.beta = Variable(torch.zeros(1), requires_grad=requires_grad)\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "\n",
    "\n",
    "        # NaN grad bug fixed at pytorch 0.3. Release note:\n",
    "        #     `when torch.norm returned 0.0, the gradient was NaN.\n",
    "        #     We now use the subgradient at 0.0, so the gradient is 0.0.`\n",
    "        norm2 = torch.norm(x, 2, 1).view(-1, 1)\n",
    "\n",
    "        # add a minor constant (1e-7) to denominator to prevent division by\n",
    "        # zero error\n",
    "        if torch.cuda.is_available():\n",
    "            cos = self.beta.cuda() * \\\n",
    "                  torch.div(torch.mm(x, x.t()), torch.mm(norm2, norm2.t()) + 1e-7)\n",
    "        else:\n",
    "            cos = self.beta * \\\n",
    "                  torch.div(torch.mm(x, x.t()), torch.mm(norm2, norm2.t()) + 1e-7)\n",
    "\n",
    "        # neighborhood masking (inspired by this repo:\n",
    "        # https://github.com/danielegrattarola/keras-gat)\n",
    "        \n",
    "        \n",
    "        mask = (torch.ones(adj.shape) - adj) * -1e9\n",
    "        masked = cos + mask\n",
    "\n",
    "        # propagation matrix\n",
    "        P = F.softmax(masked, dim=1)\n",
    "\n",
    "        # attention-guided propagation\n",
    "        output = torch.mm(P, x)\n",
    "        return output\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + ' (16 -> 16)'\n",
    "\n",
    "\n",
    "class LinearLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, in_features, out_features, initializer=nn.init.xavier_uniform_):\n",
    "        super(LinearLayer, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = Parameter(initializer(\n",
    "            torch.Tensor(in_features, out_features)))\n",
    "\n",
    "    def forward(self, input):\n",
    "        # no bias\n",
    "        return torch.mm(input, self.weight)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + ' (' \\\n",
    "            + str(self.in_features) + ' -> ' \\\n",
    "            + str(self.out_features) + ')'\n",
    "\n",
    "\n",
    "class AGNN(nn.Module):\n",
    "\n",
    "    def __init__(self, nfeat, nhid, nclass, nlayers, dropout_rate):\n",
    "        super(AGNN, self).__init__()\n",
    "\n",
    "        self.layers = nlayers\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "        self.embeddinglayer = LinearLayer(nfeat, nhid)\n",
    "        nn.init.xavier_uniform_(self.embeddinglayer.weight)\n",
    "\n",
    "        self.attentionlayers = nn.ModuleList()\n",
    "        # for Cora dataset, the first propagation layer is non-trainable\n",
    "        # and beta is fixed at 0\n",
    "        self.attentionlayers.append(GraphAttentionLayer(requires_grad=False).cuda())\n",
    "        for i in range(1, self.layers):\n",
    "            if torch.cuda.is_available():\n",
    "                self.attentionlayers.append(GraphAttentionLayer().cuda())\n",
    "            else:\n",
    "                self.attentionlayers.append(GraphAttentionLayer())\n",
    "\n",
    "\n",
    "        self.outputlayer = LinearLayer(nhid, nclass)\n",
    "        nn.init.xavier_uniform_(self.outputlayer.weight)\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        x = F.relu(self.embeddinglayer(x))\n",
    "        x = F.dropout(x, self.dropout_rate, training=self.training)\n",
    "\n",
    "        for i in range(self.layers):\n",
    "            x = self.attentionlayers[i](x, adj)\n",
    "\n",
    "        x = self.outputlayer(x)\n",
    "        x = F.dropout(x, self.dropout_rate, training=self.training)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 loss_train: 0.7236 acc_train: 0.4000 loss_val: 0.6892 acc_val: 0.5222 time: 0.0105s\n",
      "Epoch: 0002 loss_train: 0.7658 acc_train: 0.5000 loss_val: 0.6851 acc_val: 0.5222 time: 0.0076s\n",
      "Epoch: 0003 loss_train: 0.6747 acc_train: 0.6000 loss_val: 0.6866 acc_val: 0.4778 time: 0.0092s\n",
      "Epoch: 0004 loss_train: 0.7075 acc_train: 0.5000 loss_val: 0.6940 acc_val: 0.5000 time: 0.0095s\n",
      "Epoch: 0005 loss_train: 0.6940 acc_train: 0.5000 loss_val: 0.7002 acc_val: 0.4778 time: 0.0146s\n",
      "Epoch: 0006 loss_train: 0.6244 acc_train: 0.7000 loss_val: 0.7039 acc_val: 0.4778 time: 0.0124s\n",
      "Epoch: 0007 loss_train: 0.6725 acc_train: 0.7000 loss_val: 0.6983 acc_val: 0.4778 time: 0.0059s\n",
      "Epoch: 0008 loss_train: 0.7848 acc_train: 0.4000 loss_val: 0.6908 acc_val: 0.5111 time: 0.0058s\n",
      "Epoch: 0009 loss_train: 0.6623 acc_train: 0.5000 loss_val: 0.6897 acc_val: 0.5667 time: 0.0057s\n",
      "Epoch: 0010 loss_train: 0.6590 acc_train: 0.7000 loss_val: 0.6860 acc_val: 0.7000 time: 0.0055s\n",
      "Epoch: 0011 loss_train: 0.6658 acc_train: 0.6000 loss_val: 0.6815 acc_val: 0.5889 time: 0.0058s\n",
      "Epoch: 0012 loss_train: 0.6819 acc_train: 0.6000 loss_val: 0.6786 acc_val: 0.5444 time: 0.0062s\n",
      "Epoch: 0013 loss_train: 0.6742 acc_train: 0.4000 loss_val: 0.6773 acc_val: 0.5333 time: 0.0058s\n",
      "Epoch: 0014 loss_train: 0.6653 acc_train: 0.8000 loss_val: 0.6758 acc_val: 0.5000 time: 0.0056s\n",
      "Epoch: 0015 loss_train: 0.5901 acc_train: 0.8000 loss_val: 0.6746 acc_val: 0.5333 time: 0.0059s\n",
      "Epoch: 0016 loss_train: 0.6750 acc_train: 0.6000 loss_val: 0.6741 acc_val: 0.6000 time: 0.0057s\n",
      "Epoch: 0017 loss_train: 0.6738 acc_train: 0.5000 loss_val: 0.6733 acc_val: 0.6111 time: 0.0055s\n",
      "Epoch: 0018 loss_train: 0.6494 acc_train: 0.6000 loss_val: 0.6730 acc_val: 0.6000 time: 0.0055s\n",
      "Epoch: 0019 loss_train: 0.5824 acc_train: 0.8000 loss_val: 0.6716 acc_val: 0.6111 time: 0.0056s\n",
      "Epoch: 0020 loss_train: 0.6030 acc_train: 0.8000 loss_val: 0.6705 acc_val: 0.5556 time: 0.0054s\n",
      "Epoch: 0021 loss_train: 0.6106 acc_train: 0.7000 loss_val: 0.6699 acc_val: 0.6111 time: 0.0054s\n",
      "Epoch: 0022 loss_train: 0.5636 acc_train: 0.7000 loss_val: 0.6696 acc_val: 0.5889 time: 0.0059s\n",
      "Epoch: 0023 loss_train: 0.6798 acc_train: 0.5000 loss_val: 0.6705 acc_val: 0.6222 time: 0.0054s\n",
      "Epoch: 0024 loss_train: 0.5047 acc_train: 0.9000 loss_val: 0.6759 acc_val: 0.6667 time: 0.0054s\n",
      "Epoch: 0025 loss_train: 0.7489 acc_train: 0.4000 loss_val: 0.6793 acc_val: 0.6889 time: 0.0054s\n",
      "Epoch: 0026 loss_train: 0.5288 acc_train: 0.8000 loss_val: 0.6783 acc_val: 0.6889 time: 0.0054s\n",
      "Epoch: 0027 loss_train: 0.5872 acc_train: 0.7000 loss_val: 0.6728 acc_val: 0.6556 time: 0.0055s\n",
      "Epoch: 0028 loss_train: 0.5407 acc_train: 0.7000 loss_val: 0.6691 acc_val: 0.6111 time: 0.0054s\n",
      "Epoch: 0029 loss_train: 0.5909 acc_train: 0.5000 loss_val: 0.6661 acc_val: 0.6222 time: 0.0054s\n",
      "Epoch: 0030 loss_train: 0.5048 acc_train: 0.8000 loss_val: 0.6643 acc_val: 0.6111 time: 0.0055s\n",
      "Epoch: 0031 loss_train: 0.5212 acc_train: 0.8000 loss_val: 0.6633 acc_val: 0.5889 time: 0.0070s\n",
      "Epoch: 0032 loss_train: 0.6196 acc_train: 0.6000 loss_val: 0.6633 acc_val: 0.6111 time: 0.0107s\n",
      "Epoch: 0033 loss_train: 0.4541 acc_train: 1.0000 loss_val: 0.6636 acc_val: 0.6222 time: 0.0086s\n",
      "Epoch: 0034 loss_train: 0.4924 acc_train: 0.9000 loss_val: 0.6639 acc_val: 0.6111 time: 0.0095s\n",
      "Epoch: 0035 loss_train: 0.6199 acc_train: 0.6000 loss_val: 0.6652 acc_val: 0.6111 time: 0.0066s\n",
      "Epoch: 0036 loss_train: 0.5674 acc_train: 0.7000 loss_val: 0.6697 acc_val: 0.6556 time: 0.0057s\n",
      "Epoch: 0037 loss_train: 0.4315 acc_train: 0.9000 loss_val: 0.6725 acc_val: 0.6556 time: 0.0106s\n",
      "Epoch: 0038 loss_train: 0.6075 acc_train: 0.9000 loss_val: 0.6743 acc_val: 0.6556 time: 0.0062s\n",
      "Epoch: 0039 loss_train: 0.5344 acc_train: 0.8000 loss_val: 0.6745 acc_val: 0.6556 time: 0.0058s\n",
      "Epoch: 0040 loss_train: 0.5017 acc_train: 1.0000 loss_val: 0.6686 acc_val: 0.6111 time: 0.0058s\n",
      "Epoch: 0041 loss_train: 0.4257 acc_train: 0.9000 loss_val: 0.6665 acc_val: 0.6111 time: 0.0059s\n",
      "Epoch: 0042 loss_train: 0.4888 acc_train: 0.9000 loss_val: 0.6634 acc_val: 0.6222 time: 0.0058s\n",
      "Epoch: 0043 loss_train: 0.4621 acc_train: 0.8000 loss_val: 0.6624 acc_val: 0.6111 time: 0.0067s\n",
      "Epoch: 0044 loss_train: 0.4987 acc_train: 0.7000 loss_val: 0.6661 acc_val: 0.6111 time: 0.0070s\n",
      "Epoch: 0045 loss_train: 0.5144 acc_train: 0.7000 loss_val: 0.6695 acc_val: 0.6111 time: 0.0066s\n",
      "Epoch: 0046 loss_train: 0.4164 acc_train: 0.8000 loss_val: 0.6689 acc_val: 0.6111 time: 0.0071s\n",
      "Epoch: 0047 loss_train: 0.4155 acc_train: 0.9000 loss_val: 0.6668 acc_val: 0.6111 time: 0.0064s\n",
      "Epoch: 0048 loss_train: 0.5553 acc_train: 0.7000 loss_val: 0.6678 acc_val: 0.6111 time: 0.0057s\n",
      "Epoch: 0049 loss_train: 0.4308 acc_train: 0.6000 loss_val: 0.6686 acc_val: 0.6111 time: 0.0058s\n",
      "Epoch: 0050 loss_train: 0.4172 acc_train: 0.8000 loss_val: 0.6717 acc_val: 0.6222 time: 0.0059s\n",
      "Epoch: 0051 loss_train: 0.3786 acc_train: 0.9000 loss_val: 0.6776 acc_val: 0.6111 time: 0.0057s\n",
      "Epoch: 0052 loss_train: 0.4755 acc_train: 0.8000 loss_val: 0.6832 acc_val: 0.6333 time: 0.0059s\n",
      "Epoch: 0053 loss_train: 0.4082 acc_train: 0.9000 loss_val: 0.6881 acc_val: 0.6444 time: 0.0061s\n",
      "Epoch: 0054 loss_train: 0.6262 acc_train: 0.5000 loss_val: 0.6959 acc_val: 0.6556 time: 0.0067s\n",
      "Epoch: 0055 loss_train: 0.3975 acc_train: 0.8000 loss_val: 0.7009 acc_val: 0.6444 time: 0.0065s\n",
      "Epoch: 0056 loss_train: 0.4218 acc_train: 0.8000 loss_val: 0.7044 acc_val: 0.6444 time: 0.0059s\n",
      "Epoch: 0057 loss_train: 0.3673 acc_train: 0.7000 loss_val: 0.7119 acc_val: 0.6556 time: 0.0056s\n",
      "Epoch: 0058 loss_train: 0.4135 acc_train: 0.7000 loss_val: 0.7067 acc_val: 0.6556 time: 0.0065s\n",
      "Epoch: 0059 loss_train: 0.3827 acc_train: 0.9000 loss_val: 0.6982 acc_val: 0.6333 time: 0.0055s\n",
      "Epoch: 0060 loss_train: 0.3553 acc_train: 0.9000 loss_val: 0.6886 acc_val: 0.6222 time: 0.0057s\n",
      "Epoch: 0061 loss_train: 0.4703 acc_train: 0.8000 loss_val: 0.6910 acc_val: 0.6111 time: 0.0129s\n",
      "Epoch: 0062 loss_train: 0.4532 acc_train: 0.7000 loss_val: 0.6978 acc_val: 0.6111 time: 0.0082s\n",
      "Epoch: 0063 loss_train: 0.2734 acc_train: 0.9000 loss_val: 0.7006 acc_val: 0.6111 time: 0.0070s\n",
      "Epoch: 0064 loss_train: 0.4107 acc_train: 0.8000 loss_val: 0.7012 acc_val: 0.6222 time: 0.0102s\n",
      "Epoch: 0065 loss_train: 0.2442 acc_train: 1.0000 loss_val: 0.7019 acc_val: 0.6111 time: 0.0090s\n",
      "Epoch: 0066 loss_train: 0.4151 acc_train: 0.8000 loss_val: 0.7049 acc_val: 0.6222 time: 0.0058s\n",
      "Epoch: 0067 loss_train: 0.4361 acc_train: 0.8000 loss_val: 0.7116 acc_val: 0.6111 time: 0.0056s\n",
      "Epoch: 0068 loss_train: 0.2254 acc_train: 0.9000 loss_val: 0.7207 acc_val: 0.6111 time: 0.0117s\n",
      "Epoch: 0069 loss_train: 0.3287 acc_train: 0.7000 loss_val: 0.7362 acc_val: 0.6556 time: 0.0072s\n",
      "Epoch: 0070 loss_train: 0.4000 acc_train: 0.8000 loss_val: 0.7373 acc_val: 0.6444 time: 0.0071s\n",
      "Epoch: 0071 loss_train: 0.2447 acc_train: 0.8000 loss_val: 0.7365 acc_val: 0.6333 time: 0.0071s\n",
      "Epoch: 0072 loss_train: 0.2760 acc_train: 0.9000 loss_val: 0.7309 acc_val: 0.6111 time: 0.0070s\n",
      "Epoch: 0073 loss_train: 0.2706 acc_train: 0.8000 loss_val: 0.7307 acc_val: 0.6111 time: 0.0069s\n",
      "Epoch: 0074 loss_train: 0.3341 acc_train: 0.8000 loss_val: 0.7434 acc_val: 0.6111 time: 0.0069s\n",
      "Epoch: 0075 loss_train: 0.4344 acc_train: 0.7000 loss_val: 0.7553 acc_val: 0.6222 time: 0.0070s\n",
      "Epoch: 0076 loss_train: 0.3357 acc_train: 0.9000 loss_val: 0.7639 acc_val: 0.6111 time: 0.0075s\n",
      "Epoch: 0077 loss_train: 0.3503 acc_train: 0.9000 loss_val: 0.7729 acc_val: 0.6111 time: 0.0068s\n",
      "Epoch: 0078 loss_train: 0.3541 acc_train: 0.9000 loss_val: 0.7671 acc_val: 0.6111 time: 0.0058s\n",
      "Epoch: 0079 loss_train: 0.4501 acc_train: 0.7000 loss_val: 0.7607 acc_val: 0.6111 time: 0.0057s\n",
      "Epoch: 0080 loss_train: 0.2861 acc_train: 0.7000 loss_val: 0.7612 acc_val: 0.6222 time: 0.0061s\n",
      "Epoch: 0081 loss_train: 0.3619 acc_train: 0.8000 loss_val: 0.7668 acc_val: 0.6222 time: 0.0078s\n",
      "Epoch: 0082 loss_train: 0.3913 acc_train: 0.7000 loss_val: 0.7764 acc_val: 0.6111 time: 0.0063s\n",
      "Epoch: 0083 loss_train: 0.3354 acc_train: 0.9000 loss_val: 0.7780 acc_val: 0.6111 time: 0.0056s\n",
      "Epoch: 0084 loss_train: 0.3284 acc_train: 0.8000 loss_val: 0.7764 acc_val: 0.6222 time: 0.0059s\n",
      "Epoch: 0085 loss_train: 0.4070 acc_train: 0.6000 loss_val: 0.7811 acc_val: 0.6111 time: 0.0066s\n",
      "Epoch: 0086 loss_train: 0.5453 acc_train: 0.7000 loss_val: 0.7941 acc_val: 0.6111 time: 0.0064s\n",
      "Epoch: 0087 loss_train: 0.2482 acc_train: 1.0000 loss_val: 0.8076 acc_val: 0.6000 time: 0.0065s\n",
      "Epoch: 0088 loss_train: 0.3832 acc_train: 0.6000 loss_val: 0.8099 acc_val: 0.6000 time: 0.0063s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0089 loss_train: 0.4046 acc_train: 0.7000 loss_val: 0.7966 acc_val: 0.6111 time: 0.0080s\n",
      "Epoch: 0090 loss_train: 0.4104 acc_train: 0.8000 loss_val: 0.7928 acc_val: 0.6222 time: 0.0083s\n",
      "Epoch: 0091 loss_train: 0.3830 acc_train: 0.8000 loss_val: 0.7970 acc_val: 0.6222 time: 0.0076s\n",
      "Epoch: 0092 loss_train: 0.4092 acc_train: 0.7000 loss_val: 0.8061 acc_val: 0.6111 time: 0.0094s\n",
      "Epoch: 0093 loss_train: 0.3110 acc_train: 0.8000 loss_val: 0.8087 acc_val: 0.6111 time: 0.0069s\n",
      "Epoch: 0094 loss_train: 0.3236 acc_train: 0.7000 loss_val: 0.8125 acc_val: 0.6111 time: 0.0063s\n",
      "Epoch: 0095 loss_train: 0.2793 acc_train: 0.8000 loss_val: 0.8159 acc_val: 0.6222 time: 0.0116s\n",
      "Epoch: 0096 loss_train: 0.3084 acc_train: 0.9000 loss_val: 0.8210 acc_val: 0.6222 time: 0.0055s\n",
      "Epoch: 0097 loss_train: 0.2635 acc_train: 0.9000 loss_val: 0.8301 acc_val: 0.6111 time: 0.0056s\n",
      "Epoch: 0098 loss_train: 0.3799 acc_train: 0.7000 loss_val: 0.8364 acc_val: 0.6111 time: 0.0057s\n",
      "Epoch: 0099 loss_train: 0.2703 acc_train: 0.8000 loss_val: 0.8423 acc_val: 0.6222 time: 0.0057s\n",
      "Epoch: 0100 loss_train: 0.2337 acc_train: 0.9000 loss_val: 0.8490 acc_val: 0.6222 time: 0.0056s\n",
      "Epoch: 0101 loss_train: 0.3096 acc_train: 0.8000 loss_val: 0.8552 acc_val: 0.6222 time: 0.0057s\n",
      "Epoch: 0102 loss_train: 0.2724 acc_train: 0.9000 loss_val: 0.8640 acc_val: 0.6111 time: 0.0056s\n",
      "Epoch: 0103 loss_train: 0.3201 acc_train: 0.9000 loss_val: 0.8796 acc_val: 0.6111 time: 0.0057s\n",
      "Epoch: 0104 loss_train: 0.1635 acc_train: 0.9000 loss_val: 0.8913 acc_val: 0.6111 time: 0.0056s\n",
      "Epoch: 0105 loss_train: 0.2693 acc_train: 0.9000 loss_val: 0.9012 acc_val: 0.6111 time: 0.0056s\n",
      "Epoch: 0106 loss_train: 0.3700 acc_train: 0.7000 loss_val: 0.8997 acc_val: 0.6111 time: 0.0056s\n",
      "Epoch: 0107 loss_train: 0.2428 acc_train: 0.8000 loss_val: 0.8952 acc_val: 0.6111 time: 0.0055s\n",
      "Epoch: 0108 loss_train: 0.3475 acc_train: 0.7000 loss_val: 0.8951 acc_val: 0.6222 time: 0.0056s\n",
      "Epoch: 0109 loss_train: 0.1601 acc_train: 0.9000 loss_val: 0.9040 acc_val: 0.6222 time: 0.0055s\n",
      "Epoch: 0110 loss_train: 0.3509 acc_train: 0.9000 loss_val: 0.9261 acc_val: 0.6333 time: 0.0056s\n",
      "Epoch: 0111 loss_train: 0.3050 acc_train: 0.8000 loss_val: 0.9630 acc_val: 0.6556 time: 0.0055s\n",
      "Epoch: 0112 loss_train: 0.3282 acc_train: 0.7000 loss_val: 0.9910 acc_val: 0.6444 time: 0.0056s\n",
      "Epoch: 0113 loss_train: 0.3934 acc_train: 0.7000 loss_val: 0.9850 acc_val: 0.6444 time: 0.0055s\n",
      "Epoch: 0114 loss_train: 0.3182 acc_train: 0.9000 loss_val: 0.9437 acc_val: 0.6333 time: 0.0055s\n",
      "Epoch: 0115 loss_train: 0.3223 acc_train: 0.8000 loss_val: 0.9277 acc_val: 0.6222 time: 0.0056s\n",
      "Epoch: 0116 loss_train: 0.2296 acc_train: 0.9000 loss_val: 0.9321 acc_val: 0.6111 time: 0.0055s\n",
      "Epoch: 0117 loss_train: 0.3531 acc_train: 0.9000 loss_val: 0.9460 acc_val: 0.6111 time: 0.0055s\n",
      "Epoch: 0118 loss_train: 0.2146 acc_train: 1.0000 loss_val: 0.9676 acc_val: 0.6111 time: 0.0056s\n",
      "Epoch: 0119 loss_train: 0.1862 acc_train: 0.9000 loss_val: 0.9817 acc_val: 0.6000 time: 0.0056s\n",
      "Epoch: 0120 loss_train: 0.2297 acc_train: 0.8000 loss_val: 0.9798 acc_val: 0.5889 time: 0.0055s\n",
      "Epoch: 0121 loss_train: 0.1779 acc_train: 1.0000 loss_val: 0.9639 acc_val: 0.6111 time: 0.0071s\n",
      "Epoch: 0122 loss_train: 0.1798 acc_train: 1.0000 loss_val: 0.9540 acc_val: 0.6111 time: 0.0083s\n",
      "Epoch: 0123 loss_train: 0.3497 acc_train: 0.8000 loss_val: 0.9539 acc_val: 0.6222 time: 0.0082s\n",
      "Epoch: 0124 loss_train: 0.2800 acc_train: 0.9000 loss_val: 0.9607 acc_val: 0.6222 time: 0.0082s\n",
      "Epoch: 0125 loss_train: 0.2091 acc_train: 0.9000 loss_val: 0.9683 acc_val: 0.6111 time: 0.0082s\n",
      "Epoch: 0126 loss_train: 0.3598 acc_train: 0.7000 loss_val: 0.9778 acc_val: 0.6111 time: 0.0081s\n",
      "Epoch: 0127 loss_train: 0.2142 acc_train: 0.8000 loss_val: 0.9852 acc_val: 0.6333 time: 0.0096s\n",
      "Epoch: 0128 loss_train: 0.2877 acc_train: 0.9000 loss_val: 0.9816 acc_val: 0.6111 time: 0.0110s\n",
      "Epoch: 0129 loss_train: 0.2463 acc_train: 0.9000 loss_val: 0.9776 acc_val: 0.6222 time: 0.0115s\n",
      "Epoch: 0130 loss_train: 0.3743 acc_train: 0.7000 loss_val: 0.9799 acc_val: 0.6111 time: 0.0062s\n",
      "Epoch: 0131 loss_train: 0.2593 acc_train: 0.8000 loss_val: 0.9886 acc_val: 0.6111 time: 0.0071s\n",
      "Epoch: 0132 loss_train: 0.3426 acc_train: 0.8000 loss_val: 1.0009 acc_val: 0.6111 time: 0.0063s\n",
      "Epoch: 0133 loss_train: 0.2806 acc_train: 0.8000 loss_val: 1.0007 acc_val: 0.6111 time: 0.0063s\n",
      "Epoch: 0134 loss_train: 0.1170 acc_train: 1.0000 loss_val: 1.0010 acc_val: 0.6111 time: 0.0072s\n",
      "Epoch: 0135 loss_train: 0.3269 acc_train: 0.8000 loss_val: 0.9969 acc_val: 0.6111 time: 0.0067s\n",
      "Epoch: 0136 loss_train: 0.1943 acc_train: 0.9000 loss_val: 0.9976 acc_val: 0.6222 time: 0.0064s\n",
      "Epoch: 0137 loss_train: 0.3583 acc_train: 0.8000 loss_val: 1.0009 acc_val: 0.6222 time: 0.0068s\n",
      "Epoch: 0138 loss_train: 0.2539 acc_train: 0.9000 loss_val: 1.0059 acc_val: 0.6111 time: 0.0098s\n",
      "Epoch: 0139 loss_train: 0.2172 acc_train: 1.0000 loss_val: 1.0121 acc_val: 0.6111 time: 0.0064s\n",
      "Epoch: 0140 loss_train: 0.1515 acc_train: 0.9000 loss_val: 1.0196 acc_val: 0.6111 time: 0.0100s\n",
      "Epoch: 0141 loss_train: 0.2174 acc_train: 0.9000 loss_val: 1.0298 acc_val: 0.6111 time: 0.0109s\n",
      "Epoch: 0142 loss_train: 0.2061 acc_train: 0.8000 loss_val: 1.0329 acc_val: 0.6111 time: 0.0088s\n",
      "Epoch: 0143 loss_train: 0.2035 acc_train: 0.9000 loss_val: 1.0305 acc_val: 0.6111 time: 0.0155s\n",
      "Epoch: 0144 loss_train: 0.3311 acc_train: 0.7000 loss_val: 1.0263 acc_val: 0.6111 time: 0.0071s\n",
      "Epoch: 0145 loss_train: 0.3713 acc_train: 0.8000 loss_val: 1.0273 acc_val: 0.6222 time: 0.0248s\n",
      "Epoch: 0146 loss_train: 0.2225 acc_train: 0.9000 loss_val: 1.0309 acc_val: 0.6222 time: 0.0185s\n",
      "Epoch: 0147 loss_train: 0.2139 acc_train: 0.9000 loss_val: 1.0361 acc_val: 0.6222 time: 0.0124s\n",
      "Epoch: 0148 loss_train: 0.2637 acc_train: 0.9000 loss_val: 1.0417 acc_val: 0.6111 time: 0.0080s\n",
      "Epoch: 0149 loss_train: 0.3229 acc_train: 0.8000 loss_val: 1.0481 acc_val: 0.6111 time: 0.0144s\n",
      "Epoch: 0150 loss_train: 0.3265 acc_train: 0.9000 loss_val: 1.0493 acc_val: 0.6111 time: 0.0152s\n",
      "Epoch: 0151 loss_train: 0.3927 acc_train: 0.8000 loss_val: 1.0483 acc_val: 0.6222 time: 0.0166s\n",
      "Epoch: 0152 loss_train: 0.2500 acc_train: 0.9000 loss_val: 1.0627 acc_val: 0.6111 time: 0.0159s\n",
      "Epoch: 0153 loss_train: 0.2318 acc_train: 0.9000 loss_val: 1.0878 acc_val: 0.6111 time: 0.0114s\n",
      "Epoch: 0154 loss_train: 0.3771 acc_train: 0.7000 loss_val: 1.0961 acc_val: 0.6111 time: 0.0080s\n",
      "Epoch: 0155 loss_train: 0.2258 acc_train: 0.9000 loss_val: 1.0981 acc_val: 0.6111 time: 0.0133s\n",
      "Epoch: 0156 loss_train: 0.4079 acc_train: 0.8000 loss_val: 1.1026 acc_val: 0.6111 time: 0.0123s\n",
      "Epoch: 0157 loss_train: 0.2576 acc_train: 0.9000 loss_val: 1.0984 acc_val: 0.6111 time: 0.0069s\n",
      "Epoch: 0158 loss_train: 0.2471 acc_train: 0.9000 loss_val: 1.0938 acc_val: 0.6222 time: 0.0114s\n",
      "Epoch: 0159 loss_train: 0.2345 acc_train: 0.8000 loss_val: 1.0807 acc_val: 0.6111 time: 0.0062s\n",
      "Epoch: 0160 loss_train: 0.1557 acc_train: 1.0000 loss_val: 1.0778 acc_val: 0.6111 time: 0.0109s\n",
      "Epoch: 0161 loss_train: 0.3112 acc_train: 0.9000 loss_val: 1.0804 acc_val: 0.6111 time: 0.0120s\n",
      "Epoch: 0162 loss_train: 0.2457 acc_train: 0.8000 loss_val: 1.0819 acc_val: 0.6111 time: 0.0120s\n",
      "Epoch: 0163 loss_train: 0.2651 acc_train: 0.8000 loss_val: 1.0797 acc_val: 0.6222 time: 0.0088s\n",
      "Epoch: 0164 loss_train: 0.2583 acc_train: 0.9000 loss_val: 1.0826 acc_val: 0.6222 time: 0.0171s\n",
      "Epoch: 0165 loss_train: 0.1096 acc_train: 0.9000 loss_val: 1.0848 acc_val: 0.6222 time: 0.0141s\n",
      "Epoch: 0166 loss_train: 0.2796 acc_train: 0.9000 loss_val: 1.0891 acc_val: 0.6111 time: 0.0130s\n",
      "Epoch: 0167 loss_train: 0.3724 acc_train: 0.7000 loss_val: 1.0931 acc_val: 0.6111 time: 0.0074s\n",
      "Epoch: 0168 loss_train: 0.1405 acc_train: 1.0000 loss_val: 1.1018 acc_val: 0.6111 time: 0.0122s\n",
      "Epoch: 0169 loss_train: 0.3266 acc_train: 0.8000 loss_val: 1.1083 acc_val: 0.6111 time: 0.0136s\n",
      "Epoch: 0170 loss_train: 0.2740 acc_train: 0.9000 loss_val: 1.1194 acc_val: 0.6111 time: 0.0098s\n",
      "Epoch: 0171 loss_train: 0.2212 acc_train: 0.9000 loss_val: 1.1299 acc_val: 0.6222 time: 0.0080s\n",
      "Epoch: 0172 loss_train: 0.1732 acc_train: 0.9000 loss_val: 1.1341 acc_val: 0.6222 time: 0.0132s\n",
      "Epoch: 0173 loss_train: 0.2204 acc_train: 0.8000 loss_val: 1.1271 acc_val: 0.6111 time: 0.0137s\n",
      "Epoch: 0174 loss_train: 0.1720 acc_train: 0.9000 loss_val: 1.1196 acc_val: 0.6111 time: 0.0086s\n",
      "Epoch: 0175 loss_train: 0.2341 acc_train: 0.8000 loss_val: 1.1197 acc_val: 0.6222 time: 0.0103s\n",
      "Epoch: 0176 loss_train: 0.2875 acc_train: 0.8000 loss_val: 1.1240 acc_val: 0.6222 time: 0.0074s\n",
      "Epoch: 0177 loss_train: 0.3673 acc_train: 0.9000 loss_val: 1.1276 acc_val: 0.6111 time: 0.0072s\n",
      "Epoch: 0178 loss_train: 0.1781 acc_train: 0.8000 loss_val: 1.1321 acc_val: 0.6111 time: 0.0114s\n",
      "Epoch: 0179 loss_train: 0.3114 acc_train: 0.7000 loss_val: 1.1352 acc_val: 0.6111 time: 0.0082s\n",
      "Epoch: 0180 loss_train: 0.1649 acc_train: 0.9000 loss_val: 1.1390 acc_val: 0.6111 time: 0.0078s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0181 loss_train: 0.2347 acc_train: 0.8000 loss_val: 1.1408 acc_val: 0.6111 time: 0.0103s\n",
      "Epoch: 0182 loss_train: 0.1958 acc_train: 0.8000 loss_val: 1.1474 acc_val: 0.6333 time: 0.0083s\n",
      "Epoch: 0183 loss_train: 0.3515 acc_train: 0.8000 loss_val: 1.1449 acc_val: 0.6111 time: 0.0074s\n",
      "Epoch: 0184 loss_train: 0.2481 acc_train: 0.8000 loss_val: 1.1443 acc_val: 0.6111 time: 0.0066s\n",
      "Epoch: 0185 loss_train: 0.3830 acc_train: 0.7000 loss_val: 1.1447 acc_val: 0.6222 time: 0.0065s\n",
      "Epoch: 0186 loss_train: 0.2288 acc_train: 0.9000 loss_val: 1.1536 acc_val: 0.6111 time: 0.0114s\n",
      "Epoch: 0187 loss_train: 0.3337 acc_train: 0.9000 loss_val: 1.1647 acc_val: 0.6111 time: 0.0071s\n",
      "Epoch: 0188 loss_train: 0.2674 acc_train: 1.0000 loss_val: 1.1825 acc_val: 0.6111 time: 0.0068s\n",
      "Epoch: 0189 loss_train: 0.3735 acc_train: 0.8000 loss_val: 1.2016 acc_val: 0.6222 time: 0.0058s\n",
      "Epoch: 0190 loss_train: 0.3038 acc_train: 0.7000 loss_val: 1.2124 acc_val: 0.6111 time: 0.0062s\n",
      "Epoch: 0191 loss_train: 0.3558 acc_train: 0.7000 loss_val: 1.2048 acc_val: 0.6111 time: 0.0057s\n",
      "Epoch: 0192 loss_train: 0.2324 acc_train: 0.9000 loss_val: 1.1986 acc_val: 0.6111 time: 0.0057s\n",
      "Epoch: 0193 loss_train: 0.1665 acc_train: 0.9000 loss_val: 1.1915 acc_val: 0.6222 time: 0.0072s\n",
      "Epoch: 0194 loss_train: 0.2159 acc_train: 1.0000 loss_val: 1.1954 acc_val: 0.6222 time: 0.0064s\n",
      "Epoch: 0195 loss_train: 0.1672 acc_train: 0.9000 loss_val: 1.1917 acc_val: 0.6111 time: 0.0068s\n",
      "Epoch: 0196 loss_train: 0.1929 acc_train: 1.0000 loss_val: 1.1845 acc_val: 0.6111 time: 0.0064s\n",
      "Epoch: 0197 loss_train: 0.3082 acc_train: 0.8000 loss_val: 1.1879 acc_val: 0.6111 time: 0.0063s\n",
      "Epoch: 0198 loss_train: 0.0984 acc_train: 1.0000 loss_val: 1.1898 acc_val: 0.6111 time: 0.0069s\n",
      "Epoch: 0199 loss_train: 0.1679 acc_train: 1.0000 loss_val: 1.1953 acc_val: 0.6111 time: 0.0063s\n",
      "Epoch: 0200 loss_train: 0.1680 acc_train: 1.0000 loss_val: 1.2045 acc_val: 0.6111 time: 0.0068s\n",
      "Epoch: 0201 loss_train: 0.2340 acc_train: 0.8000 loss_val: 1.1993 acc_val: 0.6111 time: 0.0062s\n",
      "Epoch: 0202 loss_train: 0.1816 acc_train: 1.0000 loss_val: 1.1939 acc_val: 0.6111 time: 0.0062s\n",
      "Epoch: 0203 loss_train: 0.2606 acc_train: 0.8000 loss_val: 1.1905 acc_val: 0.6222 time: 0.0063s\n",
      "Epoch: 0204 loss_train: 0.2339 acc_train: 0.8000 loss_val: 1.1926 acc_val: 0.6222 time: 0.0061s\n",
      "Epoch: 0205 loss_train: 0.1785 acc_train: 0.8000 loss_val: 1.1950 acc_val: 0.6222 time: 0.0061s\n",
      "Epoch: 0206 loss_train: 0.1564 acc_train: 0.8000 loss_val: 1.1976 acc_val: 0.6222 time: 0.0063s\n",
      "Epoch: 0207 loss_train: 0.2492 acc_train: 0.8000 loss_val: 1.1994 acc_val: 0.6222 time: 0.0064s\n",
      "Epoch: 0208 loss_train: 0.2013 acc_train: 1.0000 loss_val: 1.2047 acc_val: 0.6111 time: 0.0063s\n",
      "Epoch: 0209 loss_train: 0.2497 acc_train: 1.0000 loss_val: 1.2175 acc_val: 0.6111 time: 0.0063s\n",
      "Epoch: 0210 loss_train: 0.2575 acc_train: 0.8000 loss_val: 1.2249 acc_val: 0.6111 time: 0.0062s\n",
      "Epoch: 0211 loss_train: 0.3541 acc_train: 0.6000 loss_val: 1.2309 acc_val: 0.6111 time: 0.0081s\n",
      "Epoch: 0212 loss_train: 0.2461 acc_train: 0.9000 loss_val: 1.2404 acc_val: 0.6222 time: 0.0057s\n",
      "Epoch: 0213 loss_train: 0.2846 acc_train: 0.8000 loss_val: 1.2408 acc_val: 0.6222 time: 0.0071s\n",
      "Epoch: 0214 loss_train: 0.2908 acc_train: 0.9000 loss_val: 1.2397 acc_val: 0.6111 time: 0.0082s\n",
      "Epoch: 0215 loss_train: 0.1027 acc_train: 1.0000 loss_val: 1.2424 acc_val: 0.6111 time: 0.0127s\n",
      "Epoch: 0216 loss_train: 0.2749 acc_train: 0.9000 loss_val: 1.2547 acc_val: 0.6222 time: 0.0072s\n",
      "Epoch: 0217 loss_train: 0.3129 acc_train: 0.8000 loss_val: 1.2577 acc_val: 0.6111 time: 0.0073s\n",
      "Epoch: 0218 loss_train: 0.2880 acc_train: 0.8000 loss_val: 1.2613 acc_val: 0.6111 time: 0.0081s\n",
      "Epoch: 0219 loss_train: 0.3269 acc_train: 0.7000 loss_val: 1.2541 acc_val: 0.6222 time: 0.0073s\n",
      "Epoch: 0220 loss_train: 0.2005 acc_train: 1.0000 loss_val: 1.2497 acc_val: 0.6111 time: 0.0073s\n",
      "Epoch: 0221 loss_train: 0.3167 acc_train: 0.7000 loss_val: 1.2410 acc_val: 0.6111 time: 0.0073s\n",
      "Epoch: 0222 loss_train: 0.2778 acc_train: 0.9000 loss_val: 1.2357 acc_val: 0.6111 time: 0.0072s\n",
      "Epoch: 0223 loss_train: 0.2811 acc_train: 0.8000 loss_val: 1.2334 acc_val: 0.6111 time: 0.0073s\n",
      "Epoch: 0224 loss_train: 0.3038 acc_train: 0.8000 loss_val: 1.2343 acc_val: 0.6222 time: 0.0070s\n",
      "Epoch: 0225 loss_train: 0.1927 acc_train: 1.0000 loss_val: 1.2370 acc_val: 0.6222 time: 0.0071s\n",
      "Epoch: 0226 loss_train: 0.3488 acc_train: 0.8000 loss_val: 1.2434 acc_val: 0.6111 time: 0.0071s\n",
      "Epoch: 0227 loss_train: 0.1486 acc_train: 1.0000 loss_val: 1.2527 acc_val: 0.6111 time: 0.0071s\n",
      "Epoch: 0228 loss_train: 0.1837 acc_train: 0.9000 loss_val: 1.2681 acc_val: 0.6222 time: 0.0071s\n",
      "Epoch: 0229 loss_train: 0.2810 acc_train: 0.8000 loss_val: 1.2729 acc_val: 0.6222 time: 0.0071s\n",
      "Epoch: 0230 loss_train: 0.2093 acc_train: 0.8000 loss_val: 1.2766 acc_val: 0.6222 time: 0.0071s\n",
      "Epoch: 0231 loss_train: 0.0719 acc_train: 1.0000 loss_val: 1.2671 acc_val: 0.6111 time: 0.0071s\n",
      "Epoch: 0232 loss_train: 0.3531 acc_train: 0.7000 loss_val: 1.2631 acc_val: 0.6111 time: 0.0071s\n",
      "Epoch: 0233 loss_train: 0.2687 acc_train: 0.9000 loss_val: 1.2633 acc_val: 0.6111 time: 0.0077s\n",
      "Epoch: 0234 loss_train: 0.3896 acc_train: 0.7000 loss_val: 1.2621 acc_val: 0.6111 time: 0.0078s\n",
      "Epoch: 0235 loss_train: 0.1963 acc_train: 0.9000 loss_val: 1.2606 acc_val: 0.6111 time: 0.0070s\n",
      "Epoch: 0236 loss_train: 0.2304 acc_train: 0.9000 loss_val: 1.2615 acc_val: 0.6222 time: 0.0071s\n",
      "Epoch: 0237 loss_train: 0.1717 acc_train: 0.9000 loss_val: 1.2641 acc_val: 0.6222 time: 0.0072s\n",
      "Epoch: 0238 loss_train: 0.3303 acc_train: 0.8000 loss_val: 1.2688 acc_val: 0.6111 time: 0.0086s\n",
      "Epoch: 0239 loss_train: 0.1893 acc_train: 1.0000 loss_val: 1.2814 acc_val: 0.6111 time: 0.0168s\n",
      "Epoch: 0240 loss_train: 0.3887 acc_train: 0.9000 loss_val: 1.3126 acc_val: 0.6111 time: 0.0072s\n",
      "Epoch: 0241 loss_train: 0.1927 acc_train: 0.9000 loss_val: 1.3495 acc_val: 0.6111 time: 0.0075s\n",
      "Epoch: 0242 loss_train: 0.2725 acc_train: 0.8000 loss_val: 1.3629 acc_val: 0.6000 time: 0.0081s\n",
      "Epoch: 0243 loss_train: 0.2043 acc_train: 1.0000 loss_val: 1.3538 acc_val: 0.6111 time: 0.0074s\n",
      "Epoch: 0244 loss_train: 0.2618 acc_train: 0.8000 loss_val: 1.3402 acc_val: 0.6222 time: 0.0072s\n",
      "Epoch: 0245 loss_train: 0.2565 acc_train: 0.8000 loss_val: 1.3262 acc_val: 0.6111 time: 0.0074s\n",
      "Epoch: 0246 loss_train: 0.1758 acc_train: 0.8000 loss_val: 1.3028 acc_val: 0.6111 time: 0.0073s\n",
      "Epoch: 0247 loss_train: 0.1742 acc_train: 0.9000 loss_val: 1.2968 acc_val: 0.6222 time: 0.0071s\n",
      "Epoch: 0248 loss_train: 0.1687 acc_train: 1.0000 loss_val: 1.3004 acc_val: 0.6222 time: 0.0071s\n",
      "Epoch: 0249 loss_train: 0.2102 acc_train: 0.8000 loss_val: 1.3052 acc_val: 0.6222 time: 0.0084s\n",
      "Epoch: 0250 loss_train: 0.2211 acc_train: 0.9000 loss_val: 1.3076 acc_val: 0.6222 time: 0.0061s\n",
      "Epoch: 0251 loss_train: 0.2439 acc_train: 0.8000 loss_val: 1.3124 acc_val: 0.6111 time: 0.0093s\n",
      "Epoch: 0252 loss_train: 0.2377 acc_train: 0.9000 loss_val: 1.3210 acc_val: 0.6111 time: 0.0057s\n",
      "Epoch: 0253 loss_train: 0.1827 acc_train: 0.9000 loss_val: 1.3309 acc_val: 0.6111 time: 0.0099s\n",
      "Epoch: 0254 loss_train: 0.2687 acc_train: 0.9000 loss_val: 1.3488 acc_val: 0.6222 time: 0.0112s\n",
      "Epoch: 0255 loss_train: 0.2519 acc_train: 0.9000 loss_val: 1.3786 acc_val: 0.6111 time: 0.0078s\n",
      "Epoch: 0256 loss_train: 0.3618 acc_train: 0.7000 loss_val: 1.3892 acc_val: 0.6111 time: 0.0105s\n",
      "Epoch: 0257 loss_train: 0.3060 acc_train: 0.9000 loss_val: 1.3698 acc_val: 0.6111 time: 0.0074s\n",
      "Epoch: 0258 loss_train: 0.2721 acc_train: 0.9000 loss_val: 1.3515 acc_val: 0.6222 time: 0.0133s\n",
      "Epoch: 0259 loss_train: 0.2735 acc_train: 0.8000 loss_val: 1.3313 acc_val: 0.6111 time: 0.0128s\n",
      "Epoch: 0260 loss_train: 0.3329 acc_train: 0.8000 loss_val: 1.3251 acc_val: 0.6111 time: 0.0072s\n",
      "Epoch: 0261 loss_train: 0.2393 acc_train: 0.7000 loss_val: 1.3213 acc_val: 0.6222 time: 0.0150s\n",
      "Epoch: 0262 loss_train: 0.4084 acc_train: 0.7000 loss_val: 1.3246 acc_val: 0.6111 time: 0.0137s\n",
      "Epoch: 0263 loss_train: 0.2606 acc_train: 0.8000 loss_val: 1.3283 acc_val: 0.6111 time: 0.0154s\n",
      "Epoch: 0264 loss_train: 0.3321 acc_train: 0.8000 loss_val: 1.3384 acc_val: 0.6111 time: 0.0132s\n",
      "Epoch: 0265 loss_train: 0.1526 acc_train: 0.9000 loss_val: 1.3551 acc_val: 0.6111 time: 0.0066s\n",
      "Epoch: 0266 loss_train: 0.3141 acc_train: 0.8000 loss_val: 1.3683 acc_val: 0.6111 time: 0.0124s\n",
      "Epoch: 0267 loss_train: 0.2013 acc_train: 0.8000 loss_val: 1.3771 acc_val: 0.6111 time: 0.0115s\n",
      "Epoch: 0268 loss_train: 0.3236 acc_train: 0.6000 loss_val: 1.3736 acc_val: 0.6111 time: 0.0059s\n",
      "Epoch: 0269 loss_train: 0.2499 acc_train: 1.0000 loss_val: 1.3727 acc_val: 0.6111 time: 0.0079s\n",
      "Epoch: 0270 loss_train: 0.1692 acc_train: 1.0000 loss_val: 1.3779 acc_val: 0.6111 time: 0.0079s\n",
      "Epoch: 0271 loss_train: 0.0885 acc_train: 1.0000 loss_val: 1.3878 acc_val: 0.5889 time: 0.0120s\n",
      "Epoch: 0272 loss_train: 0.3225 acc_train: 0.8000 loss_val: 1.3875 acc_val: 0.6111 time: 0.0078s\n",
      "Epoch: 0273 loss_train: 0.3836 acc_train: 0.6000 loss_val: 1.3683 acc_val: 0.6111 time: 0.0116s\n",
      "Epoch: 0274 loss_train: 0.3121 acc_train: 0.9000 loss_val: 1.3599 acc_val: 0.6111 time: 0.0103s\n",
      "Epoch: 0275 loss_train: 0.2871 acc_train: 0.7000 loss_val: 1.3539 acc_val: 0.6111 time: 0.0065s\n",
      "Epoch: 0276 loss_train: 0.1507 acc_train: 0.9000 loss_val: 1.3528 acc_val: 0.6222 time: 0.0113s\n",
      "Epoch: 0277 loss_train: 0.2127 acc_train: 0.9000 loss_val: 1.3549 acc_val: 0.6222 time: 0.0071s\n",
      "Epoch: 0278 loss_train: 0.1486 acc_train: 0.8000 loss_val: 1.3601 acc_val: 0.6111 time: 0.0107s\n",
      "Epoch: 0279 loss_train: 0.0981 acc_train: 0.9000 loss_val: 1.3630 acc_val: 0.6111 time: 0.0063s\n",
      "Epoch: 0280 loss_train: 0.1438 acc_train: 1.0000 loss_val: 1.3647 acc_val: 0.6111 time: 0.0107s\n",
      "Epoch: 0281 loss_train: 0.2302 acc_train: 1.0000 loss_val: 1.3658 acc_val: 0.6222 time: 0.0057s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0282 loss_train: 0.1623 acc_train: 0.9000 loss_val: 1.3700 acc_val: 0.6222 time: 0.0135s\n",
      "Epoch: 0283 loss_train: 0.2939 acc_train: 0.8000 loss_val: 1.3745 acc_val: 0.6111 time: 0.0121s\n",
      "Epoch: 0284 loss_train: 0.4117 acc_train: 0.8000 loss_val: 1.3857 acc_val: 0.6111 time: 0.0076s\n",
      "Epoch: 0285 loss_train: 0.3059 acc_train: 0.8000 loss_val: 1.4071 acc_val: 0.6111 time: 0.0110s\n",
      "Epoch: 0286 loss_train: 0.1749 acc_train: 1.0000 loss_val: 1.4287 acc_val: 0.6111 time: 0.0080s\n",
      "Epoch: 0287 loss_train: 0.2335 acc_train: 1.0000 loss_val: 1.4580 acc_val: 0.6000 time: 0.0081s\n",
      "Epoch: 0288 loss_train: 0.1136 acc_train: 1.0000 loss_val: 1.4671 acc_val: 0.6000 time: 0.0071s\n",
      "Epoch: 0289 loss_train: 0.2085 acc_train: 0.9000 loss_val: 1.4638 acc_val: 0.6000 time: 0.0072s\n",
      "Epoch: 0290 loss_train: 0.1066 acc_train: 0.9000 loss_val: 1.4378 acc_val: 0.6111 time: 0.0069s\n",
      "Epoch: 0291 loss_train: 0.2265 acc_train: 0.9000 loss_val: 1.4175 acc_val: 0.6111 time: 0.0072s\n",
      "Epoch: 0292 loss_train: 0.3256 acc_train: 0.8000 loss_val: 1.4017 acc_val: 0.6111 time: 0.0071s\n",
      "Epoch: 0293 loss_train: 0.1869 acc_train: 1.0000 loss_val: 1.3950 acc_val: 0.6222 time: 0.0074s\n",
      "Epoch: 0294 loss_train: 0.3322 acc_train: 0.9000 loss_val: 1.3966 acc_val: 0.6111 time: 0.0072s\n",
      "Epoch: 0295 loss_train: 0.2143 acc_train: 0.9000 loss_val: 1.4053 acc_val: 0.6333 time: 0.0070s\n",
      "Epoch: 0296 loss_train: 0.3472 acc_train: 0.8000 loss_val: 1.4071 acc_val: 0.6333 time: 0.0086s\n",
      "Epoch: 0297 loss_train: 0.2084 acc_train: 0.8000 loss_val: 1.4008 acc_val: 0.6111 time: 0.0062s\n",
      "Epoch: 0298 loss_train: 0.3020 acc_train: 0.9000 loss_val: 1.3963 acc_val: 0.6222 time: 0.0055s\n",
      "Epoch: 0299 loss_train: 0.0619 acc_train: 1.0000 loss_val: 1.3986 acc_val: 0.6111 time: 0.0078s\n",
      "Epoch: 0300 loss_train: 0.2141 acc_train: 0.8000 loss_val: 1.4167 acc_val: 0.6111 time: 0.0066s\n",
      "Epoch: 0301 loss_train: 0.3592 acc_train: 0.7000 loss_val: 1.4407 acc_val: 0.6111 time: 0.0070s\n",
      "Epoch: 0302 loss_train: 0.2564 acc_train: 1.0000 loss_val: 1.4756 acc_val: 0.6000 time: 0.0122s\n",
      "Epoch: 0303 loss_train: 0.1102 acc_train: 1.0000 loss_val: 1.5080 acc_val: 0.5889 time: 0.0114s\n",
      "Epoch: 0304 loss_train: 0.3041 acc_train: 0.8000 loss_val: 1.5313 acc_val: 0.5778 time: 0.0073s\n",
      "Epoch: 0305 loss_train: 0.2844 acc_train: 0.8000 loss_val: 1.5093 acc_val: 0.5889 time: 0.0137s\n",
      "Epoch: 0306 loss_train: 0.2503 acc_train: 0.9000 loss_val: 1.4727 acc_val: 0.6000 time: 0.0072s\n",
      "Epoch: 0307 loss_train: 0.3909 acc_train: 0.7000 loss_val: 1.4262 acc_val: 0.6111 time: 0.0170s\n",
      "Epoch: 0308 loss_train: 0.2282 acc_train: 0.9000 loss_val: 1.4117 acc_val: 0.6222 time: 0.0074s\n",
      "Epoch: 0309 loss_train: 0.1444 acc_train: 0.9000 loss_val: 1.4193 acc_val: 0.6111 time: 0.0119s\n",
      "Epoch: 0310 loss_train: 0.2160 acc_train: 0.7000 loss_val: 1.4454 acc_val: 0.6333 time: 0.0141s\n",
      "Epoch: 0311 loss_train: 0.4595 acc_train: 0.8000 loss_val: 1.4362 acc_val: 0.6444 time: 0.0069s\n",
      "Epoch: 0312 loss_train: 0.2505 acc_train: 0.8000 loss_val: 1.4221 acc_val: 0.6111 time: 0.0112s\n",
      "Epoch: 0313 loss_train: 0.1485 acc_train: 0.9000 loss_val: 1.4171 acc_val: 0.6222 time: 0.0077s\n",
      "Epoch: 0314 loss_train: 0.2991 acc_train: 0.6000 loss_val: 1.4207 acc_val: 0.6111 time: 0.0143s\n",
      "Epoch: 0315 loss_train: 0.2029 acc_train: 0.9000 loss_val: 1.4366 acc_val: 0.6111 time: 0.0119s\n",
      "Epoch: 0316 loss_train: 0.1911 acc_train: 0.9000 loss_val: 1.4589 acc_val: 0.6111 time: 0.0066s\n",
      "Epoch: 0317 loss_train: 0.1523 acc_train: 0.9000 loss_val: 1.4593 acc_val: 0.6111 time: 0.0130s\n",
      "Epoch: 0318 loss_train: 0.1590 acc_train: 0.8000 loss_val: 1.4531 acc_val: 0.6111 time: 0.0111s\n",
      "Epoch: 0319 loss_train: 0.2828 acc_train: 0.9000 loss_val: 1.4428 acc_val: 0.6222 time: 0.0067s\n",
      "Epoch: 0320 loss_train: 0.2865 acc_train: 0.9000 loss_val: 1.4316 acc_val: 0.6111 time: 0.0120s\n",
      "Epoch: 0321 loss_train: 0.1734 acc_train: 1.0000 loss_val: 1.4270 acc_val: 0.6111 time: 0.0116s\n",
      "Epoch: 0322 loss_train: 0.1730 acc_train: 0.8000 loss_val: 1.4228 acc_val: 0.6111 time: 0.0090s\n",
      "Epoch: 0323 loss_train: 0.2678 acc_train: 0.8000 loss_val: 1.4207 acc_val: 0.6111 time: 0.0120s\n",
      "Epoch: 0324 loss_train: 0.2880 acc_train: 0.8000 loss_val: 1.4229 acc_val: 0.6222 time: 0.0099s\n",
      "Epoch: 0325 loss_train: 0.1726 acc_train: 0.9000 loss_val: 1.4352 acc_val: 0.6111 time: 0.0100s\n",
      "Epoch: 0326 loss_train: 0.0930 acc_train: 1.0000 loss_val: 1.4551 acc_val: 0.6111 time: 0.0129s\n",
      "Epoch: 0327 loss_train: 0.3454 acc_train: 0.7000 loss_val: 1.4749 acc_val: 0.6111 time: 0.0122s\n",
      "Epoch: 0328 loss_train: 0.1373 acc_train: 1.0000 loss_val: 1.4888 acc_val: 0.6111 time: 0.0066s\n",
      "Epoch: 0329 loss_train: 0.2860 acc_train: 0.7000 loss_val: 1.4942 acc_val: 0.6111 time: 0.0109s\n",
      "Epoch: 0330 loss_train: 0.2880 acc_train: 0.8000 loss_val: 1.4956 acc_val: 0.6111 time: 0.0063s\n",
      "Epoch: 0331 loss_train: 0.1471 acc_train: 0.9000 loss_val: 1.4890 acc_val: 0.6111 time: 0.0113s\n",
      "Epoch: 0332 loss_train: 0.3907 acc_train: 0.8000 loss_val: 1.4919 acc_val: 0.6111 time: 0.0061s\n",
      "Epoch: 0333 loss_train: 0.1663 acc_train: 1.0000 loss_val: 1.4953 acc_val: 0.6111 time: 0.0092s\n",
      "Epoch: 0334 loss_train: 0.1562 acc_train: 0.9000 loss_val: 1.4958 acc_val: 0.6111 time: 0.0063s\n",
      "Epoch: 0335 loss_train: 0.1105 acc_train: 0.9000 loss_val: 1.4952 acc_val: 0.6222 time: 0.0064s\n",
      "Epoch: 0336 loss_train: 0.1178 acc_train: 0.9000 loss_val: 1.4869 acc_val: 0.6111 time: 0.0068s\n",
      "Epoch: 0337 loss_train: 0.1398 acc_train: 0.9000 loss_val: 1.4768 acc_val: 0.6111 time: 0.0063s\n",
      "Epoch: 0338 loss_train: 0.1762 acc_train: 1.0000 loss_val: 1.4723 acc_val: 0.6111 time: 0.0067s\n",
      "Epoch: 0339 loss_train: 0.2019 acc_train: 0.9000 loss_val: 1.4720 acc_val: 0.6111 time: 0.0066s\n",
      "Epoch: 0340 loss_train: 0.1656 acc_train: 1.0000 loss_val: 1.4743 acc_val: 0.6111 time: 0.0068s\n",
      "Epoch: 0341 loss_train: 0.3119 acc_train: 0.8000 loss_val: 1.4824 acc_val: 0.6111 time: 0.0069s\n",
      "Epoch: 0342 loss_train: 0.0625 acc_train: 1.0000 loss_val: 1.4937 acc_val: 0.6111 time: 0.0071s\n",
      "Epoch: 0343 loss_train: 0.0777 acc_train: 1.0000 loss_val: 1.5163 acc_val: 0.6111 time: 0.0060s\n",
      "Epoch: 0344 loss_train: 0.2567 acc_train: 0.9000 loss_val: 1.5439 acc_val: 0.6000 time: 0.0097s\n",
      "Epoch: 0345 loss_train: 0.3037 acc_train: 0.9000 loss_val: 1.5694 acc_val: 0.5889 time: 0.0062s\n",
      "Epoch: 0346 loss_train: 0.1789 acc_train: 0.8000 loss_val: 1.5765 acc_val: 0.5889 time: 0.0102s\n",
      "Epoch: 0347 loss_train: 0.1747 acc_train: 0.8000 loss_val: 1.5695 acc_val: 0.6000 time: 0.0092s\n",
      "Epoch: 0348 loss_train: 0.3445 acc_train: 0.7000 loss_val: 1.5604 acc_val: 0.6000 time: 0.0202s\n",
      "Epoch: 0349 loss_train: 0.2567 acc_train: 0.8000 loss_val: 1.5363 acc_val: 0.6111 time: 0.0113s\n",
      "Epoch: 0350 loss_train: 0.2308 acc_train: 0.8000 loss_val: 1.5223 acc_val: 0.6111 time: 0.0134s\n",
      "Epoch: 0351 loss_train: 0.2610 acc_train: 0.8000 loss_val: 1.5248 acc_val: 0.6111 time: 0.0078s\n",
      "Epoch: 0352 loss_train: 0.2515 acc_train: 0.9000 loss_val: 1.5289 acc_val: 0.6111 time: 0.0139s\n",
      "Epoch: 0353 loss_train: 0.2379 acc_train: 0.8000 loss_val: 1.5272 acc_val: 0.6111 time: 0.0152s\n",
      "Epoch: 0354 loss_train: 0.2030 acc_train: 0.8000 loss_val: 1.5221 acc_val: 0.6111 time: 0.0150s\n",
      "Epoch: 0355 loss_train: 0.1929 acc_train: 1.0000 loss_val: 1.5244 acc_val: 0.6111 time: 0.0148s\n",
      "Epoch: 0356 loss_train: 0.3865 acc_train: 0.8000 loss_val: 1.5195 acc_val: 0.6111 time: 0.0122s\n",
      "Epoch: 0357 loss_train: 0.4878 acc_train: 0.5000 loss_val: 1.5147 acc_val: 0.6111 time: 0.0055s\n",
      "Epoch: 0358 loss_train: 0.2876 acc_train: 0.7000 loss_val: 1.5115 acc_val: 0.6111 time: 0.0109s\n",
      "Epoch: 0359 loss_train: 0.1634 acc_train: 0.8000 loss_val: 1.5143 acc_val: 0.6111 time: 0.0057s\n",
      "Epoch: 0360 loss_train: 0.3527 acc_train: 0.8000 loss_val: 1.5311 acc_val: 0.6111 time: 0.0096s\n",
      "Epoch: 0361 loss_train: 0.2936 acc_train: 0.8000 loss_val: 1.5601 acc_val: 0.5778 time: 0.0084s\n",
      "Epoch: 0362 loss_train: 0.1381 acc_train: 1.0000 loss_val: 1.5850 acc_val: 0.6000 time: 0.0115s\n",
      "Epoch: 0363 loss_train: 0.2377 acc_train: 0.8000 loss_val: 1.5843 acc_val: 0.6000 time: 0.0065s\n",
      "Epoch: 0364 loss_train: 0.1644 acc_train: 0.9000 loss_val: 1.5769 acc_val: 0.6000 time: 0.0090s\n",
      "Epoch: 0365 loss_train: 0.2959 acc_train: 0.8000 loss_val: 1.5696 acc_val: 0.6000 time: 0.0061s\n",
      "Epoch: 0366 loss_train: 0.2430 acc_train: 0.9000 loss_val: 1.5658 acc_val: 0.6000 time: 0.0105s\n",
      "Epoch: 0367 loss_train: 0.1299 acc_train: 0.9000 loss_val: 1.5485 acc_val: 0.5778 time: 0.0119s\n",
      "Epoch: 0368 loss_train: 0.3205 acc_train: 0.8000 loss_val: 1.5232 acc_val: 0.6111 time: 0.0067s\n",
      "Epoch: 0369 loss_train: 0.2193 acc_train: 0.9000 loss_val: 1.5066 acc_val: 0.6111 time: 0.0118s\n",
      "Epoch: 0370 loss_train: 0.0902 acc_train: 0.9000 loss_val: 1.4973 acc_val: 0.6222 time: 0.0077s\n",
      "Epoch: 0371 loss_train: 0.1497 acc_train: 0.9000 loss_val: 1.4951 acc_val: 0.6222 time: 0.0106s\n",
      "Epoch: 0372 loss_train: 0.2492 acc_train: 0.8000 loss_val: 1.4957 acc_val: 0.6222 time: 0.0057s\n",
      "Epoch: 0373 loss_train: 0.3634 acc_train: 1.0000 loss_val: 1.4988 acc_val: 0.6222 time: 0.0084s\n",
      "Epoch: 0374 loss_train: 0.1781 acc_train: 0.9000 loss_val: 1.5150 acc_val: 0.6111 time: 0.0057s\n",
      "Epoch: 0375 loss_train: 0.1608 acc_train: 0.9000 loss_val: 1.5374 acc_val: 0.6111 time: 0.0057s\n",
      "Epoch: 0376 loss_train: 0.3342 acc_train: 0.9000 loss_val: 1.5516 acc_val: 0.5889 time: 0.0056s\n",
      "Epoch: 0377 loss_train: 0.2160 acc_train: 0.8000 loss_val: 1.5506 acc_val: 0.6000 time: 0.0058s\n",
      "Epoch: 0378 loss_train: 0.2539 acc_train: 0.8000 loss_val: 1.5301 acc_val: 0.6111 time: 0.0058s\n",
      "Epoch: 0379 loss_train: 0.1365 acc_train: 1.0000 loss_val: 1.5063 acc_val: 0.6111 time: 0.0057s\n",
      "Epoch: 0380 loss_train: 0.1632 acc_train: 0.8000 loss_val: 1.4865 acc_val: 0.6111 time: 0.0056s\n",
      "Epoch: 0381 loss_train: 0.2531 acc_train: 0.9000 loss_val: 1.4768 acc_val: 0.6111 time: 0.0057s\n",
      "Epoch: 0382 loss_train: 0.2167 acc_train: 0.9000 loss_val: 1.4724 acc_val: 0.6111 time: 0.0059s\n",
      "Epoch: 0383 loss_train: 0.1520 acc_train: 0.9000 loss_val: 1.4722 acc_val: 0.6111 time: 0.0057s\n",
      "Epoch: 0384 loss_train: 0.3466 acc_train: 0.8000 loss_val: 1.4751 acc_val: 0.6111 time: 0.0056s\n",
      "Epoch: 0385 loss_train: 0.4282 acc_train: 0.6000 loss_val: 1.4769 acc_val: 0.6111 time: 0.0055s\n",
      "Epoch: 0386 loss_train: 0.1281 acc_train: 0.9000 loss_val: 1.4808 acc_val: 0.6111 time: 0.0056s\n",
      "Epoch: 0387 loss_train: 0.1700 acc_train: 0.8000 loss_val: 1.4847 acc_val: 0.6111 time: 0.0055s\n",
      "Epoch: 0388 loss_train: 0.3231 acc_train: 0.8000 loss_val: 1.4917 acc_val: 0.6222 time: 0.0070s\n",
      "Epoch: 0389 loss_train: 0.3203 acc_train: 0.8000 loss_val: 1.4981 acc_val: 0.6111 time: 0.0074s\n",
      "Epoch: 0390 loss_train: 0.2094 acc_train: 0.8000 loss_val: 1.5030 acc_val: 0.6111 time: 0.0070s\n",
      "Epoch: 0391 loss_train: 0.2709 acc_train: 0.8000 loss_val: 1.5107 acc_val: 0.6111 time: 0.0078s\n",
      "Epoch: 0392 loss_train: 0.1404 acc_train: 0.9000 loss_val: 1.5150 acc_val: 0.6222 time: 0.0068s\n",
      "Epoch: 0393 loss_train: 0.1837 acc_train: 1.0000 loss_val: 1.5237 acc_val: 0.6111 time: 0.0065s\n",
      "Epoch: 0394 loss_train: 0.1845 acc_train: 0.9000 loss_val: 1.5343 acc_val: 0.6111 time: 0.0132s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0395 loss_train: 0.4873 acc_train: 0.8000 loss_val: 1.5428 acc_val: 0.6111 time: 0.0100s\n",
      "Epoch: 0396 loss_train: 0.4078 acc_train: 0.7000 loss_val: 1.5488 acc_val: 0.6111 time: 0.0076s\n",
      "Epoch: 0397 loss_train: 0.0951 acc_train: 1.0000 loss_val: 1.5495 acc_val: 0.6111 time: 0.0120s\n",
      "Epoch: 0398 loss_train: 0.3214 acc_train: 0.8000 loss_val: 1.5492 acc_val: 0.6111 time: 0.0168s\n",
      "Epoch: 0399 loss_train: 0.2426 acc_train: 0.9000 loss_val: 1.5445 acc_val: 0.6222 time: 0.0125s\n",
      "Epoch: 0400 loss_train: 0.1833 acc_train: 0.9000 loss_val: 1.5407 acc_val: 0.6111 time: 0.0076s\n",
      "Epoch: 0401 loss_train: 0.2191 acc_train: 0.8000 loss_val: 1.5372 acc_val: 0.6111 time: 0.0100s\n",
      "Epoch: 0402 loss_train: 0.3851 acc_train: 0.8000 loss_val: 1.5401 acc_val: 0.6111 time: 0.0129s\n",
      "Epoch: 0403 loss_train: 0.1156 acc_train: 1.0000 loss_val: 1.5453 acc_val: 0.6111 time: 0.0087s\n",
      "Epoch: 0404 loss_train: 0.2813 acc_train: 0.7000 loss_val: 1.5494 acc_val: 0.6111 time: 0.0090s\n",
      "Epoch: 0405 loss_train: 0.1028 acc_train: 0.9000 loss_val: 1.5513 acc_val: 0.6111 time: 0.0113s\n",
      "Epoch: 0406 loss_train: 0.3216 acc_train: 0.8000 loss_val: 1.5571 acc_val: 0.6111 time: 0.0063s\n",
      "Epoch: 0407 loss_train: 0.3647 acc_train: 0.8000 loss_val: 1.5583 acc_val: 0.6111 time: 0.0129s\n",
      "Epoch: 0408 loss_train: 0.1767 acc_train: 1.0000 loss_val: 1.5608 acc_val: 0.6111 time: 0.0102s\n",
      "Epoch: 0409 loss_train: 0.2446 acc_train: 0.9000 loss_val: 1.5648 acc_val: 0.6111 time: 0.0069s\n",
      "Epoch: 0410 loss_train: 0.3830 acc_train: 0.7000 loss_val: 1.5650 acc_val: 0.6111 time: 0.0125s\n",
      "Epoch: 0411 loss_train: 0.2504 acc_train: 0.9000 loss_val: 1.5653 acc_val: 0.6111 time: 0.0063s\n",
      "Epoch: 0412 loss_train: 0.0674 acc_train: 1.0000 loss_val: 1.5637 acc_val: 0.6111 time: 0.0134s\n",
      "Epoch: 0413 loss_train: 0.2372 acc_train: 0.8000 loss_val: 1.5620 acc_val: 0.6111 time: 0.0126s\n",
      "Epoch: 0414 loss_train: 0.1038 acc_train: 1.0000 loss_val: 1.5618 acc_val: 0.6111 time: 0.0144s\n",
      "Epoch: 0415 loss_train: 0.2857 acc_train: 0.8000 loss_val: 1.5624 acc_val: 0.6111 time: 0.0084s\n",
      "Epoch: 0416 loss_train: 0.3328 acc_train: 0.8000 loss_val: 1.5680 acc_val: 0.6111 time: 0.0119s\n",
      "Epoch: 0417 loss_train: 0.1613 acc_train: 0.8000 loss_val: 1.5724 acc_val: 0.6111 time: 0.0104s\n",
      "Epoch: 0418 loss_train: 0.0715 acc_train: 1.0000 loss_val: 1.5800 acc_val: 0.6222 time: 0.0087s\n",
      "Epoch: 0419 loss_train: 0.1468 acc_train: 1.0000 loss_val: 1.5823 acc_val: 0.6111 time: 0.0133s\n",
      "Epoch: 0420 loss_train: 0.1939 acc_train: 0.9000 loss_val: 1.5826 acc_val: 0.6111 time: 0.0110s\n",
      "Epoch: 0421 loss_train: 0.3831 acc_train: 0.6000 loss_val: 1.5775 acc_val: 0.6222 time: 0.0066s\n",
      "Epoch: 0422 loss_train: 0.2131 acc_train: 0.9000 loss_val: 1.5748 acc_val: 0.6222 time: 0.0110s\n",
      "Epoch: 0423 loss_train: 0.3701 acc_train: 0.9000 loss_val: 1.5752 acc_val: 0.6222 time: 0.0065s\n",
      "Epoch: 0424 loss_train: 0.2623 acc_train: 0.9000 loss_val: 1.5815 acc_val: 0.6111 time: 0.0112s\n",
      "Epoch: 0425 loss_train: 0.0895 acc_train: 1.0000 loss_val: 1.5873 acc_val: 0.5889 time: 0.0071s\n",
      "Epoch: 0426 loss_train: 0.1474 acc_train: 0.9000 loss_val: 1.5779 acc_val: 0.6111 time: 0.0109s\n",
      "Epoch: 0427 loss_train: 0.0987 acc_train: 1.0000 loss_val: 1.5660 acc_val: 0.6111 time: 0.0063s\n",
      "Epoch: 0428 loss_train: 0.2471 acc_train: 0.9000 loss_val: 1.5527 acc_val: 0.6111 time: 0.0106s\n",
      "Epoch: 0429 loss_train: 0.0864 acc_train: 0.9000 loss_val: 1.5424 acc_val: 0.6111 time: 0.0089s\n",
      "Epoch: 0430 loss_train: 0.3002 acc_train: 0.8000 loss_val: 1.5376 acc_val: 0.6111 time: 0.0072s\n",
      "Epoch: 0431 loss_train: 0.2699 acc_train: 0.9000 loss_val: 1.5359 acc_val: 0.6111 time: 0.0114s\n",
      "Epoch: 0432 loss_train: 0.1418 acc_train: 1.0000 loss_val: 1.5367 acc_val: 0.6111 time: 0.0069s\n",
      "Epoch: 0433 loss_train: 0.1332 acc_train: 1.0000 loss_val: 1.5395 acc_val: 0.6111 time: 0.0096s\n",
      "Epoch: 0434 loss_train: 0.1707 acc_train: 0.8000 loss_val: 1.5425 acc_val: 0.5889 time: 0.0058s\n",
      "Epoch: 0435 loss_train: 0.2802 acc_train: 0.8000 loss_val: 1.5477 acc_val: 0.6111 time: 0.0057s\n",
      "Epoch: 0436 loss_train: 0.2536 acc_train: 0.9000 loss_val: 1.5501 acc_val: 0.6111 time: 0.0056s\n",
      "Epoch: 0437 loss_train: 0.2846 acc_train: 0.8000 loss_val: 1.5514 acc_val: 0.6111 time: 0.0078s\n",
      "Epoch: 0438 loss_train: 0.1200 acc_train: 0.9000 loss_val: 1.5538 acc_val: 0.6222 time: 0.0102s\n",
      "Epoch: 0439 loss_train: 0.1290 acc_train: 0.9000 loss_val: 1.5541 acc_val: 0.6111 time: 0.0058s\n",
      "Epoch: 0440 loss_train: 0.1969 acc_train: 0.9000 loss_val: 1.5557 acc_val: 0.6111 time: 0.0093s\n",
      "Epoch: 0441 loss_train: 0.2585 acc_train: 0.8000 loss_val: 1.5624 acc_val: 0.6111 time: 0.0057s\n",
      "Epoch: 0442 loss_train: 0.1620 acc_train: 0.8000 loss_val: 1.5715 acc_val: 0.6222 time: 0.0066s\n",
      "Epoch: 0443 loss_train: 0.1562 acc_train: 0.9000 loss_val: 1.5766 acc_val: 0.6222 time: 0.0065s\n",
      "Epoch: 0444 loss_train: 0.1353 acc_train: 1.0000 loss_val: 1.5855 acc_val: 0.6111 time: 0.0064s\n",
      "Epoch: 0445 loss_train: 0.1649 acc_train: 1.0000 loss_val: 1.5972 acc_val: 0.6111 time: 0.0064s\n",
      "Epoch: 0446 loss_train: 0.1635 acc_train: 1.0000 loss_val: 1.6087 acc_val: 0.5889 time: 0.0065s\n",
      "Epoch: 0447 loss_train: 0.1547 acc_train: 1.0000 loss_val: 1.6244 acc_val: 0.5778 time: 0.0062s\n",
      "Epoch: 0448 loss_train: 0.2470 acc_train: 0.8000 loss_val: 1.6357 acc_val: 0.5889 time: 0.0064s\n",
      "Epoch: 0449 loss_train: 0.1382 acc_train: 0.9000 loss_val: 1.6262 acc_val: 0.5889 time: 0.0063s\n",
      "Epoch: 0450 loss_train: 0.2561 acc_train: 0.9000 loss_val: 1.6159 acc_val: 0.6111 time: 0.0063s\n",
      "Epoch: 0451 loss_train: 0.2554 acc_train: 0.8000 loss_val: 1.6119 acc_val: 0.6111 time: 0.0063s\n",
      "Epoch: 0452 loss_train: 0.3139 acc_train: 0.8000 loss_val: 1.6121 acc_val: 0.6111 time: 0.0064s\n",
      "Epoch: 0453 loss_train: 0.2860 acc_train: 0.7000 loss_val: 1.6117 acc_val: 0.6111 time: 0.0062s\n",
      "Epoch: 0454 loss_train: 0.2703 acc_train: 0.8000 loss_val: 1.6142 acc_val: 0.6111 time: 0.0062s\n",
      "Epoch: 0455 loss_train: 0.3243 acc_train: 0.9000 loss_val: 1.6206 acc_val: 0.6111 time: 0.0073s\n",
      "Epoch: 0456 loss_train: 0.0548 acc_train: 1.0000 loss_val: 1.6318 acc_val: 0.6111 time: 0.0062s\n",
      "Epoch: 0457 loss_train: 0.1465 acc_train: 0.9000 loss_val: 1.6469 acc_val: 0.5889 time: 0.0091s\n",
      "Epoch: 0458 loss_train: 0.2057 acc_train: 0.9000 loss_val: 1.6562 acc_val: 0.5889 time: 0.0072s\n",
      "Epoch: 0459 loss_train: 0.2097 acc_train: 0.9000 loss_val: 1.6660 acc_val: 0.6000 time: 0.0100s\n",
      "Epoch: 0460 loss_train: 0.2173 acc_train: 0.9000 loss_val: 1.6672 acc_val: 0.5889 time: 0.0073s\n",
      "Epoch: 0461 loss_train: 0.1971 acc_train: 0.9000 loss_val: 1.6593 acc_val: 0.6000 time: 0.0107s\n",
      "Epoch: 0462 loss_train: 0.2187 acc_train: 0.9000 loss_val: 1.6402 acc_val: 0.5778 time: 0.0066s\n",
      "Epoch: 0463 loss_train: 0.4150 acc_train: 0.7000 loss_val: 1.6159 acc_val: 0.6111 time: 0.0094s\n",
      "Epoch: 0464 loss_train: 0.2418 acc_train: 1.0000 loss_val: 1.6033 acc_val: 0.6111 time: 0.0138s\n",
      "Epoch: 0465 loss_train: 0.1275 acc_train: 0.9000 loss_val: 1.5983 acc_val: 0.6111 time: 0.0069s\n",
      "Epoch: 0466 loss_train: 0.2915 acc_train: 0.8000 loss_val: 1.5955 acc_val: 0.6111 time: 0.0135s\n",
      "Epoch: 0467 loss_train: 0.1190 acc_train: 0.9000 loss_val: 1.5963 acc_val: 0.6111 time: 0.0118s\n",
      "Epoch: 0468 loss_train: 0.3248 acc_train: 0.8000 loss_val: 1.6036 acc_val: 0.6111 time: 0.0084s\n",
      "Epoch: 0469 loss_train: 0.1630 acc_train: 0.8000 loss_val: 1.6116 acc_val: 0.6111 time: 0.0121s\n",
      "Epoch: 0470 loss_train: 0.0931 acc_train: 1.0000 loss_val: 1.6293 acc_val: 0.5889 time: 0.0115s\n",
      "Epoch: 0471 loss_train: 0.1794 acc_train: 0.8000 loss_val: 1.6417 acc_val: 0.6000 time: 0.0072s\n",
      "Epoch: 0472 loss_train: 0.1242 acc_train: 1.0000 loss_val: 1.6480 acc_val: 0.5889 time: 0.0121s\n",
      "Epoch: 0473 loss_train: 0.1557 acc_train: 0.9000 loss_val: 1.6419 acc_val: 0.6000 time: 0.0069s\n",
      "Epoch: 0474 loss_train: 0.2272 acc_train: 0.9000 loss_val: 1.6363 acc_val: 0.6000 time: 0.0106s\n",
      "Epoch: 0475 loss_train: 0.2436 acc_train: 0.9000 loss_val: 1.6281 acc_val: 0.5778 time: 0.0058s\n",
      "Epoch: 0476 loss_train: 0.1021 acc_train: 0.9000 loss_val: 1.6221 acc_val: 0.5889 time: 0.0110s\n",
      "Epoch: 0477 loss_train: 0.1540 acc_train: 0.8000 loss_val: 1.6175 acc_val: 0.6111 time: 0.0059s\n",
      "Epoch: 0478 loss_train: 0.0631 acc_train: 1.0000 loss_val: 1.6140 acc_val: 0.6111 time: 0.0120s\n",
      "Epoch: 0479 loss_train: 0.1814 acc_train: 0.8000 loss_val: 1.6131 acc_val: 0.6111 time: 0.0112s\n",
      "Epoch: 0480 loss_train: 0.2305 acc_train: 0.9000 loss_val: 1.6148 acc_val: 0.6111 time: 0.0057s\n",
      "Epoch: 0481 loss_train: 0.1714 acc_train: 1.0000 loss_val: 1.6196 acc_val: 0.6111 time: 0.0118s\n",
      "Epoch: 0482 loss_train: 0.1917 acc_train: 0.9000 loss_val: 1.6264 acc_val: 0.6111 time: 0.0065s\n",
      "Epoch: 0483 loss_train: 0.2441 acc_train: 0.9000 loss_val: 1.6370 acc_val: 0.5778 time: 0.0127s\n",
      "Epoch: 0484 loss_train: 0.1520 acc_train: 0.9000 loss_val: 1.6595 acc_val: 0.5889 time: 0.0081s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0485 loss_train: 0.1953 acc_train: 0.8000 loss_val: 1.6818 acc_val: 0.5778 time: 0.0098s\n",
      "Epoch: 0486 loss_train: 0.1909 acc_train: 0.9000 loss_val: 1.6910 acc_val: 0.5778 time: 0.0110s\n",
      "Epoch: 0487 loss_train: 0.4887 acc_train: 0.7000 loss_val: 1.6648 acc_val: 0.5778 time: 0.0135s\n",
      "Epoch: 0488 loss_train: 0.3162 acc_train: 0.9000 loss_val: 1.6241 acc_val: 0.5889 time: 0.0069s\n",
      "Epoch: 0489 loss_train: 0.2316 acc_train: 0.9000 loss_val: 1.5911 acc_val: 0.5778 time: 0.0112s\n",
      "Epoch: 0490 loss_train: 0.2298 acc_train: 0.9000 loss_val: 1.5629 acc_val: 0.6111 time: 0.0075s\n",
      "Epoch: 0491 loss_train: 0.2531 acc_train: 0.9000 loss_val: 1.5460 acc_val: 0.6111 time: 0.0108s\n",
      "Epoch: 0492 loss_train: 0.2312 acc_train: 1.0000 loss_val: 1.5408 acc_val: 0.6111 time: 0.0057s\n",
      "Epoch: 0493 loss_train: 0.3344 acc_train: 0.8000 loss_val: 1.5469 acc_val: 0.6111 time: 0.0109s\n",
      "Epoch: 0494 loss_train: 0.2961 acc_train: 0.9000 loss_val: 1.5709 acc_val: 0.6000 time: 0.0055s\n",
      "Epoch: 0495 loss_train: 0.2659 acc_train: 0.9000 loss_val: 1.5930 acc_val: 0.5778 time: 0.0116s\n",
      "Epoch: 0496 loss_train: 0.2519 acc_train: 0.8000 loss_val: 1.6162 acc_val: 0.6000 time: 0.0094s\n",
      "Epoch: 0497 loss_train: 0.1406 acc_train: 1.0000 loss_val: 1.6355 acc_val: 0.6000 time: 0.0073s\n",
      "Epoch: 0498 loss_train: 0.1341 acc_train: 1.0000 loss_val: 1.6432 acc_val: 0.6000 time: 0.0107s\n",
      "Epoch: 0499 loss_train: 0.3960 acc_train: 0.7000 loss_val: 1.6440 acc_val: 0.5778 time: 0.0071s\n",
      "Epoch: 0500 loss_train: 0.1120 acc_train: 0.9000 loss_val: 1.6436 acc_val: 0.6111 time: 0.0070s\n",
      "Epoch: 0501 loss_train: 0.2335 acc_train: 0.8000 loss_val: 1.6435 acc_val: 0.6111 time: 0.0070s\n",
      "Epoch: 0502 loss_train: 0.2004 acc_train: 0.9000 loss_val: 1.6521 acc_val: 0.6111 time: 0.0072s\n",
      "Epoch: 0503 loss_train: 0.3039 acc_train: 0.8000 loss_val: 1.6684 acc_val: 0.6111 time: 0.0070s\n",
      "Epoch: 0504 loss_train: 0.1169 acc_train: 1.0000 loss_val: 1.6918 acc_val: 0.6111 time: 0.0069s\n",
      "Epoch: 0505 loss_train: 0.2400 acc_train: 0.9000 loss_val: 1.7207 acc_val: 0.5778 time: 0.0067s\n",
      "Epoch: 0506 loss_train: 0.1727 acc_train: 0.9000 loss_val: 1.7473 acc_val: 0.5889 time: 0.0072s\n",
      "Epoch: 0507 loss_train: 0.4692 acc_train: 0.6000 loss_val: 1.7494 acc_val: 0.6000 time: 0.0067s\n",
      "Epoch: 0508 loss_train: 0.1256 acc_train: 0.9000 loss_val: 1.7470 acc_val: 0.6000 time: 0.0069s\n",
      "Epoch: 0509 loss_train: 0.2443 acc_train: 0.9000 loss_val: 1.7271 acc_val: 0.5778 time: 0.0085s\n",
      "Epoch: 0510 loss_train: 0.1820 acc_train: 0.8000 loss_val: 1.7091 acc_val: 0.6111 time: 0.0064s\n",
      "Epoch: 0511 loss_train: 0.2787 acc_train: 0.9000 loss_val: 1.6943 acc_val: 0.6111 time: 0.0062s\n",
      "Epoch: 0512 loss_train: 0.1998 acc_train: 0.8000 loss_val: 1.6862 acc_val: 0.6111 time: 0.0078s\n",
      "Epoch: 0513 loss_train: 0.4347 acc_train: 0.8000 loss_val: 1.6828 acc_val: 0.6111 time: 0.0074s\n",
      "Epoch: 0514 loss_train: 0.2859 acc_train: 0.7000 loss_val: 1.6808 acc_val: 0.6111 time: 0.0104s\n",
      "Epoch: 0515 loss_train: 0.2700 acc_train: 0.8000 loss_val: 1.6780 acc_val: 0.6111 time: 0.0092s\n",
      "Epoch: 0516 loss_train: 0.1596 acc_train: 0.9000 loss_val: 1.6754 acc_val: 0.6111 time: 0.0080s\n",
      "Epoch: 0517 loss_train: 0.1451 acc_train: 0.9000 loss_val: 1.6680 acc_val: 0.6111 time: 0.0069s\n",
      "Epoch: 0518 loss_train: 0.2777 acc_train: 0.9000 loss_val: 1.6594 acc_val: 0.6111 time: 0.0066s\n",
      "Epoch: 0519 loss_train: 0.4059 acc_train: 0.8000 loss_val: 1.6539 acc_val: 0.6111 time: 0.0067s\n",
      "Epoch: 0520 loss_train: 0.2069 acc_train: 0.9000 loss_val: 1.6516 acc_val: 0.6111 time: 0.0067s\n",
      "Epoch: 0521 loss_train: 0.3618 acc_train: 0.9000 loss_val: 1.6526 acc_val: 0.5778 time: 0.0114s\n",
      "Epoch: 0522 loss_train: 0.2514 acc_train: 0.8000 loss_val: 1.6575 acc_val: 0.6000 time: 0.0064s\n",
      "Epoch: 0523 loss_train: 0.1241 acc_train: 1.0000 loss_val: 1.6546 acc_val: 0.6000 time: 0.0062s\n",
      "Epoch: 0524 loss_train: 0.2839 acc_train: 0.9000 loss_val: 1.6536 acc_val: 0.6000 time: 0.0062s\n",
      "Epoch: 0525 loss_train: 0.2107 acc_train: 0.9000 loss_val: 1.6443 acc_val: 0.5778 time: 0.0063s\n",
      "Epoch: 0526 loss_train: 0.1757 acc_train: 0.8000 loss_val: 1.6315 acc_val: 0.6111 time: 0.0063s\n",
      "Epoch: 0527 loss_train: 0.2081 acc_train: 0.9000 loss_val: 1.6343 acc_val: 0.6111 time: 0.0061s\n",
      "Epoch: 0528 loss_train: 0.0796 acc_train: 1.0000 loss_val: 1.6372 acc_val: 0.6111 time: 0.0062s\n",
      "Epoch: 0529 loss_train: 0.1279 acc_train: 1.0000 loss_val: 1.6438 acc_val: 0.6111 time: 0.0083s\n",
      "Epoch: 0530 loss_train: 0.3438 acc_train: 0.8000 loss_val: 1.6533 acc_val: 0.6111 time: 0.0070s\n",
      "Epoch: 0531 loss_train: 0.1458 acc_train: 0.9000 loss_val: 1.6574 acc_val: 0.6111 time: 0.0079s\n",
      "Epoch: 0532 loss_train: 0.0487 acc_train: 1.0000 loss_val: 1.6631 acc_val: 0.6111 time: 0.0058s\n",
      "Epoch: 0533 loss_train: 0.1230 acc_train: 1.0000 loss_val: 1.6756 acc_val: 0.5778 time: 0.0058s\n",
      "Epoch: 0534 loss_train: 0.0679 acc_train: 1.0000 loss_val: 1.6828 acc_val: 0.5889 time: 0.0056s\n",
      "Epoch: 0535 loss_train: 0.2327 acc_train: 0.9000 loss_val: 1.6665 acc_val: 0.5889 time: 0.0076s\n",
      "Epoch: 0536 loss_train: 0.1960 acc_train: 0.9000 loss_val: 1.6447 acc_val: 0.5778 time: 0.0155s\n",
      "Epoch: 0537 loss_train: 0.3914 acc_train: 0.8000 loss_val: 1.6140 acc_val: 0.5889 time: 0.0108s\n",
      "Epoch: 0538 loss_train: 0.1850 acc_train: 1.0000 loss_val: 1.5921 acc_val: 0.6000 time: 0.0113s\n",
      "Epoch: 0539 loss_train: 0.1470 acc_train: 0.9000 loss_val: 1.5718 acc_val: 0.6111 time: 0.0096s\n",
      "Epoch: 0540 loss_train: 0.1402 acc_train: 1.0000 loss_val: 1.5554 acc_val: 0.6111 time: 0.0146s\n",
      "Epoch: 0541 loss_train: 0.2355 acc_train: 0.8000 loss_val: 1.5478 acc_val: 0.6111 time: 0.0096s\n",
      "Epoch: 0542 loss_train: 0.1595 acc_train: 1.0000 loss_val: 1.5427 acc_val: 0.6111 time: 0.0075s\n",
      "Epoch: 0543 loss_train: 0.3141 acc_train: 0.6000 loss_val: 1.5409 acc_val: 0.6111 time: 0.0120s\n",
      "Epoch: 0544 loss_train: 0.2747 acc_train: 0.9000 loss_val: 1.5372 acc_val: 0.6111 time: 0.0058s\n",
      "Epoch: 0545 loss_train: 0.2265 acc_train: 0.9000 loss_val: 1.5381 acc_val: 0.5778 time: 0.0139s\n",
      "Epoch: 0546 loss_train: 0.0958 acc_train: 1.0000 loss_val: 1.5408 acc_val: 0.5889 time: 0.0106s\n",
      "Epoch: 0547 loss_train: 0.1691 acc_train: 0.9000 loss_val: 1.5370 acc_val: 0.6000 time: 0.0071s\n",
      "Epoch: 0548 loss_train: 0.3372 acc_train: 0.7000 loss_val: 1.5337 acc_val: 0.5778 time: 0.0107s\n",
      "Epoch: 0549 loss_train: 0.3789 acc_train: 0.7000 loss_val: 1.5325 acc_val: 0.6000 time: 0.0057s\n",
      "Epoch: 0550 loss_train: 0.2075 acc_train: 0.9000 loss_val: 1.5406 acc_val: 0.6111 time: 0.0108s\n",
      "Epoch: 0551 loss_train: 0.3265 acc_train: 0.8000 loss_val: 1.5543 acc_val: 0.6111 time: 0.0058s\n",
      "Epoch: 0552 loss_train: 0.2725 acc_train: 0.8000 loss_val: 1.5725 acc_val: 0.6111 time: 0.0122s\n",
      "Epoch: 0553 loss_train: 0.3611 acc_train: 0.7000 loss_val: 1.5916 acc_val: 0.6222 time: 0.0059s\n",
      "Epoch: 0554 loss_train: 0.2060 acc_train: 0.9000 loss_val: 1.6076 acc_val: 0.6111 time: 0.0113s\n",
      "Epoch: 0555 loss_train: 0.2846 acc_train: 0.8000 loss_val: 1.6259 acc_val: 0.6111 time: 0.0078s\n",
      "Epoch: 0556 loss_train: 0.1998 acc_train: 0.9000 loss_val: 1.6532 acc_val: 0.6000 time: 0.0080s\n",
      "Epoch: 0557 loss_train: 0.2400 acc_train: 0.9000 loss_val: 1.6984 acc_val: 0.6000 time: 0.0110s\n",
      "Epoch: 0558 loss_train: 0.4163 acc_train: 0.8000 loss_val: 1.7548 acc_val: 0.5778 time: 0.0062s\n",
      "Epoch: 0559 loss_train: 0.3097 acc_train: 0.7000 loss_val: 1.7516 acc_val: 0.5778 time: 0.0137s\n",
      "Epoch: 0560 loss_train: 0.3234 acc_train: 0.7000 loss_val: 1.7036 acc_val: 0.5778 time: 0.0117s\n",
      "Epoch: 0561 loss_train: 0.0554 acc_train: 1.0000 loss_val: 1.6602 acc_val: 0.6111 time: 0.0085s\n",
      "Epoch: 0562 loss_train: 0.4652 acc_train: 0.6000 loss_val: 1.6343 acc_val: 0.6111 time: 0.0112s\n",
      "Epoch: 0563 loss_train: 0.2188 acc_train: 0.7000 loss_val: 1.6302 acc_val: 0.6111 time: 0.0063s\n",
      "Epoch: 0564 loss_train: 0.2715 acc_train: 0.8000 loss_val: 1.6301 acc_val: 0.6111 time: 0.0118s\n",
      "Epoch: 0565 loss_train: 0.3170 acc_train: 0.8000 loss_val: 1.6262 acc_val: 0.6222 time: 0.0110s\n",
      "Epoch: 0566 loss_train: 0.2675 acc_train: 0.9000 loss_val: 1.6344 acc_val: 0.6111 time: 0.0069s\n",
      "Epoch: 0567 loss_train: 0.2071 acc_train: 0.9000 loss_val: 1.6566 acc_val: 0.6111 time: 0.0115s\n",
      "Epoch: 0568 loss_train: 0.0346 acc_train: 1.0000 loss_val: 1.6886 acc_val: 0.5778 time: 0.0062s\n",
      "Epoch: 0569 loss_train: 0.1536 acc_train: 1.0000 loss_val: 1.6948 acc_val: 0.5778 time: 0.0107s\n",
      "Epoch: 0570 loss_train: 0.3318 acc_train: 0.8000 loss_val: 1.6862 acc_val: 0.5778 time: 0.0058s\n",
      "Epoch: 0571 loss_train: 0.1063 acc_train: 0.9000 loss_val: 1.6674 acc_val: 0.6000 time: 0.0107s\n",
      "Epoch: 0572 loss_train: 0.2394 acc_train: 0.8000 loss_val: 1.6495 acc_val: 0.6111 time: 0.0061s\n",
      "Epoch: 0573 loss_train: 0.2088 acc_train: 0.8000 loss_val: 1.6463 acc_val: 0.6111 time: 0.0083s\n",
      "Epoch: 0574 loss_train: 0.0833 acc_train: 1.0000 loss_val: 1.6539 acc_val: 0.6222 time: 0.0069s\n",
      "Epoch: 0575 loss_train: 0.2800 acc_train: 0.8000 loss_val: 1.6611 acc_val: 0.6222 time: 0.0071s\n",
      "Epoch: 0576 loss_train: 0.1880 acc_train: 0.9000 loss_val: 1.6667 acc_val: 0.6111 time: 0.0072s\n",
      "Epoch: 0577 loss_train: 0.0695 acc_train: 1.0000 loss_val: 1.6673 acc_val: 0.6111 time: 0.0072s\n",
      "Epoch: 0578 loss_train: 0.2710 acc_train: 0.8000 loss_val: 1.6662 acc_val: 0.6111 time: 0.0068s\n",
      "Epoch: 0579 loss_train: 0.2294 acc_train: 0.8000 loss_val: 1.6666 acc_val: 0.6000 time: 0.0079s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0580 loss_train: 0.2392 acc_train: 0.9000 loss_val: 1.6709 acc_val: 0.5778 time: 0.0084s\n",
      "Epoch: 0581 loss_train: 0.3423 acc_train: 0.7000 loss_val: 1.6772 acc_val: 0.5889 time: 0.0073s\n",
      "Epoch: 0582 loss_train: 0.2229 acc_train: 0.9000 loss_val: 1.6769 acc_val: 0.5889 time: 0.0092s\n",
      "Epoch: 0583 loss_train: 0.2647 acc_train: 0.8000 loss_val: 1.6723 acc_val: 0.5889 time: 0.0075s\n",
      "Epoch: 0584 loss_train: 0.2812 acc_train: 0.8000 loss_val: 1.6503 acc_val: 0.5778 time: 0.0079s\n",
      "Epoch: 0585 loss_train: 0.2642 acc_train: 0.9000 loss_val: 1.6317 acc_val: 0.6111 time: 0.0070s\n",
      "Epoch: 0586 loss_train: 0.1157 acc_train: 0.9000 loss_val: 1.6232 acc_val: 0.6111 time: 0.0071s\n",
      "Epoch: 0587 loss_train: 0.1661 acc_train: 0.8000 loss_val: 1.6277 acc_val: 0.6111 time: 0.0065s\n",
      "Epoch: 0588 loss_train: 0.1972 acc_train: 0.9000 loss_val: 1.6407 acc_val: 0.6111 time: 0.0083s\n",
      "Epoch: 0589 loss_train: 0.2750 acc_train: 0.7000 loss_val: 1.6569 acc_val: 0.6111 time: 0.0058s\n",
      "Epoch: 0590 loss_train: 0.0985 acc_train: 1.0000 loss_val: 1.6787 acc_val: 0.5778 time: 0.0064s\n",
      "Epoch: 0591 loss_train: 0.1756 acc_train: 1.0000 loss_val: 1.7086 acc_val: 0.5778 time: 0.0076s\n",
      "Epoch: 0592 loss_train: 0.2575 acc_train: 0.8000 loss_val: 1.7295 acc_val: 0.5889 time: 0.0080s\n",
      "Epoch: 0593 loss_train: 0.1400 acc_train: 0.9000 loss_val: 1.7092 acc_val: 0.5778 time: 0.0093s\n",
      "Epoch: 0594 loss_train: 0.1817 acc_train: 0.9000 loss_val: 1.6942 acc_val: 0.5778 time: 0.0097s\n",
      "Epoch: 0595 loss_train: 0.0862 acc_train: 0.9000 loss_val: 1.6807 acc_val: 0.6000 time: 0.0087s\n",
      "Epoch: 0596 loss_train: 0.3041 acc_train: 0.8000 loss_val: 1.6694 acc_val: 0.6111 time: 0.0085s\n",
      "Epoch: 0597 loss_train: 0.3076 acc_train: 0.9000 loss_val: 1.6586 acc_val: 0.6111 time: 0.0069s\n",
      "Epoch: 0598 loss_train: 0.1116 acc_train: 0.9000 loss_val: 1.6517 acc_val: 0.6111 time: 0.0120s\n",
      "Epoch: 0599 loss_train: 0.1346 acc_train: 0.9000 loss_val: 1.6485 acc_val: 0.6111 time: 0.0131s\n",
      "Epoch: 0600 loss_train: 0.1931 acc_train: 0.9000 loss_val: 1.6505 acc_val: 0.6111 time: 0.0070s\n",
      "Epoch: 0601 loss_train: 0.2132 acc_train: 0.9000 loss_val: 1.6506 acc_val: 0.6000 time: 0.0130s\n",
      "Epoch: 0602 loss_train: 0.2456 acc_train: 0.9000 loss_val: 1.6509 acc_val: 0.6000 time: 0.0131s\n",
      "Epoch: 0603 loss_train: 0.3051 acc_train: 0.8000 loss_val: 1.6524 acc_val: 0.6000 time: 0.0134s\n",
      "Epoch: 0604 loss_train: 0.2520 acc_train: 0.9000 loss_val: 1.6603 acc_val: 0.5778 time: 0.0080s\n",
      "Epoch: 0605 loss_train: 0.1859 acc_train: 0.8000 loss_val: 1.6676 acc_val: 0.5778 time: 0.0149s\n",
      "Epoch: 0606 loss_train: 0.1722 acc_train: 0.9000 loss_val: 1.6798 acc_val: 0.5778 time: 0.0113s\n",
      "Epoch: 0607 loss_train: 0.2338 acc_train: 0.9000 loss_val: 1.6920 acc_val: 0.5778 time: 0.0070s\n",
      "Epoch: 0608 loss_train: 0.0371 acc_train: 1.0000 loss_val: 1.7047 acc_val: 0.6000 time: 0.0119s\n",
      "Epoch: 0609 loss_train: 0.1692 acc_train: 0.8000 loss_val: 1.7150 acc_val: 0.6111 time: 0.0103s\n",
      "Epoch: 0610 loss_train: 0.2404 acc_train: 0.9000 loss_val: 1.7226 acc_val: 0.6111 time: 0.0097s\n",
      "Epoch: 0611 loss_train: 0.1544 acc_train: 0.9000 loss_val: 1.7285 acc_val: 0.6111 time: 0.0105s\n",
      "Epoch: 0612 loss_train: 0.2354 acc_train: 0.9000 loss_val: 1.7271 acc_val: 0.6111 time: 0.0056s\n",
      "Epoch: 0613 loss_train: 0.1410 acc_train: 0.9000 loss_val: 1.7386 acc_val: 0.6111 time: 0.0125s\n",
      "Epoch: 0614 loss_train: 0.2000 acc_train: 0.8000 loss_val: 1.7695 acc_val: 0.5778 time: 0.0102s\n",
      "Epoch: 0615 loss_train: 0.1932 acc_train: 0.9000 loss_val: 1.8116 acc_val: 0.5889 time: 0.0079s\n",
      "Epoch: 0616 loss_train: 0.2409 acc_train: 0.8000 loss_val: 1.8349 acc_val: 0.5778 time: 0.0123s\n",
      "Epoch: 0617 loss_train: 0.2838 acc_train: 0.9000 loss_val: 1.8266 acc_val: 0.5778 time: 0.0069s\n",
      "Epoch: 0618 loss_train: 0.3798 acc_train: 0.7000 loss_val: 1.8005 acc_val: 0.5778 time: 0.0111s\n",
      "Epoch: 0619 loss_train: 0.2019 acc_train: 0.9000 loss_val: 1.7717 acc_val: 0.6000 time: 0.0093s\n",
      "Epoch: 0620 loss_train: 0.1346 acc_train: 0.9000 loss_val: 1.7609 acc_val: 0.6111 time: 0.0077s\n",
      "Epoch: 0621 loss_train: 0.1133 acc_train: 1.0000 loss_val: 1.7561 acc_val: 0.6111 time: 0.0114s\n",
      "Epoch: 0622 loss_train: 0.3174 acc_train: 0.8000 loss_val: 1.7517 acc_val: 0.6111 time: 0.0056s\n",
      "Epoch: 0623 loss_train: 0.2851 acc_train: 0.9000 loss_val: 1.7441 acc_val: 0.6111 time: 0.0093s\n",
      "Epoch: 0624 loss_train: 0.2666 acc_train: 0.8000 loss_val: 1.7389 acc_val: 0.6111 time: 0.0082s\n",
      "Epoch: 0625 loss_train: 0.2652 acc_train: 0.8000 loss_val: 1.7348 acc_val: 0.5778 time: 0.0127s\n",
      "Epoch: 0626 loss_train: 0.2114 acc_train: 0.9000 loss_val: 1.7319 acc_val: 0.5778 time: 0.0103s\n",
      "Epoch: 0627 loss_train: 0.2192 acc_train: 1.0000 loss_val: 1.7319 acc_val: 0.5778 time: 0.0063s\n",
      "Epoch: 0628 loss_train: 0.1985 acc_train: 0.9000 loss_val: 1.7324 acc_val: 0.5778 time: 0.0095s\n",
      "Epoch: 0629 loss_train: 0.2592 acc_train: 0.8000 loss_val: 1.7345 acc_val: 0.5778 time: 0.0062s\n",
      "Epoch: 0630 loss_train: 0.2794 acc_train: 0.8000 loss_val: 1.7342 acc_val: 0.6111 time: 0.0069s\n",
      "Epoch: 0631 loss_train: 0.1723 acc_train: 0.9000 loss_val: 1.7503 acc_val: 0.6111 time: 0.0064s\n",
      "Epoch: 0632 loss_train: 0.1268 acc_train: 0.9000 loss_val: 1.7675 acc_val: 0.6000 time: 0.0063s\n",
      "Epoch: 0633 loss_train: 0.3623 acc_train: 0.6000 loss_val: 1.7806 acc_val: 0.6000 time: 0.0064s\n",
      "Epoch: 0634 loss_train: 0.1818 acc_train: 0.9000 loss_val: 1.7927 acc_val: 0.6000 time: 0.0063s\n",
      "Epoch: 0635 loss_train: 0.2511 acc_train: 0.8000 loss_val: 1.8025 acc_val: 0.5667 time: 0.0064s\n",
      "Epoch: 0636 loss_train: 0.2422 acc_train: 0.8000 loss_val: 1.8077 acc_val: 0.5667 time: 0.0064s\n",
      "Epoch: 0637 loss_train: 0.2834 acc_train: 0.9000 loss_val: 1.8099 acc_val: 0.5667 time: 0.0064s\n",
      "Epoch: 0638 loss_train: 0.2023 acc_train: 0.9000 loss_val: 1.8130 acc_val: 0.5778 time: 0.0063s\n",
      "Epoch: 0639 loss_train: 0.1679 acc_train: 1.0000 loss_val: 1.8185 acc_val: 0.5778 time: 0.0062s\n",
      "Epoch: 0640 loss_train: 0.1817 acc_train: 0.9000 loss_val: 1.8219 acc_val: 0.5778 time: 0.0062s\n",
      "Epoch: 0641 loss_train: 0.1091 acc_train: 0.9000 loss_val: 1.8211 acc_val: 0.5778 time: 0.0062s\n",
      "Epoch: 0642 loss_train: 0.2798 acc_train: 0.8000 loss_val: 1.8221 acc_val: 0.5667 time: 0.0072s\n",
      "Epoch: 0643 loss_train: 0.1258 acc_train: 1.0000 loss_val: 1.8259 acc_val: 0.6111 time: 0.0070s\n",
      "Epoch: 0644 loss_train: 0.1334 acc_train: 0.9000 loss_val: 1.8305 acc_val: 0.6111 time: 0.0070s\n",
      "Epoch: 0645 loss_train: 0.2438 acc_train: 0.7000 loss_val: 1.8338 acc_val: 0.6111 time: 0.0071s\n",
      "Epoch: 0646 loss_train: 0.1701 acc_train: 0.9000 loss_val: 1.8341 acc_val: 0.6111 time: 0.0070s\n",
      "Epoch: 0647 loss_train: 0.3081 acc_train: 0.7000 loss_val: 1.8319 acc_val: 0.6111 time: 0.0070s\n",
      "Epoch: 0648 loss_train: 0.1797 acc_train: 0.9000 loss_val: 1.8295 acc_val: 0.6111 time: 0.0070s\n",
      "Epoch: 0649 loss_train: 0.2868 acc_train: 0.8000 loss_val: 1.8287 acc_val: 0.6111 time: 0.0070s\n",
      "Epoch: 0650 loss_train: 0.2141 acc_train: 0.9000 loss_val: 1.8284 acc_val: 0.6111 time: 0.0068s\n",
      "Epoch: 0651 loss_train: 0.1166 acc_train: 0.9000 loss_val: 1.8323 acc_val: 0.5778 time: 0.0082s\n",
      "Epoch: 0652 loss_train: 0.1558 acc_train: 0.9000 loss_val: 1.8319 acc_val: 0.5778 time: 0.0098s\n",
      "Epoch: 0653 loss_train: 0.2557 acc_train: 0.8000 loss_val: 1.8321 acc_val: 0.5778 time: 0.0065s\n",
      "Epoch: 0654 loss_train: 0.2118 acc_train: 0.9000 loss_val: 1.8315 acc_val: 0.5778 time: 0.0086s\n",
      "Epoch: 0655 loss_train: 0.2100 acc_train: 0.9000 loss_val: 1.8296 acc_val: 0.5778 time: 0.0057s\n",
      "Epoch: 0656 loss_train: 0.2585 acc_train: 0.9000 loss_val: 1.8292 acc_val: 0.5778 time: 0.0059s\n",
      "Epoch: 0657 loss_train: 0.2363 acc_train: 0.8000 loss_val: 1.8330 acc_val: 0.6111 time: 0.0058s\n",
      "Epoch: 0658 loss_train: 0.2458 acc_train: 0.8000 loss_val: 1.8444 acc_val: 0.6111 time: 0.0057s\n",
      "Epoch: 0659 loss_train: 0.2469 acc_train: 1.0000 loss_val: 1.8553 acc_val: 0.6111 time: 0.0057s\n",
      "Epoch: 0660 loss_train: 0.2596 acc_train: 0.9000 loss_val: 1.8637 acc_val: 0.6111 time: 0.0057s\n",
      "Epoch: 0661 loss_train: 0.0845 acc_train: 1.0000 loss_val: 1.8635 acc_val: 0.6111 time: 0.0057s\n",
      "Epoch: 0662 loss_train: 0.3343 acc_train: 0.8000 loss_val: 1.8569 acc_val: 0.6111 time: 0.0056s\n",
      "Epoch: 0663 loss_train: 0.2245 acc_train: 0.8000 loss_val: 1.8502 acc_val: 0.6111 time: 0.0072s\n",
      "Epoch: 0664 loss_train: 0.1736 acc_train: 1.0000 loss_val: 1.8450 acc_val: 0.6000 time: 0.0065s\n",
      "Epoch: 0665 loss_train: 0.1747 acc_train: 0.9000 loss_val: 1.8418 acc_val: 0.5667 time: 0.0056s\n",
      "Epoch: 0666 loss_train: 0.2798 acc_train: 1.0000 loss_val: 1.8393 acc_val: 0.5778 time: 0.0078s\n",
      "Epoch: 0667 loss_train: 0.3719 acc_train: 0.6000 loss_val: 1.8356 acc_val: 0.5778 time: 0.0071s\n",
      "Epoch: 0668 loss_train: 0.3696 acc_train: 0.7000 loss_val: 1.8305 acc_val: 0.5778 time: 0.0070s\n",
      "Epoch: 0669 loss_train: 0.2611 acc_train: 0.9000 loss_val: 1.8359 acc_val: 0.5778 time: 0.0106s\n",
      "Epoch: 0670 loss_train: 0.1690 acc_train: 0.8000 loss_val: 1.8387 acc_val: 0.5778 time: 0.0095s\n",
      "Epoch: 0671 loss_train: 0.2656 acc_train: 0.8000 loss_val: 1.8234 acc_val: 0.5778 time: 0.0066s\n",
      "Epoch: 0672 loss_train: 0.2669 acc_train: 0.9000 loss_val: 1.7981 acc_val: 0.5778 time: 0.0063s\n",
      "Epoch: 0673 loss_train: 0.3091 acc_train: 0.8000 loss_val: 1.7817 acc_val: 0.6111 time: 0.0113s\n",
      "Epoch: 0674 loss_train: 0.3552 acc_train: 0.8000 loss_val: 1.7696 acc_val: 0.6111 time: 0.0063s\n",
      "Epoch: 0675 loss_train: 0.0494 acc_train: 1.0000 loss_val: 1.7541 acc_val: 0.6111 time: 0.0138s\n",
      "Epoch: 0676 loss_train: 0.1902 acc_train: 0.9000 loss_val: 1.7402 acc_val: 0.6111 time: 0.0111s\n",
      "Epoch: 0677 loss_train: 0.2366 acc_train: 0.8000 loss_val: 1.7186 acc_val: 0.5778 time: 0.0071s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0678 loss_train: 0.3188 acc_train: 0.8000 loss_val: 1.6957 acc_val: 0.5778 time: 0.0155s\n",
      "Epoch: 0679 loss_train: 0.2225 acc_train: 0.8000 loss_val: 1.6862 acc_val: 0.5778 time: 0.0142s\n",
      "Epoch: 0680 loss_train: 0.2254 acc_train: 0.9000 loss_val: 1.6826 acc_val: 0.5778 time: 0.0137s\n",
      "Epoch: 0681 loss_train: 0.2905 acc_train: 0.9000 loss_val: 1.6711 acc_val: 0.5778 time: 0.0114s\n",
      "Epoch: 0682 loss_train: 0.4520 acc_train: 0.7000 loss_val: 1.6580 acc_val: 0.5778 time: 0.0069s\n",
      "Epoch: 0683 loss_train: 0.1609 acc_train: 0.9000 loss_val: 1.6219 acc_val: 0.6000 time: 0.0107s\n",
      "Epoch: 0684 loss_train: 0.1381 acc_train: 0.9000 loss_val: 1.6006 acc_val: 0.5778 time: 0.0069s\n",
      "Epoch: 0685 loss_train: 0.3511 acc_train: 0.8000 loss_val: 1.5769 acc_val: 0.5889 time: 0.0110s\n",
      "Epoch: 0686 loss_train: 0.1282 acc_train: 1.0000 loss_val: 1.5554 acc_val: 0.6111 time: 0.0085s\n",
      "Epoch: 0687 loss_train: 0.2556 acc_train: 0.9000 loss_val: 1.5466 acc_val: 0.6111 time: 0.0079s\n",
      "Epoch: 0688 loss_train: 0.2413 acc_train: 1.0000 loss_val: 1.5434 acc_val: 0.6111 time: 0.0128s\n",
      "Epoch: 0689 loss_train: 0.2456 acc_train: 0.9000 loss_val: 1.5471 acc_val: 0.5778 time: 0.0076s\n",
      "Epoch: 0690 loss_train: 0.2212 acc_train: 0.9000 loss_val: 1.5573 acc_val: 0.5889 time: 0.0160s\n",
      "Epoch: 0691 loss_train: 0.2633 acc_train: 1.0000 loss_val: 1.5767 acc_val: 0.5778 time: 0.0119s\n",
      "Epoch: 0692 loss_train: 0.1648 acc_train: 1.0000 loss_val: 1.5850 acc_val: 0.5778 time: 0.0086s\n",
      "Epoch: 0693 loss_train: 0.2469 acc_train: 0.9000 loss_val: 1.5910 acc_val: 0.5778 time: 0.0071s\n",
      "Epoch: 0694 loss_train: 0.2404 acc_train: 0.9000 loss_val: 1.5491 acc_val: 0.6000 time: 0.0090s\n",
      "Epoch: 0695 loss_train: 0.4374 acc_train: 0.8000 loss_val: 1.5162 acc_val: 0.5889 time: 0.0085s\n",
      "Epoch: 0696 loss_train: 0.3448 acc_train: 0.7000 loss_val: 1.4919 acc_val: 0.6000 time: 0.0123s\n",
      "Epoch: 0697 loss_train: 0.2535 acc_train: 0.8000 loss_val: 1.4797 acc_val: 0.6111 time: 0.0057s\n",
      "Epoch: 0698 loss_train: 0.3004 acc_train: 0.8000 loss_val: 1.4742 acc_val: 0.6111 time: 0.0152s\n",
      "Epoch: 0699 loss_train: 0.2953 acc_train: 1.0000 loss_val: 1.4695 acc_val: 0.6111 time: 0.0153s\n",
      "Epoch: 0700 loss_train: 0.0731 acc_train: 1.0000 loss_val: 1.4580 acc_val: 0.6000 time: 0.0119s\n",
      "Epoch: 0701 loss_train: 0.1118 acc_train: 1.0000 loss_val: 1.4437 acc_val: 0.5778 time: 0.0070s\n",
      "Epoch: 0702 loss_train: 0.2180 acc_train: 0.8000 loss_val: 1.4421 acc_val: 0.5778 time: 0.0095s\n",
      "Epoch: 0703 loss_train: 0.1629 acc_train: 0.9000 loss_val: 1.4289 acc_val: 0.5778 time: 0.0063s\n",
      "Epoch: 0704 loss_train: 0.2979 acc_train: 0.8000 loss_val: 1.4137 acc_val: 0.5889 time: 0.0064s\n",
      "Epoch: 0705 loss_train: 0.3759 acc_train: 0.7000 loss_val: 1.4043 acc_val: 0.5889 time: 0.0063s\n",
      "Epoch: 0706 loss_train: 0.1918 acc_train: 0.9000 loss_val: 1.3800 acc_val: 0.5778 time: 0.0064s\n",
      "Epoch: 0707 loss_train: 0.3767 acc_train: 0.8000 loss_val: 1.3654 acc_val: 0.5778 time: 0.0066s\n",
      "Epoch: 0708 loss_train: 0.4224 acc_train: 0.7000 loss_val: 1.3528 acc_val: 0.5778 time: 0.0062s\n",
      "Epoch: 0709 loss_train: 0.2451 acc_train: 0.9000 loss_val: 1.3456 acc_val: 0.5889 time: 0.0064s\n",
      "Epoch: 0710 loss_train: 0.2514 acc_train: 0.8000 loss_val: 1.3354 acc_val: 0.6111 time: 0.0063s\n",
      "Epoch: 0711 loss_train: 0.2165 acc_train: 0.8000 loss_val: 1.3308 acc_val: 0.6111 time: 0.0062s\n",
      "Epoch: 0712 loss_train: 0.3340 acc_train: 0.8000 loss_val: 1.3391 acc_val: 0.6222 time: 0.0061s\n",
      "Epoch: 0713 loss_train: 0.1643 acc_train: 1.0000 loss_val: 1.3539 acc_val: 0.6222 time: 0.0062s\n",
      "Epoch: 0714 loss_train: 0.2497 acc_train: 0.9000 loss_val: 1.3669 acc_val: 0.6111 time: 0.0062s\n",
      "Epoch: 0715 loss_train: 0.2379 acc_train: 0.8000 loss_val: 1.3704 acc_val: 0.6111 time: 0.0062s\n",
      "Epoch: 0716 loss_train: 0.2508 acc_train: 0.9000 loss_val: 1.3672 acc_val: 0.6111 time: 0.0061s\n",
      "Epoch: 0717 loss_train: 0.3478 acc_train: 0.7000 loss_val: 1.3652 acc_val: 0.6111 time: 0.0062s\n",
      "Epoch: 0718 loss_train: 0.2935 acc_train: 0.8000 loss_val: 1.3669 acc_val: 0.6111 time: 0.0061s\n",
      "Epoch: 0719 loss_train: 0.2306 acc_train: 0.9000 loss_val: 1.3709 acc_val: 0.6111 time: 0.0064s\n",
      "Epoch: 0720 loss_train: 0.2784 acc_train: 0.8000 loss_val: 1.3781 acc_val: 0.6111 time: 0.0064s\n",
      "Epoch: 0721 loss_train: 0.1824 acc_train: 1.0000 loss_val: 1.3849 acc_val: 0.5778 time: 0.0063s\n",
      "Epoch: 0722 loss_train: 0.3682 acc_train: 0.7000 loss_val: 1.3833 acc_val: 0.6111 time: 0.0064s\n",
      "Epoch: 0723 loss_train: 0.1637 acc_train: 0.9000 loss_val: 1.3818 acc_val: 0.6111 time: 0.0064s\n",
      "Epoch: 0724 loss_train: 0.3151 acc_train: 0.8000 loss_val: 1.3818 acc_val: 0.6111 time: 0.0064s\n",
      "Epoch: 0725 loss_train: 0.1231 acc_train: 1.0000 loss_val: 1.3885 acc_val: 0.6111 time: 0.0064s\n",
      "Epoch: 0726 loss_train: 0.3382 acc_train: 0.7000 loss_val: 1.4032 acc_val: 0.5778 time: 0.0075s\n",
      "Epoch: 0727 loss_train: 0.2943 acc_train: 0.7000 loss_val: 1.4043 acc_val: 0.5889 time: 0.0077s\n",
      "Epoch: 0728 loss_train: 0.2421 acc_train: 1.0000 loss_val: 1.3890 acc_val: 0.6111 time: 0.0069s\n",
      "Epoch: 0729 loss_train: 0.2207 acc_train: 0.8000 loss_val: 1.3739 acc_val: 0.6111 time: 0.0130s\n",
      "Epoch: 0730 loss_train: 0.0737 acc_train: 1.0000 loss_val: 1.3783 acc_val: 0.6111 time: 0.0108s\n",
      "Epoch: 0731 loss_train: 0.0996 acc_train: 1.0000 loss_val: 1.3966 acc_val: 0.6111 time: 0.0072s\n",
      "Epoch: 0732 loss_train: 0.2232 acc_train: 0.8000 loss_val: 1.4157 acc_val: 0.6111 time: 0.0089s\n",
      "Epoch: 0733 loss_train: 0.1454 acc_train: 0.9000 loss_val: 1.4343 acc_val: 0.6111 time: 0.0060s\n",
      "Epoch: 0734 loss_train: 0.2448 acc_train: 0.9000 loss_val: 1.4541 acc_val: 0.6000 time: 0.0126s\n",
      "Epoch: 0735 loss_train: 0.3532 acc_train: 0.8000 loss_val: 1.4667 acc_val: 0.5778 time: 0.0065s\n",
      "Epoch: 0736 loss_train: 0.2660 acc_train: 0.8000 loss_val: 1.4747 acc_val: 0.5778 time: 0.0129s\n",
      "Epoch: 0737 loss_train: 0.1630 acc_train: 0.9000 loss_val: 1.4874 acc_val: 0.5778 time: 0.0114s\n",
      "Epoch: 0738 loss_train: 0.1906 acc_train: 0.9000 loss_val: 1.5152 acc_val: 0.5889 time: 0.0076s\n",
      "Epoch: 0739 loss_train: 0.2622 acc_train: 0.8000 loss_val: 1.5232 acc_val: 0.5889 time: 0.0159s\n",
      "Epoch: 0740 loss_train: 0.0851 acc_train: 1.0000 loss_val: 1.5179 acc_val: 0.5889 time: 0.0140s\n",
      "Epoch: 0741 loss_train: 0.3581 acc_train: 0.7000 loss_val: 1.5120 acc_val: 0.5889 time: 0.0115s\n",
      "Epoch: 0742 loss_train: 0.2336 acc_train: 0.9000 loss_val: 1.5037 acc_val: 0.5778 time: 0.0072s\n",
      "Epoch: 0743 loss_train: 0.2677 acc_train: 0.7000 loss_val: 1.4984 acc_val: 0.5778 time: 0.0120s\n",
      "Epoch: 0744 loss_train: 0.3547 acc_train: 0.7000 loss_val: 1.4822 acc_val: 0.5778 time: 0.0127s\n",
      "Epoch: 0745 loss_train: 0.4185 acc_train: 0.8000 loss_val: 1.4668 acc_val: 0.5889 time: 0.0072s\n",
      "Epoch: 0746 loss_train: 0.2869 acc_train: 0.7000 loss_val: 1.4434 acc_val: 0.6111 time: 0.0131s\n",
      "Epoch: 0747 loss_train: 0.2182 acc_train: 0.9000 loss_val: 1.4264 acc_val: 0.6111 time: 0.0103s\n",
      "Epoch: 0748 loss_train: 0.1845 acc_train: 0.9000 loss_val: 1.4103 acc_val: 0.6111 time: 0.0125s\n",
      "Epoch: 0749 loss_train: 0.2519 acc_train: 0.8000 loss_val: 1.4004 acc_val: 0.6111 time: 0.0067s\n",
      "Epoch: 0750 loss_train: 0.4129 acc_train: 0.6000 loss_val: 1.3876 acc_val: 0.6111 time: 0.0120s\n",
      "Epoch: 0751 loss_train: 0.3253 acc_train: 0.8000 loss_val: 1.3792 acc_val: 0.6111 time: 0.0078s\n",
      "Epoch: 0752 loss_train: 0.3048 acc_train: 0.8000 loss_val: 1.3573 acc_val: 0.6111 time: 0.0116s\n",
      "Epoch: 0753 loss_train: 0.1928 acc_train: 0.9000 loss_val: 1.3394 acc_val: 0.6111 time: 0.0112s\n",
      "Epoch: 0754 loss_train: 0.2095 acc_train: 0.8000 loss_val: 1.3257 acc_val: 0.6111 time: 0.0066s\n",
      "Epoch: 0755 loss_train: 0.1329 acc_train: 1.0000 loss_val: 1.3217 acc_val: 0.6111 time: 0.0124s\n",
      "Epoch: 0756 loss_train: 0.2320 acc_train: 0.9000 loss_val: 1.3271 acc_val: 0.6111 time: 0.0070s\n",
      "Epoch: 0757 loss_train: 0.2725 acc_train: 0.9000 loss_val: 1.3369 acc_val: 0.6111 time: 0.0123s\n",
      "Epoch: 0758 loss_train: 0.1550 acc_train: 0.9000 loss_val: 1.3593 acc_val: 0.6111 time: 0.0118s\n",
      "Epoch: 0759 loss_train: 0.1523 acc_train: 1.0000 loss_val: 1.3881 acc_val: 0.5778 time: 0.0069s\n",
      "Epoch: 0760 loss_train: 0.2508 acc_train: 1.0000 loss_val: 1.4199 acc_val: 0.5778 time: 0.0123s\n",
      "Epoch: 0761 loss_train: 0.2382 acc_train: 0.8000 loss_val: 1.4431 acc_val: 0.5778 time: 0.0115s\n",
      "Epoch: 0762 loss_train: 0.1853 acc_train: 1.0000 loss_val: 1.4373 acc_val: 0.5778 time: 0.0062s\n",
      "Epoch: 0763 loss_train: 0.3173 acc_train: 0.9000 loss_val: 1.4343 acc_val: 0.5778 time: 0.0121s\n",
      "Epoch: 0764 loss_train: 0.2892 acc_train: 0.9000 loss_val: 1.4413 acc_val: 0.5778 time: 0.0057s\n",
      "Epoch: 0765 loss_train: 0.1311 acc_train: 1.0000 loss_val: 1.4531 acc_val: 0.5778 time: 0.0105s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0766 loss_train: 0.4119 acc_train: 0.8000 loss_val: 1.4562 acc_val: 0.5778 time: 0.0073s\n",
      "Epoch: 0767 loss_train: 0.2085 acc_train: 0.8000 loss_val: 1.4610 acc_val: 0.6111 time: 0.0127s\n",
      "Epoch: 0768 loss_train: 0.1580 acc_train: 1.0000 loss_val: 1.4672 acc_val: 0.6111 time: 0.0104s\n",
      "Epoch: 0769 loss_train: 0.2764 acc_train: 0.9000 loss_val: 1.4711 acc_val: 0.5778 time: 0.0064s\n",
      "Epoch: 0770 loss_train: 0.2296 acc_train: 0.8000 loss_val: 1.4776 acc_val: 0.5778 time: 0.0094s\n",
      "Epoch: 0771 loss_train: 0.4842 acc_train: 0.5000 loss_val: 1.4790 acc_val: 0.5778 time: 0.0061s\n",
      "Epoch: 0772 loss_train: 0.2891 acc_train: 0.9000 loss_val: 1.4654 acc_val: 0.5778 time: 0.0062s\n",
      "Epoch: 0773 loss_train: 0.2706 acc_train: 0.9000 loss_val: 1.4553 acc_val: 0.5778 time: 0.0063s\n",
      "Epoch: 0774 loss_train: 0.2914 acc_train: 0.7000 loss_val: 1.4475 acc_val: 0.6111 time: 0.0062s\n",
      "Epoch: 0775 loss_train: 0.2659 acc_train: 0.9000 loss_val: 1.4516 acc_val: 0.5778 time: 0.0063s\n",
      "Epoch: 0776 loss_train: 0.2318 acc_train: 1.0000 loss_val: 1.4559 acc_val: 0.5778 time: 0.0063s\n",
      "Epoch: 0777 loss_train: 0.1787 acc_train: 1.0000 loss_val: 1.4488 acc_val: 0.5778 time: 0.0062s\n",
      "Epoch: 0778 loss_train: 0.3711 acc_train: 0.8000 loss_val: 1.4428 acc_val: 0.5778 time: 0.0069s\n",
      "Epoch: 0779 loss_train: 0.2908 acc_train: 0.8000 loss_val: 1.4220 acc_val: 0.5778 time: 0.0076s\n",
      "Epoch: 0780 loss_train: 0.4524 acc_train: 0.8000 loss_val: 1.3892 acc_val: 0.5778 time: 0.0071s\n",
      "Epoch: 0781 loss_train: 0.2872 acc_train: 0.8000 loss_val: 1.3589 acc_val: 0.5778 time: 0.0078s\n",
      "Epoch: 0782 loss_train: 0.4423 acc_train: 0.7000 loss_val: 1.3334 acc_val: 0.6111 time: 0.0070s\n",
      "Epoch: 0783 loss_train: 0.3070 acc_train: 0.9000 loss_val: 1.3196 acc_val: 0.6111 time: 0.0074s\n",
      "Epoch: 0784 loss_train: 0.1415 acc_train: 0.9000 loss_val: 1.3120 acc_val: 0.5889 time: 0.0103s\n",
      "Epoch: 0785 loss_train: 0.3111 acc_train: 0.9000 loss_val: 1.2789 acc_val: 0.6111 time: 0.0055s\n",
      "Epoch: 0786 loss_train: 0.3194 acc_train: 0.9000 loss_val: 1.2570 acc_val: 0.6111 time: 0.0104s\n",
      "Epoch: 0787 loss_train: 0.1211 acc_train: 0.9000 loss_val: 1.2543 acc_val: 0.6111 time: 0.0064s\n",
      "Epoch: 0788 loss_train: 0.1928 acc_train: 1.0000 loss_val: 1.2530 acc_val: 0.6222 time: 0.0104s\n",
      "Epoch: 0789 loss_train: 0.1130 acc_train: 0.9000 loss_val: 1.2647 acc_val: 0.6222 time: 0.0096s\n",
      "Epoch: 0790 loss_train: 0.2093 acc_train: 1.0000 loss_val: 1.2726 acc_val: 0.6111 time: 0.0095s\n",
      "Epoch: 0791 loss_train: 0.1475 acc_train: 1.0000 loss_val: 1.2840 acc_val: 0.6111 time: 0.0137s\n",
      "Epoch: 0792 loss_train: 0.3591 acc_train: 0.8000 loss_val: 1.2810 acc_val: 0.6111 time: 0.0066s\n",
      "Epoch: 0793 loss_train: 0.3987 acc_train: 0.8000 loss_val: 1.2752 acc_val: 0.5778 time: 0.0142s\n",
      "Epoch: 0794 loss_train: 0.3095 acc_train: 0.9000 loss_val: 1.2653 acc_val: 0.5778 time: 0.0146s\n",
      "Epoch: 0795 loss_train: 0.2530 acc_train: 0.9000 loss_val: 1.2170 acc_val: 0.5889 time: 0.0122s\n",
      "Epoch: 0796 loss_train: 0.1509 acc_train: 0.9000 loss_val: 1.1857 acc_val: 0.6111 time: 0.0071s\n",
      "Epoch: 0797 loss_train: 0.1186 acc_train: 1.0000 loss_val: 1.1679 acc_val: 0.6111 time: 0.0138s\n",
      "Epoch: 0798 loss_train: 0.2000 acc_train: 1.0000 loss_val: 1.1430 acc_val: 0.6111 time: 0.0140s\n",
      "Epoch: 0799 loss_train: 0.3288 acc_train: 0.8000 loss_val: 1.1133 acc_val: 0.6111 time: 0.0108s\n",
      "Epoch: 0800 loss_train: 0.3719 acc_train: 1.0000 loss_val: 1.0859 acc_val: 0.6111 time: 0.0069s\n",
      "Epoch: 0801 loss_train: 0.2821 acc_train: 0.8000 loss_val: 1.0684 acc_val: 0.6111 time: 0.0092s\n",
      "Epoch: 0802 loss_train: 0.3507 acc_train: 0.8000 loss_val: 1.0601 acc_val: 0.6111 time: 0.0062s\n",
      "Epoch: 0803 loss_train: 0.2676 acc_train: 0.9000 loss_val: 1.0609 acc_val: 0.6111 time: 0.0097s\n",
      "Epoch: 0804 loss_train: 0.2556 acc_train: 0.8000 loss_val: 1.0744 acc_val: 0.6111 time: 0.0061s\n",
      "Epoch: 0805 loss_train: 0.1846 acc_train: 1.0000 loss_val: 1.0896 acc_val: 0.6111 time: 0.0088s\n",
      "Epoch: 0806 loss_train: 0.3510 acc_train: 0.7000 loss_val: 1.1005 acc_val: 0.6111 time: 0.0078s\n",
      "Epoch: 0807 loss_train: 0.3806 acc_train: 0.7000 loss_val: 1.1136 acc_val: 0.6111 time: 0.0115s\n",
      "Epoch: 0808 loss_train: 0.2721 acc_train: 0.9000 loss_val: 1.1346 acc_val: 0.6111 time: 0.0064s\n",
      "Epoch: 0809 loss_train: 0.2102 acc_train: 0.9000 loss_val: 1.1646 acc_val: 0.6111 time: 0.0109s\n",
      "Epoch: 0810 loss_train: 0.3919 acc_train: 0.7000 loss_val: 1.1587 acc_val: 0.6111 time: 0.0062s\n",
      "Epoch: 0811 loss_train: 0.4679 acc_train: 0.6000 loss_val: 1.1625 acc_val: 0.6111 time: 0.0107s\n",
      "Epoch: 0812 loss_train: 0.4331 acc_train: 0.7000 loss_val: 1.1548 acc_val: 0.5778 time: 0.0094s\n",
      "Epoch: 0813 loss_train: 0.3115 acc_train: 0.8000 loss_val: 1.1937 acc_val: 0.5778 time: 0.0101s\n",
      "Epoch: 0814 loss_train: 0.2453 acc_train: 0.9000 loss_val: 1.2293 acc_val: 0.5556 time: 0.0123s\n",
      "Epoch: 0815 loss_train: 0.3608 acc_train: 0.8000 loss_val: 1.2769 acc_val: 0.5333 time: 0.0104s\n",
      "Epoch: 0816 loss_train: 0.2338 acc_train: 0.9000 loss_val: 1.2997 acc_val: 0.5333 time: 0.0068s\n",
      "Epoch: 0817 loss_train: 0.2437 acc_train: 0.9000 loss_val: 1.2794 acc_val: 0.5556 time: 0.0072s\n",
      "Epoch: 0818 loss_train: 0.2195 acc_train: 0.9000 loss_val: 1.2568 acc_val: 0.5889 time: 0.0070s\n",
      "Epoch: 0819 loss_train: 0.1693 acc_train: 1.0000 loss_val: 1.2354 acc_val: 0.5778 time: 0.0066s\n",
      "Epoch: 0820 loss_train: 0.2132 acc_train: 0.9000 loss_val: 1.2165 acc_val: 0.5778 time: 0.0070s\n",
      "Epoch: 0821 loss_train: 0.1495 acc_train: 0.9000 loss_val: 1.2045 acc_val: 0.6111 time: 0.0069s\n",
      "Epoch: 0822 loss_train: 0.2944 acc_train: 1.0000 loss_val: 1.2004 acc_val: 0.6111 time: 0.0070s\n",
      "Epoch: 0823 loss_train: 0.3494 acc_train: 0.8000 loss_val: 1.1998 acc_val: 0.6111 time: 0.0065s\n",
      "Epoch: 0824 loss_train: 0.0922 acc_train: 1.0000 loss_val: 1.1990 acc_val: 0.6111 time: 0.0065s\n",
      "Epoch: 0825 loss_train: 0.3973 acc_train: 0.7000 loss_val: 1.1942 acc_val: 0.6111 time: 0.0062s\n",
      "Epoch: 0826 loss_train: 0.3903 acc_train: 0.7000 loss_val: 1.1810 acc_val: 0.6111 time: 0.0062s\n",
      "Epoch: 0827 loss_train: 0.4111 acc_train: 0.8000 loss_val: 1.1610 acc_val: 0.6111 time: 0.0069s\n",
      "Epoch: 0828 loss_train: 0.2449 acc_train: 0.8000 loss_val: 1.1402 acc_val: 0.6111 time: 0.0072s\n",
      "Epoch: 0829 loss_train: 0.3189 acc_train: 0.8000 loss_val: 1.1157 acc_val: 0.6111 time: 0.0072s\n",
      "Epoch: 0830 loss_train: 0.3339 acc_train: 0.9000 loss_val: 1.1068 acc_val: 0.5889 time: 0.0079s\n",
      "Epoch: 0831 loss_train: 0.1720 acc_train: 0.8000 loss_val: 1.1079 acc_val: 0.5889 time: 0.0071s\n",
      "Epoch: 0832 loss_train: 0.2872 acc_train: 0.8000 loss_val: 1.1131 acc_val: 0.5889 time: 0.0080s\n",
      "Epoch: 0833 loss_train: 0.3504 acc_train: 0.7000 loss_val: 1.1206 acc_val: 0.5889 time: 0.0076s\n",
      "Epoch: 0834 loss_train: 0.3040 acc_train: 0.8000 loss_val: 1.1381 acc_val: 0.5889 time: 0.0091s\n",
      "Epoch: 0835 loss_train: 0.2838 acc_train: 0.7000 loss_val: 1.1769 acc_val: 0.6111 time: 0.0065s\n",
      "Epoch: 0836 loss_train: 0.3301 acc_train: 0.8000 loss_val: 1.2177 acc_val: 0.6111 time: 0.0077s\n",
      "Epoch: 0837 loss_train: 0.3149 acc_train: 0.8000 loss_val: 1.2188 acc_val: 0.6111 time: 0.0095s\n",
      "Epoch: 0838 loss_train: 0.4044 acc_train: 0.8000 loss_val: 1.2177 acc_val: 0.6111 time: 0.0106s\n",
      "Epoch: 0839 loss_train: 0.3648 acc_train: 0.8000 loss_val: 1.2246 acc_val: 0.6000 time: 0.0132s\n",
      "Epoch: 0840 loss_train: 0.1881 acc_train: 0.9000 loss_val: 1.2338 acc_val: 0.6111 time: 0.0142s\n",
      "Epoch: 0841 loss_train: 0.1895 acc_train: 0.9000 loss_val: 1.2479 acc_val: 0.5889 time: 0.0070s\n",
      "Epoch: 0842 loss_train: 0.3293 acc_train: 0.9000 loss_val: 1.2747 acc_val: 0.5778 time: 0.0129s\n",
      "Epoch: 0843 loss_train: 0.4510 acc_train: 0.8000 loss_val: 1.3127 acc_val: 0.5778 time: 0.0115s\n",
      "Epoch: 0844 loss_train: 0.1644 acc_train: 0.9000 loss_val: 1.3520 acc_val: 0.5778 time: 0.0085s\n",
      "Epoch: 0845 loss_train: 0.0976 acc_train: 0.9000 loss_val: 1.3821 acc_val: 0.5778 time: 0.0118s\n",
      "Epoch: 0846 loss_train: 0.1206 acc_train: 1.0000 loss_val: 1.4056 acc_val: 0.5778 time: 0.0100s\n",
      "Epoch: 0847 loss_train: 0.3776 acc_train: 0.9000 loss_val: 1.4299 acc_val: 0.5778 time: 0.0088s\n",
      "Epoch: 0848 loss_train: 0.2979 acc_train: 0.8000 loss_val: 1.4329 acc_val: 0.5778 time: 0.0107s\n",
      "Epoch: 0849 loss_train: 0.3422 acc_train: 0.6000 loss_val: 1.4288 acc_val: 0.5889 time: 0.0066s\n",
      "Epoch: 0850 loss_train: 0.1618 acc_train: 1.0000 loss_val: 1.4363 acc_val: 0.6111 time: 0.0131s\n",
      "Epoch: 0851 loss_train: 0.2313 acc_train: 1.0000 loss_val: 1.4376 acc_val: 0.6111 time: 0.0103s\n",
      "Epoch: 0852 loss_train: 0.2119 acc_train: 1.0000 loss_val: 1.4247 acc_val: 0.5889 time: 0.0070s\n",
      "Epoch: 0853 loss_train: 0.1639 acc_train: 1.0000 loss_val: 1.4119 acc_val: 0.5778 time: 0.0103s\n",
      "Epoch: 0854 loss_train: 0.3468 acc_train: 0.7000 loss_val: 1.4015 acc_val: 0.5778 time: 0.0125s\n",
      "Epoch: 0855 loss_train: 0.4353 acc_train: 0.6000 loss_val: 1.3855 acc_val: 0.5778 time: 0.0090s\n",
      "Epoch: 0856 loss_train: 0.3653 acc_train: 0.7000 loss_val: 1.3509 acc_val: 0.6111 time: 0.0114s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0857 loss_train: 0.1045 acc_train: 1.0000 loss_val: 1.3271 acc_val: 0.5889 time: 0.0082s\n",
      "Epoch: 0858 loss_train: 0.3702 acc_train: 0.7000 loss_val: 1.3048 acc_val: 0.5778 time: 0.0136s\n",
      "Epoch: 0859 loss_train: 0.2537 acc_train: 0.9000 loss_val: 1.2837 acc_val: 0.5778 time: 0.0154s\n",
      "Epoch: 0860 loss_train: 0.1729 acc_train: 0.9000 loss_val: 1.2670 acc_val: 0.6111 time: 0.0124s\n",
      "Epoch: 0861 loss_train: 0.1604 acc_train: 0.9000 loss_val: 1.2397 acc_val: 0.6111 time: 0.0058s\n",
      "Epoch: 0862 loss_train: 0.3298 acc_train: 0.7000 loss_val: 1.1879 acc_val: 0.6111 time: 0.0122s\n",
      "Epoch: 0863 loss_train: 0.2320 acc_train: 0.9000 loss_val: 1.1288 acc_val: 0.5889 time: 0.0092s\n",
      "Epoch: 0864 loss_train: 0.2240 acc_train: 0.9000 loss_val: 1.0758 acc_val: 0.5778 time: 0.0091s\n",
      "Epoch: 0865 loss_train: 0.3636 acc_train: 0.8000 loss_val: 1.0377 acc_val: 0.5667 time: 0.0058s\n",
      "Epoch: 0866 loss_train: 0.3659 acc_train: 0.8000 loss_val: 1.0155 acc_val: 0.5667 time: 0.0058s\n",
      "Epoch: 0867 loss_train: 0.1819 acc_train: 0.9000 loss_val: 0.9731 acc_val: 0.5889 time: 0.0057s\n",
      "Epoch: 0868 loss_train: 0.2687 acc_train: 0.9000 loss_val: 0.9662 acc_val: 0.5889 time: 0.0058s\n",
      "Epoch: 0869 loss_train: 0.2991 acc_train: 0.8000 loss_val: 0.9742 acc_val: 0.6556 time: 0.0057s\n",
      "Epoch: 0870 loss_train: 0.2936 acc_train: 0.8000 loss_val: 1.0042 acc_val: 0.6889 time: 0.0057s\n",
      "Epoch: 0871 loss_train: 0.3997 acc_train: 0.7000 loss_val: 1.0386 acc_val: 0.6222 time: 0.0058s\n",
      "Epoch: 0872 loss_train: 0.3797 acc_train: 0.9000 loss_val: 1.0482 acc_val: 0.6333 time: 0.0057s\n",
      "Epoch: 0873 loss_train: 0.2390 acc_train: 0.9000 loss_val: 1.0546 acc_val: 0.6333 time: 0.0058s\n",
      "Epoch: 0874 loss_train: 0.3101 acc_train: 0.9000 loss_val: 1.0821 acc_val: 0.6111 time: 0.0076s\n",
      "Epoch: 0875 loss_train: 0.3844 acc_train: 0.8000 loss_val: 1.1186 acc_val: 0.5778 time: 0.0056s\n",
      "Epoch: 0876 loss_train: 0.3729 acc_train: 0.8000 loss_val: 1.1749 acc_val: 0.5556 time: 0.0063s\n",
      "Epoch: 0877 loss_train: 0.3281 acc_train: 0.9000 loss_val: 1.2169 acc_val: 0.5444 time: 0.0066s\n",
      "Epoch: 0878 loss_train: 0.1566 acc_train: 1.0000 loss_val: 1.2273 acc_val: 0.5778 time: 0.0061s\n",
      "Epoch: 0879 loss_train: 0.2454 acc_train: 1.0000 loss_val: 1.2126 acc_val: 0.5778 time: 0.0058s\n",
      "Epoch: 0880 loss_train: 0.2740 acc_train: 0.9000 loss_val: 1.2002 acc_val: 0.6111 time: 0.0060s\n",
      "Epoch: 0881 loss_train: 0.3046 acc_train: 0.8000 loss_val: 1.2006 acc_val: 0.6111 time: 0.0109s\n",
      "Epoch: 0882 loss_train: 0.3361 acc_train: 1.0000 loss_val: 1.2072 acc_val: 0.6000 time: 0.0063s\n",
      "Epoch: 0883 loss_train: 0.2509 acc_train: 1.0000 loss_val: 1.2058 acc_val: 0.5778 time: 0.0117s\n",
      "Epoch: 0884 loss_train: 0.1757 acc_train: 0.9000 loss_val: 1.1877 acc_val: 0.5889 time: 0.0110s\n",
      "Epoch: 0885 loss_train: 0.1019 acc_train: 1.0000 loss_val: 1.1639 acc_val: 0.5889 time: 0.0085s\n",
      "Epoch: 0886 loss_train: 0.2718 acc_train: 0.9000 loss_val: 1.1065 acc_val: 0.5778 time: 0.0132s\n",
      "Epoch: 0887 loss_train: 0.3285 acc_train: 0.7000 loss_val: 1.0475 acc_val: 0.5778 time: 0.0119s\n",
      "Epoch: 0888 loss_train: 0.2189 acc_train: 0.9000 loss_val: 1.0039 acc_val: 0.6111 time: 0.0094s\n",
      "Epoch: 0889 loss_train: 0.4235 acc_train: 0.6000 loss_val: 0.9773 acc_val: 0.6000 time: 0.0122s\n",
      "Epoch: 0890 loss_train: 0.2955 acc_train: 0.9000 loss_val: 0.9659 acc_val: 0.6111 time: 0.0063s\n",
      "Epoch: 0891 loss_train: 0.3284 acc_train: 0.9000 loss_val: 0.9656 acc_val: 0.6333 time: 0.0122s\n",
      "Epoch: 0892 loss_train: 0.4186 acc_train: 0.9000 loss_val: 0.9449 acc_val: 0.6556 time: 0.0114s\n",
      "Epoch: 0893 loss_train: 0.3090 acc_train: 0.9000 loss_val: 0.9311 acc_val: 0.6556 time: 0.0066s\n",
      "Epoch: 0894 loss_train: 0.3219 acc_train: 0.7000 loss_val: 0.9294 acc_val: 0.6889 time: 0.0142s\n",
      "Epoch: 0895 loss_train: 0.4099 acc_train: 0.9000 loss_val: 0.9333 acc_val: 0.6111 time: 0.0126s\n",
      "Epoch: 0896 loss_train: 0.4188 acc_train: 0.8000 loss_val: 0.9478 acc_val: 0.6111 time: 0.0068s\n",
      "Epoch: 0897 loss_train: 0.3463 acc_train: 0.8000 loss_val: 0.9796 acc_val: 0.6111 time: 0.0111s\n",
      "Epoch: 0898 loss_train: 0.3619 acc_train: 0.7000 loss_val: 1.0246 acc_val: 0.6111 time: 0.0058s\n",
      "Epoch: 0899 loss_train: 0.3430 acc_train: 0.7000 loss_val: 1.0762 acc_val: 0.6111 time: 0.0132s\n",
      "Epoch: 0900 loss_train: 0.1505 acc_train: 1.0000 loss_val: 1.1257 acc_val: 0.5889 time: 0.0121s\n",
      "Optimization Finished!\n",
      "Total time elapsed: 7.9279s\n",
      "Test set results: loss= 1.0549 accuracy= 0.6182\n"
     ]
    }
   ],
   "source": [
    "cuda = False\n",
    "hidden = 16\n",
    "dropout = 0.5\n",
    "lr = 0.01\n",
    "weight_decay = 5e-4\n",
    "fastmode = False\n",
    "epochs = 900#450\n",
    "layers =2\n",
    "\n",
    "# Model and optimizer\n",
    "model = AGNN(nfeat=features.shape[1],\n",
    "                     nhid=hidden,\n",
    "                     nclass=2,\n",
    "                     nlayers=layers,\n",
    "                     dropout_rate=0.5)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(),\n",
    "                       lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "if cuda:\n",
    "    model.cuda()\n",
    "    features = features.cuda()\n",
    "    adj = adj.cuda()\n",
    "    all_labels = all_labels.cuda()\n",
    "    idx_train = idx_train.cuda()\n",
    "    idx_val = idx_val.cuda()\n",
    "    idx_test = idx_test.cuda()\n",
    "\n",
    "\n",
    "def train(epoch):\n",
    "    t = time.time()\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(features, adj)\n",
    "\n",
    "    \n",
    "    loss_train = F.nll_loss(output[idx_train], all_labels[idx_train])\n",
    "    acc_train = accuracy(output[idx_train], all_labels[idx_train])\n",
    "    loss_train.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if not fastmode:\n",
    "        # Evaluate validation set performance separately,\n",
    "        # deactivates dropout during validation run.\n",
    "        model.eval()\n",
    "        output = model(features, adj)\n",
    "\n",
    "    loss_val = F.nll_loss(output[idx_val], all_labels[idx_val])\n",
    "    acc_val = accuracy(output[idx_val], all_labels[idx_val])\n",
    "    print('Epoch: {:04d}'.format(epoch+1),\n",
    "          'loss_train: {:.4f}'.format(loss_train.item()),\n",
    "          'acc_train: {:.4f}'.format(acc_train.item()),\n",
    "          'loss_val: {:.4f}'.format(loss_val.item()),\n",
    "          'acc_val: {:.4f}'.format(acc_val.item()),\n",
    "          'time: {:.4f}s'.format(time.time() - t))\n",
    "\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    output = model(features, adj)\n",
    "    loss_test = F.nll_loss(output[idx_test], all_labels[idx_test])\n",
    "    acc_test = accuracy(output[idx_test], all_labels[idx_test])\n",
    "    print(\"Test set results:\",\n",
    "          \"loss= {:.4f}\".format(loss_test.item()),\n",
    "          \"accuracy= {:.4f}\".format(acc_test.item()))\n",
    "\n",
    "\n",
    "# Train model\n",
    "t_total = time.time()\n",
    "for epoch in range(epochs):\n",
    "    train(epoch)\n",
    "print(\"Optimization Finished!\")\n",
    "print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))\n",
    "\n",
    "# Testing\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
