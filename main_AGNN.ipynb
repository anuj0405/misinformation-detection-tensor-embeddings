{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.ArticlesHandler import ArticlesHandler\n",
    "from utils import solve, embedding_matrix_2_kNN, get_rate, accuracy, precision, recall, f1_score\n",
    "from utils import Config\n",
    "import time\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "from postprocessing.SelectLabelsPostprocessor import SelectLabelsPostprocessor\n",
    "from pygcn.utils import encode_onehot, accuracy, load_from_features\n",
    "from model import AGNN\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import config file and check some values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method of decomposition: parafac\n"
     ]
    }
   ],
   "source": [
    "config = Config(file='config')\n",
    "\n",
    "assert (config.num_fake_articles + config.num_real_articles > \n",
    "        config.num_nearest_neighbours), \"Can't have more neighbours than nodes!\"\n",
    "\n",
    "print(\"Method of decomposition:\", config.method_decomposition_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the articles and decompose the tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset Random Poltical News Dataset\n",
      "Performing decomposition...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/benamira/19793564030D4273/MCsBackup/3A/OMA/Projet/misinformation-detection-tensor-embeddings/tensorly/decomposition/_tucker.py:60: Warning: Given only one int for 'rank' intead of a list of 3 modes. Using this rank for all modes.\n",
      "  warnings.warn(message, Warning)\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading dataset\", config.dataset_name)\n",
    "articles = ArticlesHandler(config)\n",
    "\n",
    "print(\"Performing decomposition...\")\n",
    "C = articles.get_tensor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.set(\"num_unknown_labels\", 195)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = articles.articles.labels\n",
    "all_labels = articles.articles.labels_untouched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_higest_similarities = np.argpartition(C, 5, axis=1)[:, :5]\n",
    "adj_mat = np.zeros((150, 150), dtype=int)\n",
    "for i, neightbors in enumerate(index_higest_similarities):\n",
    "    adj_mat[i, neightbors] = 1\n",
    "# Force symetric\n",
    "adj_mat = adj_mat + np.transpose(adj_mat)\n",
    "adj_mat[adj_mat == 2] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj, features, all_labels = load_from_features(C, all_labels, config)\n",
    "_, _, labels = load_from_features(C, labels, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2,\n",
      "        2, 0, 0, 0, 2, 0, 2, 1, 1, 1, 2, 1, 1, 1, 1, 2, 1, 2, 2, 1, 2, 2, 2, 2,\n",
      "        1, 1, 1, 2, 1, 2, 1, 2, 2, 1, 2, 2, 1, 1, 1, 2, 1, 2, 2, 2, 2, 1, 1, 1,\n",
      "        2, 1, 1, 2, 1, 1, 2, 2, 1, 1, 1, 1, 2, 2, 1, 2, 1, 2, 2, 2, 1, 2, 2, 1,\n",
      "        1, 1, 2, 2, 2, 1])\n",
      "75\n"
     ]
    }
   ],
   "source": [
    "# idx_train = range(150)\n",
    "# idx_val = range(150, 175)\n",
    "# idx_test = range(175, 200)\n",
    "print(labels)\n",
    "idx_train = np.where(labels)[0]\n",
    "idx_val = np.where(1 - abs(labels))[0][:90]\n",
    "idx_test = np.where(1 - abs(labels))[0][90:]\n",
    "\n",
    "print(len(idx_train))\n",
    "\n",
    "idx_train = torch.LongTensor(idx_train)\n",
    "idx_val = torch.LongTensor(idx_val)\n",
    "idx_test = torch.LongTensor(idx_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class GraphAttentionLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, requires_grad=True):\n",
    "        super(GraphAttentionLayer, self).__init__()\n",
    "        if requires_grad:\n",
    "            # unifrom initialization\n",
    "            self.beta = Parameter(torch.Tensor(1).uniform_(\n",
    "                0, 1), requires_grad=requires_grad)\n",
    "        else:\n",
    "            self.beta = Variable(torch.zeros(1), requires_grad=requires_grad)\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "\n",
    "\n",
    "        # NaN grad bug fixed at pytorch 0.3. Release note:\n",
    "        #     `when torch.norm returned 0.0, the gradient was NaN.\n",
    "        #     We now use the subgradient at 0.0, so the gradient is 0.0.`\n",
    "        norm2 = torch.norm(x, 2, 1).view(-1, 1)\n",
    "\n",
    "        # add a minor constant (1e-7) to denominator to prevent division by\n",
    "        # zero error\n",
    "        if torch.cuda.is_available():\n",
    "            cos = self.beta.cuda() * \\\n",
    "                  torch.div(torch.mm(x, x.t()), torch.mm(norm2, norm2.t()) + 1e-7)\n",
    "        else:\n",
    "            cos = self.beta * \\\n",
    "                  torch.div(torch.mm(x, x.t()), torch.mm(norm2, norm2.t()) + 1e-7)\n",
    "\n",
    "        # neighborhood masking (inspired by this repo:\n",
    "        # https://github.com/danielegrattarola/keras-gat)\n",
    "        \n",
    "        \n",
    "        mask = (torch.ones(adj.shape) - adj) * -1e9\n",
    "        masked = cos + mask\n",
    "\n",
    "        # propagation matrix\n",
    "        P = F.softmax(masked, dim=1)\n",
    "\n",
    "        # attention-guided propagation\n",
    "        output = torch.mm(P, x)\n",
    "        return output\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + ' (16 -> 16)'\n",
    "\n",
    "\n",
    "class LinearLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, in_features, out_features, initializer=nn.init.xavier_uniform_):\n",
    "        super(LinearLayer, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = Parameter(initializer(\n",
    "            torch.Tensor(in_features, out_features)))\n",
    "\n",
    "    def forward(self, input):\n",
    "        # no bias\n",
    "        return torch.mm(input, self.weight)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + ' (' \\\n",
    "            + str(self.in_features) + ' -> ' \\\n",
    "            + str(self.out_features) + ')'\n",
    "\n",
    "\n",
    "class AGNN(nn.Module):\n",
    "\n",
    "    def __init__(self, nfeat, nhid, nclass, nlayers, dropout_rate):\n",
    "        super(AGNN, self).__init__()\n",
    "\n",
    "        self.layers = nlayers\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "        self.embeddinglayer = LinearLayer(nfeat, nhid)\n",
    "        nn.init.xavier_uniform_(self.embeddinglayer.weight)\n",
    "\n",
    "        self.attentionlayers = nn.ModuleList()\n",
    "        # for Cora dataset, the first propagation layer is non-trainable\n",
    "        # and beta is fixed at 0\n",
    "        self.attentionlayers.append(GraphAttentionLayer(requires_grad=False).cuda())\n",
    "        for i in range(1, self.layers):\n",
    "            if torch.cuda.is_available():\n",
    "                self.attentionlayers.append(GraphAttentionLayer().cuda())\n",
    "            else:\n",
    "                self.attentionlayers.append(GraphAttentionLayer())\n",
    "\n",
    "\n",
    "        self.outputlayer = LinearLayer(nhid, nclass)\n",
    "        nn.init.xavier_uniform_(self.outputlayer.weight)\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        x = F.relu(self.embeddinglayer(x))\n",
    "        x = F.dropout(x, self.dropout_rate, training=self.training)\n",
    "\n",
    "        for i in range(self.layers):\n",
    "            x = self.attentionlayers[i](x, adj)\n",
    "\n",
    "        x = self.outputlayer(x)\n",
    "        x = F.dropout(x, self.dropout_rate, training=self.training)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 loss_train: 1.0692 acc_train: 0.4800 loss_val: 0.6570 acc_val: 0.5667 time: 0.0164s\n",
      "Epoch: 0002 loss_train: 1.3695 acc_train: 0.4667 loss_val: 0.6532 acc_val: 0.6111 time: 0.0248s\n",
      "Epoch: 0003 loss_train: 0.7126 acc_train: 0.5333 loss_val: 0.6539 acc_val: 0.6000 time: 0.0234s\n",
      "Epoch: 0004 loss_train: 0.7185 acc_train: 0.4800 loss_val: 0.6649 acc_val: 0.6222 time: 0.0143s\n",
      "Epoch: 0005 loss_train: 0.9660 acc_train: 0.4667 loss_val: 0.6922 acc_val: 0.5778 time: 0.0139s\n",
      "Epoch: 0006 loss_train: 0.7320 acc_train: 0.5733 loss_val: 0.7363 acc_val: 0.5556 time: 0.0145s\n",
      "Epoch: 0007 loss_train: 0.6301 acc_train: 0.6000 loss_val: 0.7851 acc_val: 0.5778 time: 0.0151s\n",
      "Epoch: 0008 loss_train: 0.6461 acc_train: 0.6133 loss_val: 0.8319 acc_val: 0.6000 time: 0.0143s\n",
      "Epoch: 0009 loss_train: 0.6490 acc_train: 0.5733 loss_val: 0.8760 acc_val: 0.6111 time: 0.0140s\n",
      "Epoch: 0010 loss_train: 0.7857 acc_train: 0.4800 loss_val: 0.9166 acc_val: 0.6222 time: 0.0128s\n",
      "Epoch: 0011 loss_train: 0.6688 acc_train: 0.6533 loss_val: 0.9438 acc_val: 0.6000 time: 0.0126s\n",
      "Epoch: 0012 loss_train: 0.7572 acc_train: 0.4267 loss_val: 0.9642 acc_val: 0.6222 time: 0.0114s\n",
      "Epoch: 0013 loss_train: 0.7222 acc_train: 0.5200 loss_val: 0.9803 acc_val: 0.6222 time: 0.0115s\n",
      "Epoch: 0014 loss_train: 0.6416 acc_train: 0.6667 loss_val: 0.9884 acc_val: 0.6111 time: 0.0126s\n",
      "Epoch: 0015 loss_train: 0.6505 acc_train: 0.5600 loss_val: 0.9912 acc_val: 0.6111 time: 0.0170s\n",
      "Epoch: 0016 loss_train: 0.6185 acc_train: 0.5867 loss_val: 0.9966 acc_val: 0.6111 time: 0.0186s\n",
      "Epoch: 0017 loss_train: 0.6550 acc_train: 0.5467 loss_val: 0.9996 acc_val: 0.6000 time: 0.0225s\n",
      "Epoch: 0018 loss_train: 0.6540 acc_train: 0.5867 loss_val: 1.0033 acc_val: 0.6111 time: 0.0143s\n",
      "Epoch: 0019 loss_train: 0.6262 acc_train: 0.6267 loss_val: 1.0108 acc_val: 0.6000 time: 0.0154s\n",
      "Epoch: 0020 loss_train: 0.6607 acc_train: 0.5867 loss_val: 1.0227 acc_val: 0.6111 time: 0.0144s\n",
      "Epoch: 0021 loss_train: 0.5914 acc_train: 0.6667 loss_val: 1.0316 acc_val: 0.5889 time: 0.0133s\n",
      "Epoch: 0022 loss_train: 0.6496 acc_train: 0.4933 loss_val: 1.0514 acc_val: 0.5778 time: 0.0138s\n",
      "Epoch: 0023 loss_train: 0.6644 acc_train: 0.5867 loss_val: 1.0669 acc_val: 0.5556 time: 0.0117s\n",
      "Epoch: 0024 loss_train: 0.6403 acc_train: 0.5600 loss_val: 1.0772 acc_val: 0.5556 time: 0.0125s\n",
      "Epoch: 0025 loss_train: 0.6108 acc_train: 0.6400 loss_val: 1.0828 acc_val: 0.5556 time: 0.0146s\n",
      "Epoch: 0026 loss_train: 0.6120 acc_train: 0.6933 loss_val: 1.0909 acc_val: 0.5556 time: 0.0128s\n",
      "Epoch: 0027 loss_train: 0.5830 acc_train: 0.6667 loss_val: 1.0988 acc_val: 0.5667 time: 0.0128s\n",
      "Epoch: 0028 loss_train: 0.6258 acc_train: 0.5867 loss_val: 1.1124 acc_val: 0.5556 time: 0.0147s\n",
      "Epoch: 0029 loss_train: 0.8330 acc_train: 0.5200 loss_val: 1.1467 acc_val: 0.5667 time: 0.0166s\n",
      "Epoch: 0030 loss_train: 0.7012 acc_train: 0.5600 loss_val: 1.1816 acc_val: 0.5667 time: 0.0218s\n",
      "Epoch: 0031 loss_train: 0.5782 acc_train: 0.6667 loss_val: 1.2131 acc_val: 0.5667 time: 0.0127s\n",
      "Epoch: 0032 loss_train: 0.6236 acc_train: 0.6133 loss_val: 1.2266 acc_val: 0.5667 time: 0.0133s\n",
      "Epoch: 0033 loss_train: 0.5797 acc_train: 0.6933 loss_val: 1.2422 acc_val: 0.5667 time: 0.0123s\n",
      "Epoch: 0034 loss_train: 0.5159 acc_train: 0.7467 loss_val: 1.2570 acc_val: 0.5667 time: 0.0122s\n",
      "Epoch: 0035 loss_train: 0.5408 acc_train: 0.7467 loss_val: 1.2662 acc_val: 0.5667 time: 0.0120s\n",
      "Epoch: 0036 loss_train: 0.5490 acc_train: 0.6800 loss_val: 1.2809 acc_val: 0.5778 time: 0.0119s\n",
      "Epoch: 0037 loss_train: 0.6698 acc_train: 0.6133 loss_val: 1.3247 acc_val: 0.6333 time: 0.0116s\n",
      "Epoch: 0038 loss_train: 0.6190 acc_train: 0.7067 loss_val: 1.3649 acc_val: 0.6333 time: 0.0120s\n",
      "Epoch: 0039 loss_train: 0.5948 acc_train: 0.6800 loss_val: 1.3977 acc_val: 0.6222 time: 0.0129s\n",
      "Epoch: 0040 loss_train: 0.5474 acc_train: 0.7733 loss_val: 1.4221 acc_val: 0.6333 time: 0.0154s\n",
      "Epoch: 0041 loss_train: 0.5816 acc_train: 0.6267 loss_val: 1.4346 acc_val: 0.6333 time: 0.0142s\n",
      "Epoch: 0042 loss_train: 0.6922 acc_train: 0.5867 loss_val: 1.4427 acc_val: 0.6444 time: 0.0148s\n",
      "Epoch: 0043 loss_train: 0.5629 acc_train: 0.7467 loss_val: 1.4497 acc_val: 0.6444 time: 0.0163s\n",
      "Epoch: 0044 loss_train: 0.5603 acc_train: 0.7067 loss_val: 1.4578 acc_val: 0.6778 time: 0.0200s\n",
      "Epoch: 0045 loss_train: 0.5581 acc_train: 0.6933 loss_val: 1.4671 acc_val: 0.7111 time: 0.0155s\n",
      "Epoch: 0046 loss_train: 0.6194 acc_train: 0.6533 loss_val: 1.4764 acc_val: 0.7000 time: 0.0196s\n",
      "Epoch: 0047 loss_train: 0.5676 acc_train: 0.7200 loss_val: 1.4818 acc_val: 0.7000 time: 0.0130s\n",
      "Epoch: 0048 loss_train: 0.5711 acc_train: 0.7600 loss_val: 1.4802 acc_val: 0.7222 time: 0.0133s\n",
      "Epoch: 0049 loss_train: 0.5872 acc_train: 0.6400 loss_val: 1.4757 acc_val: 0.7111 time: 0.0120s\n",
      "Epoch: 0050 loss_train: 0.5481 acc_train: 0.6933 loss_val: 1.4722 acc_val: 0.7222 time: 0.0133s\n",
      "Epoch: 0051 loss_train: 0.5086 acc_train: 0.8267 loss_val: 1.4673 acc_val: 0.7222 time: 0.0119s\n",
      "Epoch: 0052 loss_train: 0.4965 acc_train: 0.7467 loss_val: 1.4635 acc_val: 0.7222 time: 0.0118s\n",
      "Epoch: 0053 loss_train: 0.5233 acc_train: 0.7733 loss_val: 1.4535 acc_val: 0.7222 time: 0.0118s\n",
      "Epoch: 0054 loss_train: 0.5241 acc_train: 0.7333 loss_val: 1.4466 acc_val: 0.7222 time: 0.0120s\n",
      "Epoch: 0055 loss_train: 0.5572 acc_train: 0.6800 loss_val: 1.4376 acc_val: 0.6889 time: 0.0119s\n",
      "Epoch: 0056 loss_train: 0.4748 acc_train: 0.8267 loss_val: 1.4294 acc_val: 0.6556 time: 0.0117s\n",
      "Epoch: 0057 loss_train: 0.5645 acc_train: 0.6533 loss_val: 1.4203 acc_val: 0.6667 time: 0.0118s\n",
      "Epoch: 0058 loss_train: 0.5486 acc_train: 0.7200 loss_val: 1.4119 acc_val: 0.6667 time: 0.0143s\n",
      "Epoch: 0059 loss_train: 0.6214 acc_train: 0.7467 loss_val: 1.3974 acc_val: 0.6778 time: 0.0168s\n",
      "Epoch: 0060 loss_train: 0.5303 acc_train: 0.7733 loss_val: 1.3862 acc_val: 0.6778 time: 0.0213s\n",
      "Epoch: 0061 loss_train: 0.5732 acc_train: 0.6533 loss_val: 1.3779 acc_val: 0.6778 time: 0.0136s\n",
      "Epoch: 0062 loss_train: 0.5677 acc_train: 0.7200 loss_val: 1.3758 acc_val: 0.6667 time: 0.0123s\n",
      "Epoch: 0063 loss_train: 0.4719 acc_train: 0.8000 loss_val: 1.3735 acc_val: 0.6667 time: 0.0117s\n",
      "Epoch: 0064 loss_train: 0.5609 acc_train: 0.6800 loss_val: 1.3687 acc_val: 0.6556 time: 0.0138s\n",
      "Epoch: 0065 loss_train: 0.5478 acc_train: 0.6667 loss_val: 1.3676 acc_val: 0.6556 time: 0.0237s\n",
      "Epoch: 0066 loss_train: 0.5306 acc_train: 0.7600 loss_val: 1.3673 acc_val: 0.6556 time: 0.0189s\n",
      "Epoch: 0067 loss_train: 0.5386 acc_train: 0.6267 loss_val: 1.3682 acc_val: 0.6556 time: 0.0221s\n",
      "Epoch: 0068 loss_train: 0.5063 acc_train: 0.7733 loss_val: 1.3660 acc_val: 0.6556 time: 0.0489s\n",
      "Epoch: 0069 loss_train: 0.5606 acc_train: 0.7067 loss_val: 1.3645 acc_val: 0.6556 time: 0.0345s\n",
      "Epoch: 0070 loss_train: 0.5062 acc_train: 0.7200 loss_val: 1.3665 acc_val: 0.6556 time: 0.0310s\n",
      "Epoch: 0071 loss_train: 0.5139 acc_train: 0.8267 loss_val: 1.3721 acc_val: 0.6556 time: 0.0445s\n",
      "Epoch: 0072 loss_train: 0.5561 acc_train: 0.7067 loss_val: 1.3802 acc_val: 0.6556 time: 0.0259s\n",
      "Epoch: 0073 loss_train: 0.5165 acc_train: 0.7467 loss_val: 1.3862 acc_val: 0.6556 time: 0.0545s\n",
      "Epoch: 0074 loss_train: 0.5155 acc_train: 0.7733 loss_val: 1.3868 acc_val: 0.6556 time: 0.0698s\n",
      "Epoch: 0075 loss_train: 0.5807 acc_train: 0.6533 loss_val: 1.3903 acc_val: 0.6556 time: 0.0336s\n",
      "Epoch: 0076 loss_train: 0.5323 acc_train: 0.7467 loss_val: 1.3904 acc_val: 0.6556 time: 0.0496s\n",
      "Epoch: 0077 loss_train: 0.5359 acc_train: 0.7733 loss_val: 1.3885 acc_val: 0.6556 time: 0.0220s\n",
      "Epoch: 0078 loss_train: 0.5318 acc_train: 0.7067 loss_val: 1.3905 acc_val: 0.6556 time: 0.0120s\n",
      "Epoch: 0079 loss_train: 0.5040 acc_train: 0.7733 loss_val: 1.3931 acc_val: 0.7111 time: 0.0122s\n",
      "Epoch: 0080 loss_train: 0.5554 acc_train: 0.7600 loss_val: 1.3999 acc_val: 0.7000 time: 0.0124s\n",
      "Epoch: 0081 loss_train: 0.5222 acc_train: 0.6400 loss_val: 1.4080 acc_val: 0.7222 time: 0.0144s\n",
      "Epoch: 0082 loss_train: 0.7352 acc_train: 0.5867 loss_val: 1.3993 acc_val: 0.7111 time: 0.0137s\n",
      "Epoch: 0083 loss_train: 0.5381 acc_train: 0.7867 loss_val: 1.3873 acc_val: 0.7222 time: 0.0135s\n",
      "Epoch: 0084 loss_train: 0.5423 acc_train: 0.7867 loss_val: 1.3754 acc_val: 0.7111 time: 0.0132s\n",
      "Epoch: 0085 loss_train: 0.5595 acc_train: 0.7067 loss_val: 1.3654 acc_val: 0.6889 time: 0.0155s\n",
      "Epoch: 0086 loss_train: 0.5326 acc_train: 0.7200 loss_val: 1.3525 acc_val: 0.6889 time: 0.0331s\n",
      "Epoch: 0087 loss_train: 0.4613 acc_train: 0.7867 loss_val: 1.3423 acc_val: 0.6778 time: 0.0263s\n",
      "Epoch: 0088 loss_train: 0.6315 acc_train: 0.6933 loss_val: 1.3236 acc_val: 0.6778 time: 0.0139s\n",
      "Epoch: 0089 loss_train: 0.5143 acc_train: 0.7600 loss_val: 1.3078 acc_val: 0.6667 time: 0.0142s\n",
      "Epoch: 0090 loss_train: 0.5148 acc_train: 0.7467 loss_val: 1.2922 acc_val: 0.6667 time: 0.0152s\n",
      "Epoch: 0091 loss_train: 0.5159 acc_train: 0.7200 loss_val: 1.2763 acc_val: 0.6556 time: 0.0144s\n",
      "Epoch: 0092 loss_train: 0.6203 acc_train: 0.6400 loss_val: 1.2541 acc_val: 0.6556 time: 0.0129s\n",
      "Epoch: 0093 loss_train: 0.5223 acc_train: 0.8133 loss_val: 1.2318 acc_val: 0.6444 time: 0.0138s\n",
      "Epoch: 0094 loss_train: 0.4687 acc_train: 0.7733 loss_val: 1.2146 acc_val: 0.6444 time: 0.0126s\n",
      "Epoch: 0095 loss_train: 0.5428 acc_train: 0.7067 loss_val: 1.1996 acc_val: 0.6444 time: 0.0121s\n",
      "Epoch: 0096 loss_train: 0.5543 acc_train: 0.7200 loss_val: 1.1868 acc_val: 0.6444 time: 0.0119s\n",
      "Epoch: 0097 loss_train: 0.5055 acc_train: 0.7600 loss_val: 1.1728 acc_val: 0.6444 time: 0.0118s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0098 loss_train: 0.5788 acc_train: 0.7467 loss_val: 1.1609 acc_val: 0.6444 time: 0.0157s\n",
      "Epoch: 0099 loss_train: 0.4897 acc_train: 0.7600 loss_val: 1.1503 acc_val: 0.6444 time: 0.0168s\n",
      "Epoch: 0100 loss_train: 0.5268 acc_train: 0.7200 loss_val: 1.1455 acc_val: 0.6444 time: 0.0171s\n",
      "Epoch: 0101 loss_train: 0.4662 acc_train: 0.7467 loss_val: 1.1448 acc_val: 0.6444 time: 0.0169s\n",
      "Epoch: 0102 loss_train: 0.5056 acc_train: 0.6933 loss_val: 1.1444 acc_val: 0.6444 time: 0.0130s\n",
      "Epoch: 0103 loss_train: 0.5304 acc_train: 0.7067 loss_val: 1.1507 acc_val: 0.6444 time: 0.0129s\n",
      "Epoch: 0104 loss_train: 0.5168 acc_train: 0.7067 loss_val: 1.1602 acc_val: 0.6444 time: 0.0130s\n",
      "Epoch: 0105 loss_train: 0.5153 acc_train: 0.7067 loss_val: 1.1688 acc_val: 0.6556 time: 0.0128s\n",
      "Epoch: 0106 loss_train: 0.5477 acc_train: 0.7067 loss_val: 1.1769 acc_val: 0.6667 time: 0.0143s\n",
      "Epoch: 0107 loss_train: 0.5408 acc_train: 0.6533 loss_val: 1.1832 acc_val: 0.6444 time: 0.0141s\n",
      "Epoch: 0108 loss_train: 0.5352 acc_train: 0.7733 loss_val: 1.1828 acc_val: 0.6444 time: 0.0132s\n",
      "Epoch: 0109 loss_train: 0.5603 acc_train: 0.7200 loss_val: 1.1765 acc_val: 0.6444 time: 0.0134s\n",
      "Epoch: 0110 loss_train: 0.5533 acc_train: 0.8000 loss_val: 1.1680 acc_val: 0.6444 time: 0.0133s\n",
      "Epoch: 0111 loss_train: 0.5141 acc_train: 0.8133 loss_val: 1.1564 acc_val: 0.6444 time: 0.0132s\n",
      "Epoch: 0112 loss_train: 0.4937 acc_train: 0.7067 loss_val: 1.1445 acc_val: 0.6667 time: 0.0166s\n",
      "Epoch: 0113 loss_train: 0.5576 acc_train: 0.8267 loss_val: 1.1267 acc_val: 0.6444 time: 0.0219s\n",
      "Epoch: 0114 loss_train: 0.5201 acc_train: 0.7333 loss_val: 1.1108 acc_val: 0.6444 time: 0.0309s\n",
      "Epoch: 0115 loss_train: 0.4587 acc_train: 0.8000 loss_val: 1.0965 acc_val: 0.6444 time: 0.0265s\n",
      "Epoch: 0116 loss_train: 0.5429 acc_train: 0.7467 loss_val: 1.0791 acc_val: 0.6444 time: 0.0269s\n",
      "Epoch: 0117 loss_train: 0.4723 acc_train: 0.7867 loss_val: 1.0655 acc_val: 0.6333 time: 0.0140s\n",
      "Epoch: 0118 loss_train: 0.5561 acc_train: 0.6933 loss_val: 1.0558 acc_val: 0.6444 time: 0.0121s\n",
      "Epoch: 0119 loss_train: 0.5145 acc_train: 0.7200 loss_val: 1.0481 acc_val: 0.6444 time: 0.0120s\n",
      "Epoch: 0120 loss_train: 0.5686 acc_train: 0.6800 loss_val: 1.0435 acc_val: 0.6444 time: 0.0127s\n",
      "Epoch: 0121 loss_train: 0.4981 acc_train: 0.7333 loss_val: 1.0358 acc_val: 0.6444 time: 0.0125s\n",
      "Epoch: 0122 loss_train: 0.5009 acc_train: 0.7733 loss_val: 1.0312 acc_val: 0.6444 time: 0.0118s\n",
      "Epoch: 0123 loss_train: 0.4857 acc_train: 0.7733 loss_val: 1.0284 acc_val: 0.6111 time: 0.0124s\n",
      "Epoch: 0124 loss_train: 0.6035 acc_train: 0.6533 loss_val: 1.0291 acc_val: 0.6111 time: 0.0168s\n",
      "Epoch: 0125 loss_train: 0.5890 acc_train: 0.6000 loss_val: 1.0308 acc_val: 0.6222 time: 0.0170s\n",
      "Epoch: 0126 loss_train: 0.5293 acc_train: 0.6800 loss_val: 1.0352 acc_val: 0.6222 time: 0.0186s\n",
      "Epoch: 0127 loss_train: 0.4598 acc_train: 0.7200 loss_val: 1.0398 acc_val: 0.6333 time: 0.0125s\n",
      "Epoch: 0128 loss_train: 0.5129 acc_train: 0.6933 loss_val: 1.0436 acc_val: 0.6333 time: 0.0120s\n",
      "Epoch: 0129 loss_train: 0.4790 acc_train: 0.7733 loss_val: 1.0489 acc_val: 0.6444 time: 0.0119s\n",
      "Epoch: 0130 loss_train: 0.5459 acc_train: 0.7867 loss_val: 1.0484 acc_val: 0.6444 time: 0.0117s\n",
      "Epoch: 0131 loss_train: 0.4526 acc_train: 0.8000 loss_val: 1.0424 acc_val: 0.6444 time: 0.0117s\n",
      "Epoch: 0132 loss_train: 0.4960 acc_train: 0.7867 loss_val: 1.0374 acc_val: 0.6444 time: 0.0119s\n",
      "Epoch: 0133 loss_train: 0.5166 acc_train: 0.7200 loss_val: 1.0334 acc_val: 0.6444 time: 0.0116s\n",
      "Epoch: 0134 loss_train: 0.4642 acc_train: 0.7733 loss_val: 1.0295 acc_val: 0.6444 time: 0.0116s\n",
      "Epoch: 0135 loss_train: 0.5418 acc_train: 0.6667 loss_val: 1.0262 acc_val: 0.6444 time: 0.0116s\n",
      "Epoch: 0136 loss_train: 0.5153 acc_train: 0.7333 loss_val: 1.0166 acc_val: 0.6444 time: 0.0117s\n",
      "Epoch: 0137 loss_train: 0.4598 acc_train: 0.7867 loss_val: 1.0085 acc_val: 0.6444 time: 0.0119s\n",
      "Epoch: 0138 loss_train: 0.5051 acc_train: 0.7867 loss_val: 0.9977 acc_val: 0.6444 time: 0.0120s\n",
      "Epoch: 0139 loss_train: 0.4955 acc_train: 0.6933 loss_val: 0.9886 acc_val: 0.6333 time: 0.0119s\n",
      "Epoch: 0140 loss_train: 0.4993 acc_train: 0.7467 loss_val: 0.9819 acc_val: 0.6333 time: 0.0142s\n",
      "Epoch: 0141 loss_train: 0.5226 acc_train: 0.7333 loss_val: 0.9767 acc_val: 0.6333 time: 0.0123s\n",
      "Epoch: 0142 loss_train: 0.5341 acc_train: 0.7467 loss_val: 0.9711 acc_val: 0.6222 time: 0.0159s\n",
      "Epoch: 0143 loss_train: 0.4581 acc_train: 0.7733 loss_val: 0.9671 acc_val: 0.6222 time: 0.0226s\n",
      "Epoch: 0144 loss_train: 0.5374 acc_train: 0.7333 loss_val: 0.9425 acc_val: 0.6222 time: 0.0124s\n",
      "Epoch: 0145 loss_train: 0.4759 acc_train: 0.6933 loss_val: 0.9339 acc_val: 0.6444 time: 0.0131s\n",
      "Epoch: 0146 loss_train: 0.4791 acc_train: 0.7600 loss_val: 0.9204 acc_val: 0.6667 time: 0.0120s\n",
      "Epoch: 0147 loss_train: 0.5069 acc_train: 0.7333 loss_val: 0.9067 acc_val: 0.6778 time: 0.0124s\n",
      "Epoch: 0148 loss_train: 0.4901 acc_train: 0.7333 loss_val: 0.8955 acc_val: 0.6778 time: 0.0117s\n",
      "Epoch: 0149 loss_train: 0.5801 acc_train: 0.7200 loss_val: 0.8897 acc_val: 0.6778 time: 0.0123s\n",
      "Epoch: 0150 loss_train: 0.4974 acc_train: 0.7200 loss_val: 0.8893 acc_val: 0.7000 time: 0.0132s\n",
      "Epoch: 0151 loss_train: 0.4345 acc_train: 0.8000 loss_val: 0.9113 acc_val: 0.7111 time: 0.0137s\n",
      "Epoch: 0152 loss_train: 0.4349 acc_train: 0.7333 loss_val: 0.9285 acc_val: 0.7222 time: 0.0148s\n",
      "Epoch: 0153 loss_train: 0.4634 acc_train: 0.8133 loss_val: 0.9455 acc_val: 0.7444 time: 0.0146s\n",
      "Epoch: 0154 loss_train: 0.5695 acc_train: 0.6933 loss_val: 0.9603 acc_val: 0.7667 time: 0.0133s\n",
      "Epoch: 0155 loss_train: 0.4967 acc_train: 0.7733 loss_val: 0.9689 acc_val: 0.7667 time: 0.0162s\n",
      "Epoch: 0156 loss_train: 0.4916 acc_train: 0.7333 loss_val: 0.9779 acc_val: 0.7667 time: 0.0183s\n",
      "Epoch: 0157 loss_train: 0.4624 acc_train: 0.7867 loss_val: 0.9881 acc_val: 0.7667 time: 0.0191s\n",
      "Epoch: 0158 loss_train: 0.5070 acc_train: 0.7733 loss_val: 0.9935 acc_val: 0.7667 time: 0.0132s\n",
      "Epoch: 0159 loss_train: 0.5410 acc_train: 0.7333 loss_val: 0.9918 acc_val: 0.7667 time: 0.0132s\n",
      "Epoch: 0160 loss_train: 0.4829 acc_train: 0.7733 loss_val: 0.9897 acc_val: 0.7667 time: 0.0132s\n",
      "Epoch: 0161 loss_train: 0.4692 acc_train: 0.8000 loss_val: 0.9899 acc_val: 0.7667 time: 0.0131s\n",
      "Epoch: 0162 loss_train: 0.4449 acc_train: 0.7867 loss_val: 0.9867 acc_val: 0.7667 time: 0.0128s\n",
      "Epoch: 0163 loss_train: 0.4980 acc_train: 0.7067 loss_val: 0.9809 acc_val: 0.7667 time: 0.0137s\n",
      "Epoch: 0164 loss_train: 0.5203 acc_train: 0.7067 loss_val: 0.9738 acc_val: 0.7667 time: 0.0152s\n",
      "Epoch: 0165 loss_train: 0.5285 acc_train: 0.6933 loss_val: 0.9715 acc_val: 0.7667 time: 0.0148s\n",
      "Epoch: 0166 loss_train: 0.6561 acc_train: 0.6933 loss_val: 0.9653 acc_val: 0.7556 time: 0.0143s\n",
      "Epoch: 0167 loss_train: 0.4783 acc_train: 0.7600 loss_val: 0.9568 acc_val: 0.7444 time: 0.0121s\n",
      "Epoch: 0168 loss_train: 0.4831 acc_train: 0.7600 loss_val: 0.9523 acc_val: 0.7444 time: 0.0127s\n",
      "Epoch: 0169 loss_train: 0.4708 acc_train: 0.7867 loss_val: 0.9503 acc_val: 0.7111 time: 0.0142s\n",
      "Epoch: 0170 loss_train: 0.4454 acc_train: 0.8267 loss_val: 0.9487 acc_val: 0.7111 time: 0.0153s\n",
      "Epoch: 0171 loss_train: 0.5155 acc_train: 0.6800 loss_val: 0.9496 acc_val: 0.7333 time: 0.0216s\n",
      "Epoch: 0172 loss_train: 0.5296 acc_train: 0.7333 loss_val: 0.9546 acc_val: 0.7333 time: 0.0198s\n",
      "Epoch: 0173 loss_train: 0.4888 acc_train: 0.7200 loss_val: 0.9647 acc_val: 0.7333 time: 0.0129s\n",
      "Epoch: 0174 loss_train: 0.5179 acc_train: 0.6667 loss_val: 0.9762 acc_val: 0.7111 time: 0.0230s\n",
      "Epoch: 0175 loss_train: 0.5032 acc_train: 0.6933 loss_val: 0.9838 acc_val: 0.7111 time: 0.0129s\n",
      "Epoch: 0176 loss_train: 0.5291 acc_train: 0.7200 loss_val: 0.9929 acc_val: 0.6778 time: 0.0129s\n",
      "Epoch: 0177 loss_train: 0.5220 acc_train: 0.7867 loss_val: 0.9947 acc_val: 0.6778 time: 0.0129s\n",
      "Epoch: 0178 loss_train: 0.5539 acc_train: 0.7200 loss_val: 0.9917 acc_val: 0.6778 time: 0.0128s\n",
      "Epoch: 0179 loss_train: 0.5414 acc_train: 0.6800 loss_val: 0.9902 acc_val: 0.6778 time: 0.0126s\n",
      "Epoch: 0180 loss_train: 0.4809 acc_train: 0.8133 loss_val: 0.9899 acc_val: 0.7000 time: 0.0128s\n",
      "Epoch: 0181 loss_train: 0.5097 acc_train: 0.6800 loss_val: 0.9868 acc_val: 0.7000 time: 0.0127s\n",
      "Epoch: 0182 loss_train: 0.5605 acc_train: 0.7467 loss_val: 0.9872 acc_val: 0.7000 time: 0.0134s\n",
      "Epoch: 0183 loss_train: 0.4260 acc_train: 0.8400 loss_val: 0.9841 acc_val: 0.7000 time: 0.0145s\n",
      "Epoch: 0184 loss_train: 0.4483 acc_train: 0.7600 loss_val: 0.9886 acc_val: 0.7111 time: 0.0277s\n",
      "Epoch: 0185 loss_train: 0.4537 acc_train: 0.8133 loss_val: 0.9921 acc_val: 0.7333 time: 0.0143s\n",
      "Epoch: 0186 loss_train: 0.4723 acc_train: 0.7467 loss_val: 0.9984 acc_val: 0.7333 time: 0.0138s\n",
      "Epoch: 0187 loss_train: 0.5740 acc_train: 0.7067 loss_val: 1.0046 acc_val: 0.7333 time: 0.0130s\n",
      "Epoch: 0188 loss_train: 0.4578 acc_train: 0.8000 loss_val: 1.0134 acc_val: 0.7222 time: 0.0120s\n",
      "Epoch: 0189 loss_train: 0.4337 acc_train: 0.8000 loss_val: 1.0181 acc_val: 0.7222 time: 0.0127s\n",
      "Epoch: 0190 loss_train: 0.5532 acc_train: 0.6667 loss_val: 1.0267 acc_val: 0.7444 time: 0.0139s\n",
      "Epoch: 0191 loss_train: 0.4296 acc_train: 0.8000 loss_val: 1.0392 acc_val: 0.7556 time: 0.0166s\n",
      "Epoch: 0192 loss_train: 0.4721 acc_train: 0.7867 loss_val: 1.0479 acc_val: 0.7556 time: 0.0168s\n",
      "Epoch: 0193 loss_train: 0.5332 acc_train: 0.6933 loss_val: 1.0543 acc_val: 0.7667 time: 0.0219s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0194 loss_train: 0.4537 acc_train: 0.7733 loss_val: 1.0597 acc_val: 0.7667 time: 0.0480s\n",
      "Epoch: 0195 loss_train: 0.5095 acc_train: 0.7467 loss_val: 1.0597 acc_val: 0.7556 time: 0.0346s\n",
      "Epoch: 0196 loss_train: 0.5280 acc_train: 0.7867 loss_val: 1.0567 acc_val: 0.7444 time: 0.0331s\n",
      "Epoch: 0197 loss_train: 0.4485 acc_train: 0.7600 loss_val: 1.0534 acc_val: 0.7333 time: 0.0310s\n",
      "Epoch: 0198 loss_train: 0.5096 acc_train: 0.7600 loss_val: 1.0448 acc_val: 0.7333 time: 0.0282s\n",
      "Epoch: 0199 loss_train: 0.5132 acc_train: 0.6533 loss_val: 1.0341 acc_val: 0.7222 time: 0.0216s\n",
      "Epoch: 0200 loss_train: 0.4585 acc_train: 0.7733 loss_val: 1.0228 acc_val: 0.7222 time: 0.0257s\n",
      "Epoch: 0201 loss_train: 0.5455 acc_train: 0.7067 loss_val: 0.9991 acc_val: 0.6778 time: 0.0188s\n",
      "Epoch: 0202 loss_train: 0.4822 acc_train: 0.7600 loss_val: 0.9810 acc_val: 0.6778 time: 0.0278s\n",
      "Epoch: 0203 loss_train: 0.4729 acc_train: 0.7333 loss_val: 0.9672 acc_val: 0.6778 time: 0.0223s\n",
      "Epoch: 0204 loss_train: 0.5152 acc_train: 0.6533 loss_val: 0.9582 acc_val: 0.6889 time: 0.0182s\n",
      "Epoch: 0205 loss_train: 0.4446 acc_train: 0.7867 loss_val: 0.9464 acc_val: 0.6889 time: 0.0137s\n",
      "Epoch: 0206 loss_train: 0.4239 acc_train: 0.8000 loss_val: 0.9393 acc_val: 0.6889 time: 0.0133s\n",
      "Epoch: 0207 loss_train: 0.5125 acc_train: 0.7333 loss_val: 0.9365 acc_val: 0.7111 time: 0.0122s\n",
      "Epoch: 0208 loss_train: 0.4247 acc_train: 0.7733 loss_val: 0.9321 acc_val: 0.7111 time: 0.0121s\n",
      "Epoch: 0209 loss_train: 0.5351 acc_train: 0.6667 loss_val: 0.9302 acc_val: 0.7111 time: 0.0133s\n",
      "Epoch: 0210 loss_train: 0.5383 acc_train: 0.7867 loss_val: 0.9220 acc_val: 0.7111 time: 0.0119s\n",
      "Epoch: 0211 loss_train: 0.4226 acc_train: 0.7867 loss_val: 0.9156 acc_val: 0.7111 time: 0.0136s\n",
      "Epoch: 0212 loss_train: 0.4429 acc_train: 0.7867 loss_val: 0.9131 acc_val: 0.7111 time: 0.0172s\n",
      "Epoch: 0213 loss_train: 0.5304 acc_train: 0.6933 loss_val: 0.9159 acc_val: 0.7222 time: 0.0243s\n",
      "Epoch: 0214 loss_train: 0.4766 acc_train: 0.7333 loss_val: 0.9181 acc_val: 0.7444 time: 0.0433s\n",
      "Epoch: 0215 loss_train: 0.5060 acc_train: 0.7333 loss_val: 0.9275 acc_val: 0.7556 time: 0.0384s\n",
      "Epoch: 0216 loss_train: 0.4666 acc_train: 0.7600 loss_val: 0.9374 acc_val: 0.7667 time: 0.0251s\n",
      "Epoch: 0217 loss_train: 0.4570 acc_train: 0.7733 loss_val: 0.9516 acc_val: 0.7556 time: 0.0209s\n",
      "Epoch: 0218 loss_train: 0.4541 acc_train: 0.8133 loss_val: 0.9604 acc_val: 0.7556 time: 0.0186s\n",
      "Epoch: 0219 loss_train: 0.4651 acc_train: 0.7200 loss_val: 0.9712 acc_val: 0.7667 time: 0.0203s\n",
      "Epoch: 0220 loss_train: 0.4530 acc_train: 0.7733 loss_val: 0.9778 acc_val: 0.7667 time: 0.0205s\n",
      "Epoch: 0221 loss_train: 0.4867 acc_train: 0.7333 loss_val: 0.9854 acc_val: 0.7667 time: 0.0185s\n",
      "Epoch: 0222 loss_train: 0.5099 acc_train: 0.7733 loss_val: 0.9864 acc_val: 0.7778 time: 0.0167s\n",
      "Epoch: 0223 loss_train: 0.4652 acc_train: 0.7733 loss_val: 0.9854 acc_val: 0.7667 time: 0.0122s\n",
      "Epoch: 0224 loss_train: 0.5367 acc_train: 0.7333 loss_val: 0.9745 acc_val: 0.7778 time: 0.0174s\n",
      "Epoch: 0225 loss_train: 0.4758 acc_train: 0.7600 loss_val: 0.9656 acc_val: 0.7667 time: 0.0218s\n",
      "Epoch: 0226 loss_train: 0.4791 acc_train: 0.7867 loss_val: 0.9333 acc_val: 0.7667 time: 0.0167s\n",
      "Epoch: 0227 loss_train: 0.5403 acc_train: 0.7333 loss_val: 0.9027 acc_val: 0.7556 time: 0.0180s\n",
      "Epoch: 0228 loss_train: 0.4112 acc_train: 0.8667 loss_val: 0.8674 acc_val: 0.7333 time: 0.0130s\n",
      "Epoch: 0229 loss_train: 0.5243 acc_train: 0.7333 loss_val: 0.8518 acc_val: 0.7444 time: 0.0153s\n",
      "Epoch: 0230 loss_train: 0.4944 acc_train: 0.6667 loss_val: 0.8417 acc_val: 0.7333 time: 0.0129s\n",
      "Epoch: 0231 loss_train: 0.4570 acc_train: 0.7867 loss_val: 0.8530 acc_val: 0.7222 time: 0.0129s\n",
      "Epoch: 0232 loss_train: 0.4647 acc_train: 0.7600 loss_val: 0.8646 acc_val: 0.7333 time: 0.0126s\n",
      "Epoch: 0233 loss_train: 0.4753 acc_train: 0.7600 loss_val: 0.8782 acc_val: 0.7333 time: 0.0127s\n",
      "Epoch: 0234 loss_train: 0.4163 acc_train: 0.7867 loss_val: 0.8816 acc_val: 0.7333 time: 0.0127s\n",
      "Epoch: 0235 loss_train: 0.5094 acc_train: 0.6667 loss_val: 0.8889 acc_val: 0.7111 time: 0.0127s\n",
      "Epoch: 0236 loss_train: 0.5067 acc_train: 0.7200 loss_val: 0.9059 acc_val: 0.7444 time: 0.0136s\n",
      "Epoch: 0237 loss_train: 0.5107 acc_train: 0.7733 loss_val: 0.9170 acc_val: 0.7222 time: 0.0152s\n",
      "Epoch: 0238 loss_train: 0.4534 acc_train: 0.7333 loss_val: 0.9261 acc_val: 0.7222 time: 0.0137s\n",
      "Epoch: 0239 loss_train: 0.4823 acc_train: 0.7333 loss_val: 0.9358 acc_val: 0.7222 time: 0.0246s\n",
      "Epoch: 0240 loss_train: 0.4708 acc_train: 0.7333 loss_val: 0.9434 acc_val: 0.7222 time: 0.0268s\n",
      "Epoch: 0241 loss_train: 0.5246 acc_train: 0.7200 loss_val: 0.9561 acc_val: 0.7222 time: 0.0299s\n",
      "Epoch: 0242 loss_train: 0.4955 acc_train: 0.6933 loss_val: 0.9666 acc_val: 0.7111 time: 0.0302s\n",
      "Epoch: 0243 loss_train: 0.5067 acc_train: 0.7867 loss_val: 0.9722 acc_val: 0.7000 time: 0.0213s\n",
      "Epoch: 0244 loss_train: 0.4984 acc_train: 0.7467 loss_val: 0.9808 acc_val: 0.7000 time: 0.0231s\n",
      "Epoch: 0245 loss_train: 0.4545 acc_train: 0.7600 loss_val: 0.9857 acc_val: 0.7000 time: 0.0211s\n",
      "Epoch: 0246 loss_train: 0.4186 acc_train: 0.7867 loss_val: 0.9873 acc_val: 0.7000 time: 0.0187s\n",
      "Epoch: 0247 loss_train: 0.4540 acc_train: 0.7333 loss_val: 0.9951 acc_val: 0.7000 time: 0.0279s\n",
      "Epoch: 0248 loss_train: 0.4489 acc_train: 0.7867 loss_val: 0.9820 acc_val: 0.7000 time: 0.0200s\n",
      "Epoch: 0249 loss_train: 0.4742 acc_train: 0.8267 loss_val: 0.9612 acc_val: 0.7000 time: 0.0241s\n",
      "Epoch: 0250 loss_train: 0.4922 acc_train: 0.7467 loss_val: 0.9428 acc_val: 0.7000 time: 0.0230s\n",
      "Epoch: 0251 loss_train: 0.4776 acc_train: 0.7733 loss_val: 0.9269 acc_val: 0.7111 time: 0.0141s\n",
      "Epoch: 0252 loss_train: 0.4441 acc_train: 0.7067 loss_val: 0.9068 acc_val: 0.7111 time: 0.0200s\n",
      "Epoch: 0253 loss_train: 0.4831 acc_train: 0.7333 loss_val: 0.8896 acc_val: 0.7111 time: 0.0125s\n",
      "Epoch: 0254 loss_train: 0.4542 acc_train: 0.7733 loss_val: 0.8703 acc_val: 0.7333 time: 0.0118s\n",
      "Epoch: 0255 loss_train: 0.5191 acc_train: 0.6667 loss_val: 0.8890 acc_val: 0.7444 time: 0.0120s\n",
      "Epoch: 0256 loss_train: 0.4825 acc_train: 0.7600 loss_val: 0.8907 acc_val: 0.7333 time: 0.0123s\n",
      "Epoch: 0257 loss_train: 0.4127 acc_train: 0.8000 loss_val: 0.8977 acc_val: 0.7333 time: 0.0115s\n",
      "Epoch: 0258 loss_train: 0.4859 acc_train: 0.7067 loss_val: 0.9050 acc_val: 0.7556 time: 0.0124s\n",
      "Epoch: 0259 loss_train: 0.4744 acc_train: 0.7333 loss_val: 0.9043 acc_val: 0.7556 time: 0.0127s\n",
      "Epoch: 0260 loss_train: 0.4743 acc_train: 0.7733 loss_val: 0.9020 acc_val: 0.7556 time: 0.0153s\n",
      "Epoch: 0261 loss_train: 0.4278 acc_train: 0.8133 loss_val: 0.9036 acc_val: 0.7556 time: 0.0136s\n",
      "Epoch: 0262 loss_train: 0.5128 acc_train: 0.6933 loss_val: 0.9080 acc_val: 0.7556 time: 0.0160s\n",
      "Epoch: 0263 loss_train: 0.4429 acc_train: 0.7467 loss_val: 0.9170 acc_val: 0.7556 time: 0.0171s\n",
      "Epoch: 0264 loss_train: 0.4750 acc_train: 0.7333 loss_val: 0.9268 acc_val: 0.7556 time: 0.0121s\n",
      "Epoch: 0265 loss_train: 0.4878 acc_train: 0.7467 loss_val: 0.9309 acc_val: 0.7556 time: 0.0140s\n",
      "Epoch: 0266 loss_train: 0.4669 acc_train: 0.8000 loss_val: 0.9322 acc_val: 0.7667 time: 0.0171s\n",
      "Epoch: 0267 loss_train: 0.4579 acc_train: 0.8000 loss_val: 0.9307 acc_val: 0.7667 time: 0.0163s\n",
      "Epoch: 0268 loss_train: 0.4853 acc_train: 0.7600 loss_val: 0.9250 acc_val: 0.7556 time: 0.0284s\n",
      "Epoch: 0269 loss_train: 0.4460 acc_train: 0.8000 loss_val: 0.9207 acc_val: 0.7556 time: 0.0260s\n",
      "Epoch: 0270 loss_train: 0.4838 acc_train: 0.7733 loss_val: 0.9125 acc_val: 0.7556 time: 0.0227s\n",
      "Epoch: 0271 loss_train: 0.4162 acc_train: 0.8000 loss_val: 0.9071 acc_val: 0.7556 time: 0.0340s\n",
      "Epoch: 0272 loss_train: 0.4369 acc_train: 0.7867 loss_val: 0.9080 acc_val: 0.7444 time: 0.0353s\n",
      "Epoch: 0273 loss_train: 0.4241 acc_train: 0.7600 loss_val: 0.9145 acc_val: 0.7444 time: 0.0281s\n",
      "Epoch: 0274 loss_train: 0.4252 acc_train: 0.7600 loss_val: 0.9207 acc_val: 0.7333 time: 0.0197s\n",
      "Epoch: 0275 loss_train: 0.4307 acc_train: 0.8267 loss_val: 0.9247 acc_val: 0.7222 time: 0.0216s\n",
      "Epoch: 0276 loss_train: 0.4952 acc_train: 0.7467 loss_val: 0.9282 acc_val: 0.7222 time: 0.0182s\n",
      "Epoch: 0277 loss_train: 0.4296 acc_train: 0.7600 loss_val: 0.9346 acc_val: 0.7000 time: 0.0128s\n",
      "Epoch: 0278 loss_train: 0.4813 acc_train: 0.7733 loss_val: 0.9415 acc_val: 0.7000 time: 0.0120s\n",
      "Epoch: 0279 loss_train: 0.4881 acc_train: 0.6800 loss_val: 0.9481 acc_val: 0.7000 time: 0.0122s\n",
      "Epoch: 0280 loss_train: 0.4912 acc_train: 0.7067 loss_val: 0.9644 acc_val: 0.7000 time: 0.0121s\n",
      "Epoch: 0281 loss_train: 0.4395 acc_train: 0.7067 loss_val: 0.9864 acc_val: 0.7333 time: 0.0118s\n",
      "Epoch: 0282 loss_train: 0.4701 acc_train: 0.6800 loss_val: 1.0039 acc_val: 0.7333 time: 0.0120s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0283 loss_train: 0.4552 acc_train: 0.8133 loss_val: 1.0239 acc_val: 0.7333 time: 0.0159s\n",
      "Epoch: 0284 loss_train: 0.4706 acc_train: 0.7333 loss_val: 1.0450 acc_val: 0.7444 time: 0.0182s\n",
      "Epoch: 0285 loss_train: 0.4345 acc_train: 0.7600 loss_val: 1.0656 acc_val: 0.7556 time: 0.0114s\n",
      "Epoch: 0286 loss_train: 0.5163 acc_train: 0.6933 loss_val: 1.0825 acc_val: 0.7556 time: 0.0178s\n",
      "Epoch: 0287 loss_train: 0.5152 acc_train: 0.6933 loss_val: 1.0901 acc_val: 0.7556 time: 0.0137s\n",
      "Epoch: 0288 loss_train: 0.4340 acc_train: 0.7733 loss_val: 1.0956 acc_val: 0.7556 time: 0.0172s\n",
      "Epoch: 0289 loss_train: 0.4907 acc_train: 0.7067 loss_val: 1.1010 acc_val: 0.7556 time: 0.0171s\n",
      "Epoch: 0290 loss_train: 0.4946 acc_train: 0.7600 loss_val: 1.1059 acc_val: 0.7556 time: 0.0208s\n",
      "Epoch: 0291 loss_train: 0.5119 acc_train: 0.7733 loss_val: 1.1072 acc_val: 0.7556 time: 0.0186s\n",
      "Epoch: 0292 loss_train: 0.5586 acc_train: 0.6667 loss_val: 1.1164 acc_val: 0.7556 time: 0.0226s\n",
      "Epoch: 0293 loss_train: 0.4735 acc_train: 0.6933 loss_val: 1.1190 acc_val: 0.7667 time: 0.0236s\n",
      "Epoch: 0294 loss_train: 0.4767 acc_train: 0.6800 loss_val: 1.1169 acc_val: 0.7667 time: 0.0282s\n",
      "Epoch: 0295 loss_train: 0.4639 acc_train: 0.7333 loss_val: 1.1145 acc_val: 0.7667 time: 0.0369s\n",
      "Epoch: 0296 loss_train: 0.4444 acc_train: 0.7600 loss_val: 1.0966 acc_val: 0.7667 time: 0.0224s\n",
      "Epoch: 0297 loss_train: 0.4536 acc_train: 0.7067 loss_val: 1.0837 acc_val: 0.7667 time: 0.0278s\n",
      "Epoch: 0298 loss_train: 0.4879 acc_train: 0.6933 loss_val: 1.0564 acc_val: 0.7667 time: 0.0187s\n",
      "Epoch: 0299 loss_train: 0.4513 acc_train: 0.7600 loss_val: 1.0413 acc_val: 0.7556 time: 0.0173s\n",
      "Epoch: 0300 loss_train: 0.4064 acc_train: 0.7333 loss_val: 1.0357 acc_val: 0.7556 time: 0.0188s\n",
      "Epoch: 0301 loss_train: 0.4602 acc_train: 0.7600 loss_val: 1.0247 acc_val: 0.7667 time: 0.0183s\n",
      "Epoch: 0302 loss_train: 0.3872 acc_train: 0.8133 loss_val: 1.0047 acc_val: 0.7667 time: 0.0139s\n",
      "Epoch: 0303 loss_train: 0.4691 acc_train: 0.7200 loss_val: 0.9841 acc_val: 0.7667 time: 0.0143s\n",
      "Epoch: 0304 loss_train: 0.4970 acc_train: 0.8267 loss_val: 0.9481 acc_val: 0.7667 time: 0.0159s\n",
      "Epoch: 0305 loss_train: 0.4060 acc_train: 0.8133 loss_val: 0.9164 acc_val: 0.7667 time: 0.0131s\n",
      "Epoch: 0306 loss_train: 0.4331 acc_train: 0.7467 loss_val: 0.8902 acc_val: 0.7556 time: 0.0229s\n",
      "Epoch: 0307 loss_train: 0.4124 acc_train: 0.8000 loss_val: 0.8664 acc_val: 0.7556 time: 0.0150s\n",
      "Epoch: 0308 loss_train: 0.5111 acc_train: 0.7200 loss_val: 0.8972 acc_val: 0.7444 time: 0.0171s\n",
      "Epoch: 0309 loss_train: 0.4721 acc_train: 0.7067 loss_val: 0.8971 acc_val: 0.7444 time: 0.0172s\n",
      "Epoch: 0310 loss_train: 0.4288 acc_train: 0.8000 loss_val: 0.8855 acc_val: 0.7556 time: 0.0195s\n",
      "Epoch: 0311 loss_train: 0.4090 acc_train: 0.7733 loss_val: 0.8847 acc_val: 0.7667 time: 0.0270s\n",
      "Epoch: 0312 loss_train: 0.5228 acc_train: 0.6933 loss_val: 0.8918 acc_val: 0.7556 time: 0.0292s\n",
      "Epoch: 0313 loss_train: 0.4302 acc_train: 0.8000 loss_val: 0.8990 acc_val: 0.7556 time: 0.0235s\n",
      "Epoch: 0314 loss_train: 0.5083 acc_train: 0.7600 loss_val: 0.9151 acc_val: 0.7778 time: 0.0259s\n",
      "Epoch: 0315 loss_train: 0.4472 acc_train: 0.8267 loss_val: 0.9229 acc_val: 0.7667 time: 0.0246s\n",
      "Epoch: 0316 loss_train: 0.4453 acc_train: 0.7467 loss_val: 0.9425 acc_val: 0.7667 time: 0.0200s\n",
      "Epoch: 0317 loss_train: 0.4537 acc_train: 0.8133 loss_val: 0.9640 acc_val: 0.7667 time: 0.0183s\n",
      "Epoch: 0318 loss_train: 0.4893 acc_train: 0.7333 loss_val: 0.9877 acc_val: 0.7667 time: 0.0194s\n",
      "Epoch: 0319 loss_train: 0.4647 acc_train: 0.7333 loss_val: 1.0046 acc_val: 0.7667 time: 0.0161s\n",
      "Epoch: 0320 loss_train: 0.6100 acc_train: 0.6667 loss_val: 1.0119 acc_val: 0.7667 time: 0.0119s\n",
      "Epoch: 0321 loss_train: 0.4846 acc_train: 0.7733 loss_val: 1.0181 acc_val: 0.7667 time: 0.0120s\n",
      "Epoch: 0322 loss_train: 0.4712 acc_train: 0.7467 loss_val: 1.0192 acc_val: 0.7667 time: 0.0117s\n",
      "Epoch: 0323 loss_train: 0.5369 acc_train: 0.7200 loss_val: 1.0143 acc_val: 0.7667 time: 0.0120s\n",
      "Epoch: 0324 loss_train: 0.4493 acc_train: 0.7733 loss_val: 1.0043 acc_val: 0.7556 time: 0.0117s\n",
      "Epoch: 0325 loss_train: 0.5259 acc_train: 0.7200 loss_val: 0.9835 acc_val: 0.7444 time: 0.0118s\n",
      "Epoch: 0326 loss_train: 0.4495 acc_train: 0.8133 loss_val: 0.9676 acc_val: 0.7444 time: 0.0116s\n",
      "Epoch: 0327 loss_train: 0.4391 acc_train: 0.8000 loss_val: 0.9521 acc_val: 0.7333 time: 0.0112s\n",
      "Epoch: 0328 loss_train: 0.4810 acc_train: 0.7867 loss_val: 0.9368 acc_val: 0.7222 time: 0.0146s\n",
      "Epoch: 0329 loss_train: 0.4516 acc_train: 0.7333 loss_val: 0.9229 acc_val: 0.7222 time: 0.0156s\n",
      "Epoch: 0330 loss_train: 0.4712 acc_train: 0.8000 loss_val: 0.9232 acc_val: 0.7222 time: 0.0198s\n",
      "Epoch: 0331 loss_train: 0.5441 acc_train: 0.7867 loss_val: 0.9115 acc_val: 0.7222 time: 0.0171s\n",
      "Epoch: 0332 loss_train: 0.4836 acc_train: 0.7467 loss_val: 0.9039 acc_val: 0.7222 time: 0.0200s\n",
      "Epoch: 0333 loss_train: 0.4862 acc_train: 0.7333 loss_val: 0.9002 acc_val: 0.7222 time: 0.0261s\n",
      "Epoch: 0334 loss_train: 0.4283 acc_train: 0.6933 loss_val: 0.8935 acc_val: 0.7222 time: 0.0373s\n",
      "Epoch: 0335 loss_train: 0.4737 acc_train: 0.7733 loss_val: 0.8763 acc_val: 0.7222 time: 0.0444s\n",
      "Epoch: 0336 loss_train: 0.4743 acc_train: 0.7333 loss_val: 0.8586 acc_val: 0.7222 time: 0.0331s\n",
      "Epoch: 0337 loss_train: 0.4579 acc_train: 0.7867 loss_val: 0.8495 acc_val: 0.7222 time: 0.0332s\n",
      "Epoch: 0338 loss_train: 0.3953 acc_train: 0.8267 loss_val: 0.8371 acc_val: 0.7222 time: 0.0263s\n",
      "Epoch: 0339 loss_train: 0.4427 acc_train: 0.7200 loss_val: 0.8300 acc_val: 0.7222 time: 0.0192s\n",
      "Epoch: 0340 loss_train: 0.5134 acc_train: 0.7200 loss_val: 0.8156 acc_val: 0.7333 time: 0.0179s\n",
      "Epoch: 0341 loss_train: 0.4369 acc_train: 0.7867 loss_val: 0.8177 acc_val: 0.7333 time: 0.0127s\n",
      "Epoch: 0342 loss_train: 0.4737 acc_train: 0.7467 loss_val: 0.8176 acc_val: 0.7444 time: 0.0128s\n",
      "Epoch: 0343 loss_train: 0.4818 acc_train: 0.7067 loss_val: 0.8288 acc_val: 0.7556 time: 0.0127s\n",
      "Epoch: 0344 loss_train: 0.4810 acc_train: 0.7067 loss_val: 0.8186 acc_val: 0.7556 time: 0.0130s\n",
      "Epoch: 0345 loss_train: 0.4891 acc_train: 0.7200 loss_val: 0.8114 acc_val: 0.7556 time: 0.0132s\n",
      "Epoch: 0346 loss_train: 0.4313 acc_train: 0.7600 loss_val: 0.8071 acc_val: 0.7556 time: 0.0143s\n",
      "Epoch: 0347 loss_train: 0.4094 acc_train: 0.7733 loss_val: 0.8078 acc_val: 0.7556 time: 0.0117s\n",
      "Epoch: 0348 loss_train: 0.5353 acc_train: 0.6667 loss_val: 0.8093 acc_val: 0.7667 time: 0.0130s\n",
      "Epoch: 0349 loss_train: 0.4981 acc_train: 0.7467 loss_val: 0.8212 acc_val: 0.7667 time: 0.0136s\n",
      "Epoch: 0350 loss_train: 0.5464 acc_train: 0.6800 loss_val: 0.8334 acc_val: 0.7667 time: 0.0175s\n",
      "Epoch: 0351 loss_train: 0.5032 acc_train: 0.6933 loss_val: 0.8366 acc_val: 0.7667 time: 0.0169s\n",
      "Epoch: 0352 loss_train: 0.4544 acc_train: 0.7733 loss_val: 0.8438 acc_val: 0.7667 time: 0.0130s\n",
      "Epoch: 0353 loss_train: 0.3805 acc_train: 0.7467 loss_val: 0.8494 acc_val: 0.7556 time: 0.0131s\n",
      "Epoch: 0354 loss_train: 0.4462 acc_train: 0.6933 loss_val: 0.8386 acc_val: 0.7667 time: 0.0130s\n",
      "Epoch: 0355 loss_train: 0.5493 acc_train: 0.7333 loss_val: 0.8297 acc_val: 0.7556 time: 0.0139s\n",
      "Epoch: 0356 loss_train: 0.4181 acc_train: 0.8000 loss_val: 0.8187 acc_val: 0.7556 time: 0.0149s\n",
      "Epoch: 0357 loss_train: 0.5274 acc_train: 0.7067 loss_val: 0.7874 acc_val: 0.7556 time: 0.0151s\n",
      "Epoch: 0358 loss_train: 0.4723 acc_train: 0.7467 loss_val: 0.7580 acc_val: 0.7556 time: 0.0259s\n",
      "Epoch: 0359 loss_train: 0.4559 acc_train: 0.7600 loss_val: 0.7337 acc_val: 0.7444 time: 0.0305s\n",
      "Epoch: 0360 loss_train: 0.4902 acc_train: 0.7333 loss_val: 0.7139 acc_val: 0.7000 time: 0.0332s\n",
      "Epoch: 0361 loss_train: 0.4637 acc_train: 0.7733 loss_val: 0.6974 acc_val: 0.7222 time: 0.0326s\n",
      "Epoch: 0362 loss_train: 0.4168 acc_train: 0.7600 loss_val: 0.6855 acc_val: 0.7222 time: 0.0224s\n",
      "Epoch: 0363 loss_train: 0.5002 acc_train: 0.7067 loss_val: 0.6819 acc_val: 0.7222 time: 0.0167s\n",
      "Epoch: 0364 loss_train: 0.4698 acc_train: 0.6800 loss_val: 0.6816 acc_val: 0.7222 time: 0.0274s\n",
      "Epoch: 0365 loss_train: 0.4407 acc_train: 0.7733 loss_val: 0.6840 acc_val: 0.7222 time: 0.0283s\n",
      "Epoch: 0366 loss_train: 0.4915 acc_train: 0.6800 loss_val: 0.6893 acc_val: 0.7222 time: 0.0205s\n",
      "Epoch: 0367 loss_train: 0.5635 acc_train: 0.7600 loss_val: 0.6986 acc_val: 0.7222 time: 0.0196s\n",
      "Epoch: 0368 loss_train: 0.5296 acc_train: 0.6800 loss_val: 0.7116 acc_val: 0.7000 time: 0.0120s\n",
      "Epoch: 0369 loss_train: 0.4994 acc_train: 0.7467 loss_val: 0.7310 acc_val: 0.7000 time: 0.0121s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0370 loss_train: 0.4309 acc_train: 0.7200 loss_val: 0.7508 acc_val: 0.7000 time: 0.0147s\n",
      "Epoch: 0371 loss_train: 0.5548 acc_train: 0.6933 loss_val: 0.7724 acc_val: 0.7000 time: 0.0161s\n",
      "Epoch: 0372 loss_train: 0.4668 acc_train: 0.7600 loss_val: 0.7898 acc_val: 0.7333 time: 0.0138s\n",
      "Epoch: 0373 loss_train: 0.4306 acc_train: 0.8000 loss_val: 0.8052 acc_val: 0.7444 time: 0.0181s\n",
      "Epoch: 0374 loss_train: 0.4385 acc_train: 0.7733 loss_val: 0.8198 acc_val: 0.7556 time: 0.0122s\n",
      "Epoch: 0375 loss_train: 0.4633 acc_train: 0.6933 loss_val: 0.8352 acc_val: 0.7444 time: 0.0122s\n",
      "Epoch: 0376 loss_train: 0.4598 acc_train: 0.7733 loss_val: 0.8468 acc_val: 0.7556 time: 0.0122s\n",
      "Epoch: 0377 loss_train: 0.4471 acc_train: 0.8000 loss_val: 0.8482 acc_val: 0.7556 time: 0.0121s\n",
      "Epoch: 0378 loss_train: 0.4225 acc_train: 0.8133 loss_val: 0.8439 acc_val: 0.7556 time: 0.0119s\n",
      "Epoch: 0379 loss_train: 0.3983 acc_train: 0.8533 loss_val: 0.8315 acc_val: 0.7556 time: 0.0122s\n",
      "Epoch: 0380 loss_train: 0.4591 acc_train: 0.7067 loss_val: 0.8212 acc_val: 0.7444 time: 0.0128s\n",
      "Epoch: 0381 loss_train: 0.4896 acc_train: 0.7333 loss_val: 0.8111 acc_val: 0.7444 time: 0.0126s\n",
      "Epoch: 0382 loss_train: 0.4392 acc_train: 0.8000 loss_val: 0.8037 acc_val: 0.7444 time: 0.0129s\n",
      "Epoch: 0383 loss_train: 0.4724 acc_train: 0.7467 loss_val: 0.7998 acc_val: 0.7444 time: 0.0125s\n",
      "Epoch: 0384 loss_train: 0.4672 acc_train: 0.8400 loss_val: 0.7992 acc_val: 0.7444 time: 0.0137s\n",
      "Epoch: 0385 loss_train: 0.4384 acc_train: 0.7333 loss_val: 0.7953 acc_val: 0.7444 time: 0.0150s\n",
      "Epoch: 0386 loss_train: 0.3954 acc_train: 0.7600 loss_val: 0.7952 acc_val: 0.7556 time: 0.0159s\n",
      "Epoch: 0387 loss_train: 0.4507 acc_train: 0.7600 loss_val: 0.7979 acc_val: 0.7556 time: 0.0271s\n",
      "Epoch: 0388 loss_train: 0.4610 acc_train: 0.7600 loss_val: 0.8040 acc_val: 0.7556 time: 0.0329s\n",
      "Epoch: 0389 loss_train: 0.4354 acc_train: 0.7867 loss_val: 0.8154 acc_val: 0.7444 time: 0.0232s\n",
      "Epoch: 0390 loss_train: 0.4102 acc_train: 0.8000 loss_val: 0.8241 acc_val: 0.7444 time: 0.0294s\n",
      "Epoch: 0391 loss_train: 0.4667 acc_train: 0.7467 loss_val: 0.8361 acc_val: 0.7556 time: 0.0262s\n",
      "Epoch: 0392 loss_train: 0.4519 acc_train: 0.7600 loss_val: 0.8531 acc_val: 0.7556 time: 0.0255s\n",
      "Epoch: 0393 loss_train: 0.4806 acc_train: 0.7333 loss_val: 0.8622 acc_val: 0.7444 time: 0.0289s\n",
      "Epoch: 0394 loss_train: 0.4513 acc_train: 0.7467 loss_val: 0.8750 acc_val: 0.7667 time: 0.0275s\n",
      "Epoch: 0395 loss_train: 0.4677 acc_train: 0.6800 loss_val: 0.8923 acc_val: 0.7667 time: 0.0217s\n",
      "Epoch: 0396 loss_train: 0.3872 acc_train: 0.7733 loss_val: 0.9027 acc_val: 0.7556 time: 0.0214s\n",
      "Epoch: 0397 loss_train: 0.5479 acc_train: 0.6933 loss_val: 0.9169 acc_val: 0.7556 time: 0.0205s\n",
      "Epoch: 0398 loss_train: 0.4135 acc_train: 0.8133 loss_val: 0.9303 acc_val: 0.7556 time: 0.0198s\n",
      "Epoch: 0399 loss_train: 0.4446 acc_train: 0.7333 loss_val: 0.9377 acc_val: 0.7556 time: 0.0175s\n",
      "Epoch: 0400 loss_train: 0.4408 acc_train: 0.7867 loss_val: 0.9417 acc_val: 0.7667 time: 0.0127s\n",
      "Epoch: 0401 loss_train: 0.4610 acc_train: 0.7067 loss_val: 0.9437 acc_val: 0.7667 time: 0.0131s\n",
      "Epoch: 0402 loss_train: 0.4644 acc_train: 0.7467 loss_val: 0.9449 acc_val: 0.7556 time: 0.0132s\n",
      "Epoch: 0403 loss_train: 0.5250 acc_train: 0.7067 loss_val: 0.9353 acc_val: 0.7333 time: 0.0129s\n",
      "Epoch: 0404 loss_train: 0.4291 acc_train: 0.6800 loss_val: 0.9292 acc_val: 0.7333 time: 0.0125s\n",
      "Epoch: 0405 loss_train: 0.4685 acc_train: 0.8267 loss_val: 0.9089 acc_val: 0.7222 time: 0.0154s\n",
      "Epoch: 0406 loss_train: 0.4354 acc_train: 0.8000 loss_val: 0.8982 acc_val: 0.7222 time: 0.0159s\n",
      "Epoch: 0407 loss_train: 0.4610 acc_train: 0.7733 loss_val: 0.8945 acc_val: 0.7222 time: 0.0195s\n",
      "Epoch: 0408 loss_train: 0.4730 acc_train: 0.7733 loss_val: 0.8953 acc_val: 0.7222 time: 0.0125s\n",
      "Epoch: 0409 loss_train: 0.3854 acc_train: 0.7467 loss_val: 0.8964 acc_val: 0.7222 time: 0.0120s\n",
      "Epoch: 0410 loss_train: 0.4824 acc_train: 0.6800 loss_val: 0.8963 acc_val: 0.7222 time: 0.0116s\n",
      "Epoch: 0411 loss_train: 0.4434 acc_train: 0.7733 loss_val: 0.8892 acc_val: 0.7222 time: 0.0164s\n",
      "Epoch: 0412 loss_train: 0.3774 acc_train: 0.8400 loss_val: 0.8773 acc_val: 0.7111 time: 0.0211s\n",
      "Epoch: 0413 loss_train: 0.4141 acc_train: 0.7467 loss_val: 0.8690 acc_val: 0.7444 time: 0.0156s\n",
      "Epoch: 0414 loss_train: 0.4660 acc_train: 0.7333 loss_val: 0.8505 acc_val: 0.7444 time: 0.0263s\n",
      "Epoch: 0415 loss_train: 0.4522 acc_train: 0.8000 loss_val: 0.8386 acc_val: 0.7444 time: 0.0428s\n",
      "Epoch: 0416 loss_train: 0.4712 acc_train: 0.8400 loss_val: 0.8314 acc_val: 0.7444 time: 0.0340s\n",
      "Epoch: 0417 loss_train: 0.4498 acc_train: 0.7600 loss_val: 0.8193 acc_val: 0.7000 time: 0.0229s\n",
      "Epoch: 0418 loss_train: 0.4477 acc_train: 0.7333 loss_val: 0.8112 acc_val: 0.7111 time: 0.0234s\n",
      "Epoch: 0419 loss_train: 0.4254 acc_train: 0.7733 loss_val: 0.8105 acc_val: 0.7333 time: 0.0248s\n",
      "Epoch: 0420 loss_train: 0.4403 acc_train: 0.8133 loss_val: 0.8117 acc_val: 0.7333 time: 0.0247s\n",
      "Epoch: 0421 loss_train: 0.5146 acc_train: 0.7333 loss_val: 0.8105 acc_val: 0.7444 time: 0.0316s\n",
      "Epoch: 0422 loss_train: 0.5196 acc_train: 0.6667 loss_val: 0.8104 acc_val: 0.7556 time: 0.0295s\n",
      "Epoch: 0423 loss_train: 0.3834 acc_train: 0.8000 loss_val: 0.8088 acc_val: 0.7667 time: 0.0183s\n",
      "Epoch: 0424 loss_train: 0.3740 acc_train: 0.8533 loss_val: 0.8084 acc_val: 0.7556 time: 0.0183s\n",
      "Epoch: 0425 loss_train: 0.3915 acc_train: 0.8000 loss_val: 0.8162 acc_val: 0.7556 time: 0.0289s\n",
      "Epoch: 0426 loss_train: 0.3987 acc_train: 0.7467 loss_val: 0.8228 acc_val: 0.7556 time: 0.0217s\n",
      "Epoch: 0427 loss_train: 0.4250 acc_train: 0.7867 loss_val: 0.8169 acc_val: 0.7556 time: 0.0204s\n",
      "Epoch: 0428 loss_train: 0.4477 acc_train: 0.8133 loss_val: 0.8125 acc_val: 0.7556 time: 0.0156s\n",
      "Epoch: 0429 loss_train: 0.5134 acc_train: 0.7067 loss_val: 0.8082 acc_val: 0.7556 time: 0.0137s\n",
      "Epoch: 0430 loss_train: 0.4021 acc_train: 0.8133 loss_val: 0.8060 acc_val: 0.7556 time: 0.0127s\n",
      "Epoch: 0431 loss_train: 0.4547 acc_train: 0.7200 loss_val: 0.8065 acc_val: 0.7556 time: 0.0121s\n",
      "Epoch: 0432 loss_train: 0.4076 acc_train: 0.8000 loss_val: 0.8042 acc_val: 0.7556 time: 0.0120s\n",
      "Epoch: 0433 loss_train: 0.4686 acc_train: 0.6933 loss_val: 0.8023 acc_val: 0.7556 time: 0.0121s\n",
      "Epoch: 0434 loss_train: 0.4304 acc_train: 0.7467 loss_val: 0.8061 acc_val: 0.7556 time: 0.0121s\n",
      "Epoch: 0435 loss_train: 0.3919 acc_train: 0.7867 loss_val: 0.8100 acc_val: 0.7556 time: 0.0117s\n",
      "Epoch: 0436 loss_train: 0.4031 acc_train: 0.7333 loss_val: 0.8160 acc_val: 0.7556 time: 0.0117s\n",
      "Epoch: 0437 loss_train: 0.4024 acc_train: 0.7733 loss_val: 0.8221 acc_val: 0.7556 time: 0.0116s\n",
      "Epoch: 0438 loss_train: 0.4682 acc_train: 0.7733 loss_val: 0.8267 acc_val: 0.7556 time: 0.0117s\n",
      "Epoch: 0439 loss_train: 0.4543 acc_train: 0.7733 loss_val: 0.8298 acc_val: 0.7556 time: 0.0118s\n",
      "Epoch: 0440 loss_train: 0.5190 acc_train: 0.7600 loss_val: 0.8335 acc_val: 0.7556 time: 0.0148s\n",
      "Epoch: 0441 loss_train: 0.4464 acc_train: 0.7733 loss_val: 0.8359 acc_val: 0.7556 time: 0.0163s\n",
      "Epoch: 0442 loss_train: 0.3884 acc_train: 0.8400 loss_val: 0.8459 acc_val: 0.7444 time: 0.0188s\n",
      "Epoch: 0443 loss_train: 0.4427 acc_train: 0.7867 loss_val: 0.8555 acc_val: 0.7778 time: 0.0140s\n",
      "Epoch: 0444 loss_train: 0.4067 acc_train: 0.8000 loss_val: 0.8664 acc_val: 0.7889 time: 0.0135s\n",
      "Epoch: 0445 loss_train: 0.4971 acc_train: 0.6667 loss_val: 0.8643 acc_val: 0.7889 time: 0.0140s\n",
      "Epoch: 0446 loss_train: 0.5040 acc_train: 0.6933 loss_val: 0.8720 acc_val: 0.7889 time: 0.0151s\n",
      "Epoch: 0447 loss_train: 0.4397 acc_train: 0.7200 loss_val: 0.8800 acc_val: 0.7889 time: 0.0161s\n",
      "Epoch: 0448 loss_train: 0.4265 acc_train: 0.7600 loss_val: 0.8937 acc_val: 0.7778 time: 0.0166s\n",
      "Epoch: 0449 loss_train: 0.4441 acc_train: 0.7867 loss_val: 0.9105 acc_val: 0.7778 time: 0.0171s\n",
      "Epoch: 0450 loss_train: 0.4020 acc_train: 0.7333 loss_val: 0.9180 acc_val: 0.7778 time: 0.0202s\n",
      "Optimization Finished!\n",
      "Total time elapsed: 8.2177s\n",
      "Test set results: loss= 0.3049 accuracy= 0.9130\n"
     ]
    }
   ],
   "source": [
    "cuda = False\n",
    "hidden = 16\n",
    "dropout = 0.5\n",
    "lr = 0.01\n",
    "weight_decay = 5e-4\n",
    "fastmode = False\n",
    "epochs = 450\n",
    "layers =4\n",
    "\n",
    "# Model and optimizer\n",
    "model = AGNN(nfeat=features.shape[1],\n",
    "                     nhid=hidden,\n",
    "                     nclass=2,\n",
    "                     nlayers=layers,\n",
    "                     dropout_rate=0.5)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(),\n",
    "                       lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "if cuda:\n",
    "    model.cuda()\n",
    "    features = features.cuda()\n",
    "    adj = adj.cuda()\n",
    "    all_labels = all_labels.cuda()\n",
    "    idx_train = idx_train.cuda()\n",
    "    idx_val = idx_val.cuda()\n",
    "    idx_test = idx_test.cuda()\n",
    "\n",
    "\n",
    "def train(epoch):\n",
    "    t = time.time()\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(features, adj)\n",
    "\n",
    "    \n",
    "    loss_train = F.nll_loss(output[idx_train], all_labels[idx_train])\n",
    "    acc_train = accuracy(output[idx_train], all_labels[idx_train])\n",
    "    loss_train.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if not fastmode:\n",
    "        # Evaluate validation set performance separately,\n",
    "        # deactivates dropout during validation run.\n",
    "        model.eval()\n",
    "        output = model(features, adj)\n",
    "\n",
    "    loss_val = F.nll_loss(output[idx_val], all_labels[idx_val])\n",
    "    acc_val = accuracy(output[idx_val], all_labels[idx_val])\n",
    "    print('Epoch: {:04d}'.format(epoch+1),\n",
    "          'loss_train: {:.4f}'.format(loss_train.item()),\n",
    "          'acc_train: {:.4f}'.format(acc_train.item()),\n",
    "          'loss_val: {:.4f}'.format(loss_val.item()),\n",
    "          'acc_val: {:.4f}'.format(acc_val.item()),\n",
    "          'time: {:.4f}s'.format(time.time() - t))\n",
    "\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    output = model(features, adj)\n",
    "    loss_test = F.nll_loss(output[idx_test], all_labels[idx_test])\n",
    "    acc_test = accuracy(output[idx_test], all_labels[idx_test])\n",
    "    print(\"Test set results:\",\n",
    "          \"loss= {:.4f}\".format(loss_test.item()),\n",
    "          \"accuracy= {:.4f}\".format(acc_test.item()))\n",
    "\n",
    "\n",
    "# Train model\n",
    "t_total = time.time()\n",
    "for epoch in range(epochs):\n",
    "    train(epoch)\n",
    "print(\"Optimization Finished!\")\n",
    "print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))\n",
    "\n",
    "# Testing\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
